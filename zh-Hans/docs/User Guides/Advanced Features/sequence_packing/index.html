<!doctype html>
<html lang="zh-Hans" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-User Guides/Advanced Features/sequence_packing" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">ROLL SEQUENCE PACKING | ROLL</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://alibaba.github.io/ROLL/zh-Hans/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://alibaba.github.io/ROLL/zh-Hans/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://alibaba.github.io/ROLL/zh-Hans/docs/User Guides/Advanced Features/sequence_packing"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="ROLL SEQUENCE PACKING | ROLL"><meta data-rh="true" name="description" content="ROLL框架目前支持了Sequence Packing功能，通过句子打包来避免pad token，提高计算效率。本文档详细介绍该功能的实现思路以及相应使用配置方法。"><meta data-rh="true" property="og:description" content="ROLL框架目前支持了Sequence Packing功能，通过句子打包来避免pad token，提高计算效率。本文档详细介绍该功能的实现思路以及相应使用配置方法。"><link data-rh="true" rel="icon" href="https://img.alicdn.com/imgextra/i4/O1CN01bo6EZl2192CAIjFwE_!!6000000006941-2-tps-465-367.png"><link data-rh="true" rel="canonical" href="https://alibaba.github.io/ROLL/zh-Hans/docs/User Guides/Advanced Features/sequence_packing"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/User Guides/Advanced Features/sequence_packing" hreflang="en"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/zh-Hans/docs/User Guides/Advanced Features/sequence_packing" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/User Guides/Advanced Features/sequence_packing" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"ROLL SEQUENCE PACKING","item":"https://alibaba.github.io/ROLL/zh-Hans/docs/User Guides/Advanced Features/sequence_packing"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D6R4GXHVFP"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-D6R4GXHVFP",{anonymize_ip:!0})</script><link rel="stylesheet" href="/ROLL/zh-Hans/assets/css/styles.4016bf18.css">
<script src="/ROLL/zh-Hans/assets/js/runtime~main.138e3150.js" defer="defer"></script>
<script src="/ROLL/zh-Hans/assets/js/main.b037e3bf.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"dark"),document.documentElement.setAttribute("data-theme-choice",t||"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav class="navbar navbar--fixed-top navbar_MONK"><div class="navbar__inner"><div class="logoWrap_HHlA navbar__items"><div class="logo_Ufb2"><div class="ant-image css-plsjn" style="width:40px;height:32px"><img alt="ROLL" class="ant-image-img css-plsjn" style="height:32px" src="https://img.alicdn.com/imgextra/i3/O1CN016Mlxas1MHNA3NEbZ0_!!6000000001409-2-tps-465-367.png" width="40" height="32"></div></div><div><div class="title_E_95">ROLL</div><div class="subTitle_M9Ik">像一名强化学习算法开发者一样</div></div></div><div class="navbar__items navbar__items--right"><a href="/ROLL/zh-Hans/" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false">首页</a><a href="/ROLL/zh-Hans/#core" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false">核心算法</a><a href="/ROLL/zh-Hans/#research" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false">开源社区</a><a href="/ROLL/zh-Hans/docs/Overview" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3 primary_vbUC" tabindex="0" aria-disabled="false">文档</a><a href="https://github.com/alibaba/ROLL" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false"><span>Github</span><span role="img" aria-label="export" class="anticon anticon-export"><svg fill-rule="evenodd" viewBox="64 64 896 896" focusable="false" data-icon="export" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M880 912H144c-17.7 0-32-14.3-32-32V144c0-17.7 14.3-32 32-32h360c4.4 0 8 3.6 8 8v56c0 4.4-3.6 8-8 8H184v656h656V520c0-4.4 3.6-8 8-8h56c4.4 0 8 3.6 8 8v360c0 17.7-14.3 32-32 32zM770.87 199.13l-52.2-52.2a8.01 8.01 0 014.7-13.6l179.4-21c5.1-.6 9.5 3.7 8.9 8.9l-21 179.4c-.8 6.6-8.9 9.4-13.6 4.7l-52.4-52.4-256.2 256.2a8.03 8.03 0 01-11.3 0l-42.4-42.4a8.03 8.03 0 010-11.3l256.1-256.3z"></path></svg></span></a><button type="button" class="ant-btn css-plsjn ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-dropdown-trigger language_XaW5"><span class="ant-btn-icon"><span role="img" aria-label="global" class="anticon anticon-global"><svg viewBox="64 64 896 896" focusable="false" data-icon="global" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M854.4 800.9c.2-.3.5-.6.7-.9C920.6 722.1 960 621.7 960 512s-39.4-210.1-104.8-288c-.2-.3-.5-.5-.7-.8-1.1-1.3-2.1-2.5-3.2-3.7-.4-.5-.8-.9-1.2-1.4l-4.1-4.7-.1-.1c-1.5-1.7-3.1-3.4-4.6-5.1l-.1-.1c-3.2-3.4-6.4-6.8-9.7-10.1l-.1-.1-4.8-4.8-.3-.3c-1.5-1.5-3-2.9-4.5-4.3-.5-.5-1-1-1.6-1.5-1-1-2-1.9-3-2.8-.3-.3-.7-.6-1-1C736.4 109.2 629.5 64 512 64s-224.4 45.2-304.3 119.2c-.3.3-.7.6-1 1-1 .9-2 1.9-3 2.9-.5.5-1 1-1.6 1.5-1.5 1.4-3 2.9-4.5 4.3l-.3.3-4.8 4.8-.1.1c-3.3 3.3-6.5 6.7-9.7 10.1l-.1.1c-1.6 1.7-3.1 3.4-4.6 5.1l-.1.1c-1.4 1.5-2.8 3.1-4.1 4.7-.4.5-.8.9-1.2 1.4-1.1 1.2-2.1 2.5-3.2 3.7-.2.3-.5.5-.7.8C103.4 301.9 64 402.3 64 512s39.4 210.1 104.8 288c.2.3.5.6.7.9l3.1 3.7c.4.5.8.9 1.2 1.4l4.1 4.7c0 .1.1.1.1.2 1.5 1.7 3 3.4 4.6 5l.1.1c3.2 3.4 6.4 6.8 9.6 10.1l.1.1c1.6 1.6 3.1 3.2 4.7 4.7l.3.3c3.3 3.3 6.7 6.5 10.1 9.6 80.1 74 187 119.2 304.5 119.2s224.4-45.2 304.3-119.2a300 300 0 0010-9.6l.3-.3c1.6-1.6 3.2-3.1 4.7-4.7l.1-.1c3.3-3.3 6.5-6.7 9.6-10.1l.1-.1c1.5-1.7 3.1-3.3 4.6-5 0-.1.1-.1.1-.2 1.4-1.5 2.8-3.1 4.1-4.7.4-.5.8-.9 1.2-1.4a99 99 0 003.3-3.7zm4.1-142.6c-13.8 32.6-32 62.8-54.2 90.2a444.07 444.07 0 00-81.5-55.9c11.6-46.9 18.8-98.4 20.7-152.6H887c-3 40.9-12.6 80.6-28.5 118.3zM887 484H743.5c-1.9-54.2-9.1-105.7-20.7-152.6 29.3-15.6 56.6-34.4 81.5-55.9A373.86 373.86 0 01887 484zM658.3 165.5c39.7 16.8 75.8 40 107.6 69.2a394.72 394.72 0 01-59.4 41.8c-15.7-45-35.8-84.1-59.2-115.4 3.7 1.4 7.4 2.9 11 4.4zm-90.6 700.6c-9.2 7.2-18.4 12.7-27.7 16.4V697a389.1 389.1 0 01115.7 26.2c-8.3 24.6-17.9 47.3-29 67.8-17.4 32.4-37.8 58.3-59 75.1zm59-633.1c11 20.6 20.7 43.3 29 67.8A389.1 389.1 0 01540 327V141.6c9.2 3.7 18.5 9.1 27.7 16.4 21.2 16.7 41.6 42.6 59 75zM540 640.9V540h147.5c-1.6 44.2-7.1 87.1-16.3 127.8l-.3 1.2A445.02 445.02 0 00540 640.9zm0-156.9V383.1c45.8-2.8 89.8-12.5 130.9-28.1l.3 1.2c9.2 40.7 14.7 83.5 16.3 127.8H540zm-56 56v100.9c-45.8 2.8-89.8 12.5-130.9 28.1l-.3-1.2c-9.2-40.7-14.7-83.5-16.3-127.8H484zm-147.5-56c1.6-44.2 7.1-87.1 16.3-127.8l.3-1.2c41.1 15.6 85 25.3 130.9 28.1V484H336.5zM484 697v185.4c-9.2-3.7-18.5-9.1-27.7-16.4-21.2-16.7-41.7-42.7-59.1-75.1-11-20.6-20.7-43.3-29-67.8 37.2-14.6 75.9-23.3 115.8-26.1zm0-370a389.1 389.1 0 01-115.7-26.2c8.3-24.6 17.9-47.3 29-67.8 17.4-32.4 37.8-58.4 59.1-75.1 9.2-7.2 18.4-12.7 27.7-16.4V327zM365.7 165.5c3.7-1.5 7.3-3 11-4.4-23.4 31.3-43.5 70.4-59.2 115.4-21-12-40.9-26-59.4-41.8 31.8-29.2 67.9-52.4 107.6-69.2zM165.5 365.7c13.8-32.6 32-62.8 54.2-90.2 24.9 21.5 52.2 40.3 81.5 55.9-11.6 46.9-18.8 98.4-20.7 152.6H137c3-40.9 12.6-80.6 28.5-118.3zM137 540h143.5c1.9 54.2 9.1 105.7 20.7 152.6a444.07 444.07 0 00-81.5 55.9A373.86 373.86 0 01137 540zm228.7 318.5c-39.7-16.8-75.8-40-107.6-69.2 18.5-15.8 38.4-29.7 59.4-41.8 15.7 45 35.8 84.1 59.2 115.4-3.7-1.4-7.4-2.9-11-4.4zm292.6 0c-3.7 1.5-7.3 3-11 4.4 23.4-31.3 43.5-70.4 59.2-115.4 21 12 40.9 26 59.4 41.8a373.81 373.81 0 01-107.6 69.2z"></path></svg></span></span><span>简体中文</span></button><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="搜索" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div><button type="button" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text ant-btn-icon-only" style="margin-left:6px"><span class="ant-btn-icon"><span role="img" aria-label="sun" style="font-size:20px" class="anticon anticon-sun"><svg fill-rule="evenodd" viewBox="64 64 896 896" focusable="false" data-icon="sun" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M548 818v126a16 16 0 01-16 16h-40a16 16 0 01-16-16V818c15.85 1.64 27.84 2.46 36 2.46 8.15 0 20.16-.82 36-2.46m205.25-115.66l89.1 89.1a16 16 0 010 22.62l-28.29 28.29a16 16 0 01-22.62 0l-89.1-89.1c12.37-10.04 21.43-17.95 27.2-23.71 5.76-5.77 13.67-14.84 23.71-27.2m-482.5 0c10.04 12.36 17.95 21.43 23.71 27.2 5.77 5.76 14.84 13.67 27.2 23.71l-89.1 89.1a16 16 0 01-22.62 0l-28.29-28.29a16 16 0 010-22.63zM512 278c129.24 0 234 104.77 234 234S641.24 746 512 746 278 641.24 278 512s104.77-234 234-234m0 72c-89.47 0-162 72.53-162 162s72.53 162 162 162 162-72.53 162-162-72.53-162-162-162M206 476c-1.64 15.85-2.46 27.84-2.46 36 0 8.15.82 20.16 2.46 36H80a16 16 0 01-16-16v-40a16 16 0 0116-16zm738 0a16 16 0 0116 16v40a16 16 0 01-16 16H818c1.64-15.85 2.46-27.84 2.46-36 0-8.15-.82-20.16-2.46-36zM814.06 180.65l28.29 28.29a16 16 0 010 22.63l-89.1 89.09c-10.04-12.37-17.95-21.43-23.71-27.2-5.77-5.76-14.84-13.67-27.2-23.71l89.1-89.1a16 16 0 0122.62 0m-581.5 0l89.1 89.1c-12.37 10.04-21.43 17.95-27.2 23.71-5.76 5.77-13.67 14.84-23.71 27.2l-89.1-89.1a16 16 0 010-22.62l28.29-28.29a16 16 0 0122.62 0M532 64a16 16 0 0116 16v126c-15.85-1.64-27.84-2.46-36-2.46-8.15 0-20.16.82-36 2.46V80a16 16 0 0116-16z"></path></svg></span></span></button></div></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ROLL/zh-Hans/docs/Overview"><span title="概览" class="linkLabel_WmDU">概览</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/ROLL/zh-Hans/docs/Getting Started/Installation/image_address"><span title="快速入门" class="categoryLinkLabel_W154">快速入门</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/Getting Started/Installation/image_address"><span title="安装指南" class="categoryLinkLabel_W154">安装指南</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/Getting Started/Quick Start/aliyun_serverless_devpod_quick_start"><span title="快速开始" class="categoryLinkLabel_W154">快速开始</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/Getting Started/Debugging Guide/debug_guide"><span title="调试" class="categoryLinkLabel_W154">调试</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/Getting Started/FAQ/qa_issues"><span title="答疑" class="categoryLinkLabel_W154">答疑</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/ROLL/zh-Hans/docs/User Guides/Configuration/config_guide"><span title="使用指南" class="categoryLinkLabel_W154">使用指南</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Configuration/config_guide"><span title="配置" class="categoryLinkLabel_W154">配置</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Pipeline/agent_pipeline_start"><span title="流水线" class="categoryLinkLabel_W154">流水线</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Algorithms/GRPO"><span title="算法" class="categoryLinkLabel_W154">算法</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Agentic/Tool_Use"><span title="Agentic" class="categoryLinkLabel_W154">Agentic</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/async_parallel_rollout"><span title="高级特性" class="categoryLinkLabel_W154">高级特性</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/async_parallel_rollout"><span title="Agentic 异步并行 Rollout" class="linkLabel_WmDU">Agentic 异步并行 Rollout</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/async_training"><span title="ROLL 异步训练功能使用指南" class="linkLabel_WmDU">ROLL 异步训练功能使用指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/checkpoint_and_resume"><span title="检查点保存与恢复指南" class="linkLabel_WmDU">检查点保存与恢复指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/dynamic_batching"><span title="ROLL Dynamic Batching" class="linkLabel_WmDU">ROLL Dynamic Batching</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/megatron_convert_2_hf"><span title="MCoreAdapter 模型转换为 Hugging Face 格式" class="linkLabel_WmDU">MCoreAdapter 模型转换为 Hugging Face 格式</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/offload_reload_control"><span title="GPU 时分复用控制指南" class="linkLabel_WmDU">GPU 时分复用控制指南</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/sequence_packing"><span title="ROLL SEQUENCE PACKING" class="linkLabel_WmDU">ROLL SEQUENCE PACKING</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Tracker &amp; Metrics/trackers_and_metrics"><span title="Tracker 和 Metrics" class="categoryLinkLabel_W154">Tracker 和 Metrics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/User Guides/Hardware Support/ascend_usage"><span title="硬件支持" class="categoryLinkLabel_W154">硬件支持</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/ROLL/zh-Hans/docs/Development/Architecture/AgenticPipeline"><span title="开发" class="categoryLinkLabel_W154">开发</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/Development/Architecture/AgenticPipeline"><span title="架构" class="categoryLinkLabel_W154">架构</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/zh-Hans/docs/Development/Developer Guide/support_new_models"><span title="开发者指南" class="categoryLinkLabel_W154">开发者指南</span></a></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/ROLL/zh-Hans/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">使用指南</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">高级特性</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">ROLL SEQUENCE PACKING</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><header><h1>ROLL SEQUENCE PACKING</h1></header>
<p>ROLL框架目前支持了Sequence Packing功能，通过句子打包来避免pad token，提高计算效率。本文档详细介绍该功能的实现思路以及相应使用配置方法。</p>
<blockquote>
<p><strong>注意</strong>：目前只有 <code>megatron_strategy</code> 支持了 <code>sequence_packing</code>。</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-简介">1. 简介<a href="#1-简介" class="hash-link" aria-label="1. 简介的直接链接" title="1. 简介的直接链接" translate="no">​</a></h2>
<p>在RL训练场景中，rollout数据的分布通常具有长尾效应。而在常规的训练过程中，我们通常将一个micro batch的数据组合为一个batch进行训练，每条样本都会被pad到预设的最大长度，这不仅导致了算力被消耗在了大量pad token上，而且拖慢了训练速度。</p>
<p>为了解决上面的问题，ROLL中提供了Sequence Packing这一特性，其核心思路是：</p>
<ul>
<li class="">将当前micro batch中长短不同的句子打包在一起以消除pad token</li>
<li class="">使用打包算法优化打包效率，减少micro batch数量，提高训练效率</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-实现原理">2. 实现原理<a href="#2-实现原理" class="hash-link" aria-label="2. 实现原理的直接链接" title="2. 实现原理的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="21-数据划分层次结构">2.1 数据划分层次结构<a href="#21-数据划分层次结构" class="hash-link" aria-label="2.1 数据划分层次结构的直接链接" title="2.1 数据划分层次结构的直接链接" translate="no">​</a></h3>
<p>在分布式训练中，数据按照以下层次结构进行划分：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">GLOBAL BATCH (全局批次)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── DP RANK 0 → BATCH 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── MINI BATCH 0 (用于一次梯度更新)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├── MICRO BATCH 0 (最小计算单元)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├── MICRO BATCH 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       └── ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── DP RANK 1 → BATCH 1  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── MINI BATCH 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├── MICRO BATCH 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       └── ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── ...</span><br></span></code></pre></div></div>
<ul>
<li class=""><strong>GLOBAL BATCH</strong>: actor_infer产生的完整rollout结果</li>
<li class=""><strong>BATCH</strong>: Global Batch按DP rank划分后的子集</li>
<li class=""><strong>MINI BATCH</strong>: Batch中用于单次梯度更新的数据（考虑gradient accumulation）</li>
<li class=""><strong>MICRO BATCH</strong>: Mini Batch进一步划分的最小计算单元，参与单次forward/backward</li>
</ul>
<p>在常规训练中，每个micro batch中的样本都会被padding到固定长度，造成大量计算资源浪费。Sequence Packing通过在micro batch级别进行序列打包来解决这个问题。</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="22-序列打包核心机制">2.2 序列打包核心机制<a href="#22-序列打包核心机制" class="hash-link" aria-label="2.2 序列打包核心机制的直接链接" title="2.2 序列打包核心机制的直接链接" translate="no">​</a></h3>
<p>Sequence Packing的核心目标是在消除padding token的同时，确保在复杂的分布式训练环境下（特别是Context Parallel和Tensor Parallel）能够正确、高效地运行。为了实现这一目标，打包过程需要满足特定的对齐要求，这些要求直接关系到模型能否正常训练以及训练效率的高低。</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="221-对齐要求2cp_sizetp_size的倍数">2.2.1 对齐要求：2×CP_SIZE×TP_SIZE的倍数<a href="#221-对齐要求2cp_sizetp_size的倍数" class="hash-link" aria-label="2.2.1 对齐要求：2×CP_SIZE×TP_SIZE的倍数的直接链接" title="2.2.1 对齐要求：2×CP_SIZE×TP_SIZE的倍数的直接链接" translate="no">​</a></h4>
<p>在启用Context Parallel (CP) 和 Tensor Parallel (TP) 的情况下，序列长度必须是 <strong>2 × CP_SIZE × TP_SIZE</strong> 的倍数。</p>
<p>这个对齐要求来源于两个并行策略的需求：</p>
<ol>
<li class="">
<p><strong>TENSOR PARALLEL (TP) 需求</strong>：当启用Sequence Parallel时，序列会在forward过程中被切分到不同的TP rank上处理，因此序列长度需要能被TP_SIZE整除。</p>
</li>
<li class="">
<p><strong>CONTEXT PARALLEL (CP) 需求</strong>：为了实现CP负载均衡，序列需要被切分为2×CP_SIZE个逻辑块，因此序列长度需要能被2×CP_SIZE整除。</p>
</li>
</ol>
<p>综合这两个需求，序列长度必须是 <strong>2 × CP_SIZE × TP_SIZE</strong> 的倍数，这样才能同时满足TP和CP的正确运行要求。</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="222-为什么需要因子2cp负载均衡详解">2.2.2 为什么需要因子2？CP负载均衡详解<a href="#222-为什么需要因子2cp负载均衡详解" class="hash-link" aria-label="2.2.2 为什么需要因子2？CP负载均衡详解的直接链接" title="2.2.2 为什么需要因子2？CP负载均衡详解的直接链接" translate="no">​</a></h4>
<p>在Context Parallel (CP) 训练中，因果注意力机制的特殊性会导致严重的负载不均衡问题。</p>
<p><strong>问题根源 - 因果注意力的不对称性</strong></p>
<p>考虑一个长度为6的序列 <code>[0, 1, 2, 3, 4, 5]</code>，在CP=2的情况下：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">完整的因果注意力掩码:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     0  1  2  3  4  5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">0  [ 1  0  0  0  0  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1  [ 1  1  0  0  0  0 ]  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2  [ 1  1  1  0  0  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3  [ 1  1  1  1  0  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">4  [ 1  1  1  1  1  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">5  [ 1  1  1  1  1  1 ]</span><br></span></code></pre></div></div>
<p><strong>朴素切分方案的问题</strong>：</p>
<p>如果简单地将序列均分为两部分：</p>
<ul>
<li class="">CP0负责: <code>[0, 1, 2]</code></li>
<li class="">CP1负责: <code>[3, 4, 5]</code></li>
</ul>
<p>那么实际的计算负载为：</p>
<ul>
<li class=""><strong>CP0</strong>: 只需要计算自己负责位置的注意力权重（6个权重计算）</li>
<li class=""><strong>CP1</strong>: 需要计算自己负责位置对所有前面位置的注意力权重（15个权重计算）</li>
</ul>
<p><strong>负载比例: 6:15 = 2:5</strong>，CP1的计算量是CP0的2.5倍！</p>
<p><strong>解决方案 - 2×CP交错切分</strong></p>
<p>Megatron-Core采用的解决方案是将序列切分为 <strong>2×CP</strong> 个块，然后采用交错分 配策略：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">原始序列: [0, 1, 2, 3, 4, 5]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">切分为4块: |[0,1]|[2,3]|[4,5]|[p,p]|  (需要padding到4的倍数)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">交错分配:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- 块0 [0,1] → CP0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- 块1 [2,3] → CP1  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- 块2 [4,5] → CP1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- 块3 [p,p] → CP0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">最终分配:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- CP0: [0,1] + [p,p]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- CP1: [2,3] + [4,5]</span><br></span></code></pre></div></div>
<p>通过这种精心设计的分配策略，两个CP rank的计算负载变得相对均衡，避免了明显的性能瓶颈。</p>
<p>因此，<strong>因子2是CP负载均衡的核心设计</strong>，确保在因果注意力机制下各个CP rank的工作量基 本相等。</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="223-完整打包示例">2.2.3 完整打包示例<a href="#223-完整打包示例" class="hash-link" aria-label="2.2.3 完整打包示例的直接链接" title="2.2.3 完整打包示例的直接链接" translate="no">​</a></h4>
<p>假设当前microbatch包含以下样本（原始序列长度为8）：</p>
<table><thead><tr><th>样本ID</th><th>原始序列</th><th>有效长度</th></tr></thead><tbody><tr><td>0</td><td><code>[0, 0, p, p, p, p, p, p]</code></td><td>2</td></tr><tr><td>1</td><td><code>[1, 1, 1, 1, p, p, p, p]</code></td><td>4</td></tr><tr><td>2</td><td><code>[2, 2, 2, 2, 2, 2, p, p]</code></td><td>6</td></tr><tr><td>3</td><td><code>[3, p, p, p, p, p, p, p]</code></td><td>1</td></tr></tbody></table>
<p>配置参数：<code>CP_SIZE=2</code>, <code>TP_SIZE=1</code></p>
<p><strong>步骤1：移除原始padding</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本0: [0, 0]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本1: [1, 1, 1, 1]  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本2: [2, 2, 2, 2, 2, 2]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本3: [3]</span><br></span></code></pre></div></div>
<p><strong>步骤2：重新padding到对齐边界</strong></p>
<ul>
<li class="">对齐因子 = 2 × CP_SIZE × TP_SIZE = 2 × 2 × 1 = 4</li>
</ul>
<p>重新padding后的序列：</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本0: [0, 0, p, p] → 长度4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本1: [1, 1, 1, 1] → 长度4  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本2: [2, 2, 2, 2, 2, 2, p, p] → 长度8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">样本3: [3, p, p, p] → 长度4</span><br></span></code></pre></div></div>
<p><strong>步骤3：CP切分详细过程</strong></p>
<p>在CP_SIZE=2的情况下，每个序列会被逻辑上切分为 <strong>2×CP_SIZE = 4</strong> 个部分，然后按照交错规则分配给不同的CP rank。</p>
<p>具体切分和分配规则如下：</p>
<p>对于任意长度为L的序列，在CP_SIZE=2时：</p>
<ul>
<li class="">序列被划分为4个连续的段：段0、段1、段2、段3</li>
<li class="">每个段的长度为 L/4</li>
<li class="">分配规则：<!-- -->
<ul>
<li class=""><strong>CP0</strong>: 段0 + 段3</li>
<li class=""><strong>CP1</strong>: 段1 + 段2</li>
</ul>
</li>
</ul>
<p>应用到我们的例子：</p>
<ul>
<li class="">
<p><strong>样本0</strong> <code>[0, 0, p, p]</code> (长度4):</p>
<ul>
<li class="">段0: <code>[0]</code>, 段1: <code>[0]</code>, 段2: <code>[p]</code>, 段3: <code>[p]</code></li>
<li class="">CP0获得: 段0 + 段3 = <code>[0] + [p]</code> → 实际处理 <code>[0, p]</code></li>
<li class="">CP1获得: 段1 + 段2 = <code>[0] + [p]</code> → 实际处理 <code>[0, p]</code></li>
</ul>
</li>
<li class="">
<p><strong>样本1</strong> <code>[1, 1, 1, 1]</code> (长度4):</p>
<ul>
<li class="">段0: <code>[1]</code>, 段1: <code>[1]</code>, 段2: <code>[1]</code>, 段3: <code>[1]</code></li>
<li class="">CP0获得: <code>[1] + [1]</code>  → <code>[1, 1]</code></li>
<li class="">CP1获得: <code>[1] + [1]</code> → <code>[1, 1]</code></li>
</ul>
</li>
<li class="">
<p><strong>样本2</strong> <code>[2, 2, 2, 2, 2, 2, p, p]</code> (长度8):</p>
<ul>
<li class="">段0: <code>[2, 2]</code>, 段1: <code>[2, 2]</code>, 段2: <code>[2, 2]</code>, 段3: <code>[p, p]</code></li>
<li class="">CP0获得: <code>[2, 2] + [p, p]</code> → <code>[2, 2, p, p]</code></li>
<li class="">CP1获得: <code>[2, 2] + [2, 2]</code> → <code>[2, 2, 2, 2]</code></li>
</ul>
</li>
<li class="">
<p><strong>样本3</strong> <code>[3, p, p, p]</code> (长度4):</p>
<ul>
<li class="">段0: <code>[3]</code>, 段1: <code>[p]</code>, 段2: <code>[p]</code>, 段3: <code>[p]</code></li>
<li class="">CP0获得: <code>[3] + [p]</code> → <code>[3, p]</code></li>
<li class="">CP1获得: <code>[p] + [p]</code> → <code>[p, p]</code></li>
</ul>
</li>
</ul>
<p><strong>步骤4：各CP rank的最终打包结果</strong></p>
<ul>
<li class=""><strong>CP0的完整输入</strong>: <code>[0, p, 1, 1, 2, 2, p, p, 3, p]</code></li>
<li class=""><strong>CP1的完整输入</strong>: <code>[0, p, 1, 1, 2, 2, 2, 2, p, p]</code></li>
</ul>
<p><strong>步骤5：累积序列长度计算</strong></p>
<p>Padded累积长度: <code>[0, 4, 8, 16, 20]</code></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="23-loss计算流程">2.3 LOSS计算流程<a href="#23-loss计算流程" class="hash-link" aria-label="2.3 LOSS计算流程的直接链接" title="2.3 LOSS计算流程的直接链接" translate="no">​</a></h3>
<p>在Sequence Packing模式下，loss计算需要特殊的处理流程：</p>
<ol>
<li class="">
<p><strong>模型输出解包</strong>：使用<code>_unpack_sequences</code>函数将packed的输出还原为单个序列</p>
<ul>
<li class="">根据<code>cu_seqlens_padded</code>计算每个序列在当前CP rank上的起止位置</li>
<li class=""><code>seq_starts = cu_seqlens_padded[:-1] // cp_size</code></li>
<li class=""><code>seq_ends = cu_seqlens_padded[1:] // cp_size</code></li>
</ul>
</li>
<li class="">
<p><strong>逐序列loss计算</strong>：</p>
<ul>
<li class="">对每个解包后的序列单独调用loss函数</li>
<li class="">需要将原始数据调整到对应的序列长度（使用<code>adjust_sequence_length</code>）</li>
<li class="">累加所有序列的loss值</li>
</ul>
</li>
<li class="">
<p><strong>结果聚合</strong>：</p>
<ul>
<li class="">将所有序列的loss相加得到总loss</li>
<li class="">聚合各个序列的metrics</li>
<li class="">应用loss scaling（如果启用）</li>
</ul>
</li>
</ol>
<p>这种逐序列计算的方式确保了loss计算的正确性，即使在复杂的CP+TP+packing组合场景下也能准确计算梯度。</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="24-负载均衡优化">2.4 负载均衡优化<a href="#24-负载均衡优化" class="hash-link" aria-label="2.4 负载均衡优化的直接链接" title="2.4 负载均衡优化的直接链接" translate="no">​</a></h3>
<p>为了最大化Sequence Packing的效果，ROLL在多个层面应用了<strong>Karmarkar-Karp算法</strong>进行负载均衡优化。</p>
<p><strong>Karmarkar-Karp算法简介</strong>：
这是一种经典的多路划分算法，用于将一组数字划分为k个子集，使得各子集的和尽可能接近。在Sequence Packing场景中，该算法被用来确保各个计算单元的负载相对均衡，避免性能瓶颈。</p>
<p>主要优化包括：</p>
<ul>
<li class=""><strong>GLOBAL BATCH → DP RANK 负载均衡</strong>：确保每个DP rank获得相似的总token数量</li>
<li class=""><strong>MINI BATCH → MICRO BATCH 负载均衡</strong>：确保每个micro batch的计算负载均衡</li>
</ul>
<p>具体的实现细节和责任分工请参考第3.2节。</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-实现流程">3. 实现流程<a href="#3-实现流程" class="hash-link" aria-label="3. 实现流程的直接链接" title="3. 实现流程的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="31-打包与解包核心逻辑">3.1 打包与解包核心逻辑<a href="#31-打包与解包核心逻辑" class="hash-link" aria-label="3.1 打包与解包核心逻辑的直接链接" title="3.1 打包与解包核心逻辑的直接链接" translate="no">​</a></h3>
<p>pack部分主要是在strategy中进行处理的，开启<code>use_sequence_packing</code>后strategy会自动对microbatch进行pack，并对输出的logits进行unpack并计算loss。</p>
<p><strong>核心打包函数 <code>_pack_sequences</code></strong> 实现了以下逻辑：</p>
<ol>
<li class="">移除原始padding，提取有效token</li>
<li class="">计算累积序列长度（原始和padded版本）</li>
<li class="">重新padding到<code>2*cp_size*tp_size</code>的倍数</li>
<li class="">处理CP切分和分配</li>
<li class="">拼接序列并创建<code>PackedSeqParams</code></li>
</ol>
<p><strong>Loss计算</strong>通过<code>loss_wrapper</code>实现解包和逐序列loss计算。</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="32-负载均衡责任分工">3.2 负载均衡责任分工<a href="#32-负载均衡责任分工" class="hash-link" aria-label="3.2 负载均衡责任分工的直接链接" title="3.2 负载均衡责任分工的直接链接" translate="no">​</a></h3>
<p>负载均衡在ROLL框架中有明确的责任分工：</p>
<ol>
<li class="">
<p><strong>GLOBAL BATCH → DP RANK 负载均衡</strong>：</p>
<ul>
<li class=""><strong>负责模块</strong>: Pipeline层（<code>batch_balance</code>函数）</li>
<li class=""><strong>优化目标</strong>: 确保每个DP rank获得相似的总token数量</li>
<li class=""><strong>实现方式</strong>: 在数据分发前使用Karmarkar-Karp算法重排序</li>
</ul>
</li>
<li class="">
<p><strong>MINI BATCH → MICRO BATCH 负载均衡</strong>：</p>
<ul>
<li class=""><strong>负责模块</strong>: Strategy层（<code>make_micro_batch_iter_for_sequence_packing</code>）</li>
<li class=""><strong>优化目标</strong>: 确保每个micro batch的计算负载均衡</li>
<li class=""><strong>实现方式</strong>: 在micro batch生成时应用Karmarkar-Karp算法</li>
</ul>
</li>
<li class="">
<p><strong>随机性保留</strong>：</p>
<ul>
<li class="">Batch → Mini Batch的划分保持随机性（用于shuffle），因此不进行负载均衡优化</li>
</ul>
</li>
</ol>
<p>这种分层优化策略确保了从全局到局部的各个层面都能获得良好的负载均衡，最大化硬件利用率。</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-参数配置">4. 参数配置<a href="#4-参数配置" class="hash-link" aria-label="4. 参数配置的直接链接" title="4. 参数配置的直接链接" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="41-如何启用sequence-packing">4.1 如何启用SEQUENCE PACKING<a href="#41-如何启用sequence-packing" class="hash-link" aria-label="4.1 如何启用SEQUENCE PACKING的直接链接" title="4.1 如何启用SEQUENCE PACKING的直接链接" translate="no">​</a></h3>
<p>要使用Sequence Packing功能，只需要在配置文件中设置 <code>use_sequence_packing: true</code> 即可。</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="42-配置参数详解通俗版">4.2 配置参数详解（通俗版）<a href="#42-配置参数详解通俗版" class="hash-link" aria-label="4.2 配置参数详解（通俗版）的直接链接" title="4.2 配置参数详解（通俗版）的直接链接" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="algorithm打包算法"><code>algorithm</code>（打包算法）<a href="#algorithm打包算法" class="hash-link" aria-label="algorithm打包算法的直接链接" title="algorithm打包算法的直接链接" translate="no">​</a></h4>
<ul>
<li class=""><strong><code>none</code></strong>：默认的简单打包方式，按照数据原有的顺序进行打包</li>
<li class=""><strong><code>load_balance</code></strong>：智能负载均衡打包，会重新排列数据使得每个micro batch的计算 量更加均衡，推荐使用</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="max_packed_sequence_length_train训练时最大打包长度"><code>max_packed_sequence_length_train</code>（训练时最大打包长度）<a href="#max_packed_sequence_length_train训练时最大打包长度" class="hash-link" aria-label="max_packed_sequence_length_train训练时最大打包长度的直接链接" title="max_packed_sequence_length_train训练时最大打包长度的直接链接" translate="no">​</a></h4>
<ul>
<li class="">这个参数控制在训练时，打包后的序列最长可以有多长</li>
<li class="">比如设置为8192，意味着打包后的序列总长度不会超过8192个token</li>
<li class="">设置合理的值可以避免内存溢出，同时保证打包效率</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="max_packed_sequence_length_forward推理时最大打包长度"><code>max_packed_sequence_length_forward</code>（推理时最大打包长度）<a href="#max_packed_sequence_length_forward推理时最大打包长度" class="hash-link" aria-label="max_packed_sequence_length_forward推理时最大打包长度的直接链接" title="max_packed_sequence_length_forward推理时最大打包长度的直接链接" translate="no">​</a></h4>
<ul>
<li class="">和训练时的参数类似，但专门用于推理阶段</li>
<li class="">通常可以和训练时设置相同的值</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="min_num_micro_batches_train训练时最少micro-batch数量"><code>min_num_micro_batches_train</code>（训练时最少micro batch数量）<a href="#min_num_micro_batches_train训练时最少micro-batch数量" class="hash-link" aria-label="min_num_micro_batches_train训练时最少micro-batch数量的直接链接" title="min_num_micro_batches_train训练时最少micro-batch数量的直接链接" translate="no">​</a></h4>
<ul>
<li class="">控制每个mini batch至少要分成多少个micro batch</li>
<li class="">设置为1表示不限 制，让系统自动决定最优的划分方式</li>
<li class="">如果遇到显存不足的问题，可以适当增大这个值来减少每个micro batch的大小</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="min_num_micro_batches_forward推理时最少micro-batch数量"><code>min_num_micro_batches_forward</code>（推理时最少micro batch数量）<a href="#min_num_micro_batches_forward推理时最少micro-batch数量" class="hash-link" aria-label="min_num_micro_batches_forward推理时最少micro-batch数量的直接链接" title="min_num_micro_batches_forward推理时最少micro-batch数量的直接链接" translate="no">​</a></h4>
<ul>
<li class="">和训练时的参数类似，但用于推理阶段</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="43-完整配置示例">4.3 完整配置示例<a href="#43-完整配置示例" class="hash-link" aria-label="4.3 完整配置示例的直接链接" title="4.3 完整配置示例的直接链接" translate="no">​</a></h3>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">actor_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># 启用sequence packing功能</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">use_sequence_packing</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token boolean important">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># sequence packing的具体配置</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">sequence_packing_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 使用负载均衡算法，效果更好</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">algorithm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> load_balance</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 训练时打包后的最大序列长度为8192</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">max_packed_sequence_length_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">8192</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 推理时打包后的最大序列长度为8192  </span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">max_packed_sequence_length_forward</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">8192</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 训练时最少分成1个micro batch（即不限制）</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">min_num_micro_batches_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 推理时最少分成1个micro batch</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">min_num_micro_batches_forward</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># 必须使用megatron策略才能支持sequence packing</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> megatron_train</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="44-使用建议">4.4 使用建议<a href="#44-使用建议" class="hash-link" aria-label="4.4 使用建议的直接链接" title="4.4 使用建议的直接链接" translate="no">​</a></h3>
<ol>
<li class=""><strong>必选条件</strong>：只能在<code>megatron_train</code>或<code>megatron_infer</code>策略下使用</li>
<li class=""><strong>推荐配置</strong>：建议使用<code>load_balance</code>算法，可以获得更好的性能</li>
<li class=""><strong>长度设置</strong>：<code>max_packed_sequence_length</code>应该根据你的GPU显存大小来调整，一般可以设置为模型支持的最大序列长度
4<strong>自定义Loss函数</strong>：如果是自定义loss func使用sequence packing的话，请参考自定义loss func文档，确保正确设置了<code>apply_loss_scale</code>参数</li>
</ol>
<p>通过合理配置Sequence Packing，可以在保持模型性能的同时显著提升训练效率，特别是在处理变长序列的强化学习场景中效果尤为明显。</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Advanced Features/sequence_packing.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">最后<!-- -->于 <b><time datetime="2026-02-09T13:05:30.000Z" itemprop="dateModified">2026年2月9日</time></b> <!-- -->更新</span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/ROLL/zh-Hans/docs/User Guides/Advanced Features/offload_reload_control"><div class="pagination-nav__sublabel">上一页</div><div class="pagination-nav__label">GPU 时分复用控制指南</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ROLL/zh-Hans/docs/User Guides/Tracker &amp; Metrics/trackers_and_metrics"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">tracker和metrics</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-简介" class="table-of-contents__link toc-highlight">1. 简介</a></li><li><a href="#2-实现原理" class="table-of-contents__link toc-highlight">2. 实现原理</a><ul><li><a href="#21-数据划分层次结构" class="table-of-contents__link toc-highlight">2.1 数据划分层次结构</a></li><li><a href="#22-序列打包核心机制" class="table-of-contents__link toc-highlight">2.2 序列打包核心机制</a></li><li><a href="#23-loss计算流程" class="table-of-contents__link toc-highlight">2.3 LOSS计算流程</a></li><li><a href="#24-负载均衡优化" class="table-of-contents__link toc-highlight">2.4 负载均衡优化</a></li></ul></li><li><a href="#3-实现流程" class="table-of-contents__link toc-highlight">3. 实现流程</a><ul><li><a href="#31-打包与解包核心逻辑" class="table-of-contents__link toc-highlight">3.1 打包与解包核心逻辑</a></li><li><a href="#32-负载均衡责任分工" class="table-of-contents__link toc-highlight">3.2 负载均衡责任分工</a></li></ul></li><li><a href="#4-参数配置" class="table-of-contents__link toc-highlight">4. 参数配置</a><ul><li><a href="#41-如何启用sequence-packing" class="table-of-contents__link toc-highlight">4.1 如何启用SEQUENCE PACKING</a></li><li><a href="#42-配置参数详解通俗版" class="table-of-contents__link toc-highlight">4.2 配置参数详解（通俗版）</a></li><li><a href="#43-完整配置示例" class="table-of-contents__link toc-highlight">4.3 完整配置示例</a></li><li><a href="#44-使用建议" class="table-of-contents__link toc-highlight">4.4 使用建议</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Examples</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ROLL/zh-Hans/docs/Getting Started/Quick Start/single_node_quick_start">ROLL单机实践手册</a></li><li class="footer__item"><a class="footer__link-item" href="/ROLL/zh-Hans/docs/User Guides/Configuration/config_guide">配置指南</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Alibaba.</div></div></div></footer></div>
</body>
</html>