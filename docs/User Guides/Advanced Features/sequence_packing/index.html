<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-User Guides/Advanced Features/sequence_packing" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">SEQUENCE PACKING IN ROLL | ROLL</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://alibaba.github.io/ROLL/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://alibaba.github.io/ROLL/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://alibaba.github.io/ROLL/docs/User Guides/Advanced Features/sequence_packing"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" property="og:locale:alternate" content="zh_Hans"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="SEQUENCE PACKING IN ROLL | ROLL"><meta data-rh="true" name="description" content="The ROLL framework now supports Sequence Packing, a feature that eliminates padding tokens by packing variable-length sequences together, thereby improving computational efficiency. This document provides a detailed explanation of the implementation rationale and configuration methods for this feature."><meta data-rh="true" property="og:description" content="The ROLL framework now supports Sequence Packing, a feature that eliminates padding tokens by packing variable-length sequences together, thereby improving computational efficiency. This document provides a detailed explanation of the implementation rationale and configuration methods for this feature."><link data-rh="true" rel="icon" href="https://img.alicdn.com/imgextra/i4/O1CN01bo6EZl2192CAIjFwE_!!6000000006941-2-tps-465-367.png"><link data-rh="true" rel="canonical" href="https://alibaba.github.io/ROLL/docs/User Guides/Advanced Features/sequence_packing"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/User Guides/Advanced Features/sequence_packing" hreflang="en"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/zh-Hans/docs/User Guides/Advanced Features/sequence_packing" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/User Guides/Advanced Features/sequence_packing" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"SEQUENCE PACKING IN ROLL","item":"https://alibaba.github.io/ROLL/docs/User Guides/Advanced Features/sequence_packing"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D6R4GXHVFP"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-D6R4GXHVFP",{anonymize_ip:!0})</script><link rel="stylesheet" href="/ROLL/assets/css/styles.4016bf18.css">
<script src="/ROLL/assets/js/runtime~main.f1a23ec3.js" defer="defer"></script>
<script src="/ROLL/assets/js/main.d5b57526.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"dark"),document.documentElement.setAttribute("data-theme-choice",t||"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbar_MONK"><div class="navbar__inner"><div class="logoWrap_HHlA navbar__items"><div class="logo_Ufb2"><div class="ant-image css-plsjn" style="width:40px;height:32px"><img alt="ROLL" class="ant-image-img css-plsjn" style="height:32px" src="https://img.alicdn.com/imgextra/i3/O1CN016Mlxas1MHNA3NEbZ0_!!6000000001409-2-tps-465-367.png" width="40" height="32"></div></div><div><div class="title_E_95">ROLL</div><div class="subTitle_M9Ik">like a Reinforcement Learning Algorithm Developer</div></div></div><div class="navbar__items navbar__items--right"><a href="/ROLL/" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false">Home</a><a href="/ROLL/#core" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false">Core Algorithms</a><a href="/ROLL/#research" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false">Research Community</a><a href="/ROLL/docs/Overview" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3 primary_vbUC" tabindex="0" aria-disabled="false">API Docs</a><a href="https://github.com/alibaba/ROLL" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false"><span>Github</span><span role="img" aria-label="export" class="anticon anticon-export"><svg fill-rule="evenodd" viewBox="64 64 896 896" focusable="false" data-icon="export" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M880 912H144c-17.7 0-32-14.3-32-32V144c0-17.7 14.3-32 32-32h360c4.4 0 8 3.6 8 8v56c0 4.4-3.6 8-8 8H184v656h656V520c0-4.4 3.6-8 8-8h56c4.4 0 8 3.6 8 8v360c0 17.7-14.3 32-32 32zM770.87 199.13l-52.2-52.2a8.01 8.01 0 014.7-13.6l179.4-21c5.1-.6 9.5 3.7 8.9 8.9l-21 179.4c-.8 6.6-8.9 9.4-13.6 4.7l-52.4-52.4-256.2 256.2a8.03 8.03 0 01-11.3 0l-42.4-42.4a8.03 8.03 0 010-11.3l256.1-256.3z"></path></svg></span></a><button type="button" class="ant-btn css-plsjn ant-btn-default ant-btn-color-default ant-btn-variant-outlined ant-dropdown-trigger language_XaW5"><span class="ant-btn-icon"><span role="img" aria-label="global" class="anticon anticon-global"><svg viewBox="64 64 896 896" focusable="false" data-icon="global" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M854.4 800.9c.2-.3.5-.6.7-.9C920.6 722.1 960 621.7 960 512s-39.4-210.1-104.8-288c-.2-.3-.5-.5-.7-.8-1.1-1.3-2.1-2.5-3.2-3.7-.4-.5-.8-.9-1.2-1.4l-4.1-4.7-.1-.1c-1.5-1.7-3.1-3.4-4.6-5.1l-.1-.1c-3.2-3.4-6.4-6.8-9.7-10.1l-.1-.1-4.8-4.8-.3-.3c-1.5-1.5-3-2.9-4.5-4.3-.5-.5-1-1-1.6-1.5-1-1-2-1.9-3-2.8-.3-.3-.7-.6-1-1C736.4 109.2 629.5 64 512 64s-224.4 45.2-304.3 119.2c-.3.3-.7.6-1 1-1 .9-2 1.9-3 2.9-.5.5-1 1-1.6 1.5-1.5 1.4-3 2.9-4.5 4.3l-.3.3-4.8 4.8-.1.1c-3.3 3.3-6.5 6.7-9.7 10.1l-.1.1c-1.6 1.7-3.1 3.4-4.6 5.1l-.1.1c-1.4 1.5-2.8 3.1-4.1 4.7-.4.5-.8.9-1.2 1.4-1.1 1.2-2.1 2.5-3.2 3.7-.2.3-.5.5-.7.8C103.4 301.9 64 402.3 64 512s39.4 210.1 104.8 288c.2.3.5.6.7.9l3.1 3.7c.4.5.8.9 1.2 1.4l4.1 4.7c0 .1.1.1.1.2 1.5 1.7 3 3.4 4.6 5l.1.1c3.2 3.4 6.4 6.8 9.6 10.1l.1.1c1.6 1.6 3.1 3.2 4.7 4.7l.3.3c3.3 3.3 6.7 6.5 10.1 9.6 80.1 74 187 119.2 304.5 119.2s224.4-45.2 304.3-119.2a300 300 0 0010-9.6l.3-.3c1.6-1.6 3.2-3.1 4.7-4.7l.1-.1c3.3-3.3 6.5-6.7 9.6-10.1l.1-.1c1.5-1.7 3.1-3.3 4.6-5 0-.1.1-.1.1-.2 1.4-1.5 2.8-3.1 4.1-4.7.4-.5.8-.9 1.2-1.4a99 99 0 003.3-3.7zm4.1-142.6c-13.8 32.6-32 62.8-54.2 90.2a444.07 444.07 0 00-81.5-55.9c11.6-46.9 18.8-98.4 20.7-152.6H887c-3 40.9-12.6 80.6-28.5 118.3zM887 484H743.5c-1.9-54.2-9.1-105.7-20.7-152.6 29.3-15.6 56.6-34.4 81.5-55.9A373.86 373.86 0 01887 484zM658.3 165.5c39.7 16.8 75.8 40 107.6 69.2a394.72 394.72 0 01-59.4 41.8c-15.7-45-35.8-84.1-59.2-115.4 3.7 1.4 7.4 2.9 11 4.4zm-90.6 700.6c-9.2 7.2-18.4 12.7-27.7 16.4V697a389.1 389.1 0 01115.7 26.2c-8.3 24.6-17.9 47.3-29 67.8-17.4 32.4-37.8 58.3-59 75.1zm59-633.1c11 20.6 20.7 43.3 29 67.8A389.1 389.1 0 01540 327V141.6c9.2 3.7 18.5 9.1 27.7 16.4 21.2 16.7 41.6 42.6 59 75zM540 640.9V540h147.5c-1.6 44.2-7.1 87.1-16.3 127.8l-.3 1.2A445.02 445.02 0 00540 640.9zm0-156.9V383.1c45.8-2.8 89.8-12.5 130.9-28.1l.3 1.2c9.2 40.7 14.7 83.5 16.3 127.8H540zm-56 56v100.9c-45.8 2.8-89.8 12.5-130.9 28.1l-.3-1.2c-9.2-40.7-14.7-83.5-16.3-127.8H484zm-147.5-56c1.6-44.2 7.1-87.1 16.3-127.8l.3-1.2c41.1 15.6 85 25.3 130.9 28.1V484H336.5zM484 697v185.4c-9.2-3.7-18.5-9.1-27.7-16.4-21.2-16.7-41.7-42.7-59.1-75.1-11-20.6-20.7-43.3-29-67.8 37.2-14.6 75.9-23.3 115.8-26.1zm0-370a389.1 389.1 0 01-115.7-26.2c8.3-24.6 17.9-47.3 29-67.8 17.4-32.4 37.8-58.4 59.1-75.1 9.2-7.2 18.4-12.7 27.7-16.4V327zM365.7 165.5c3.7-1.5 7.3-3 11-4.4-23.4 31.3-43.5 70.4-59.2 115.4-21-12-40.9-26-59.4-41.8 31.8-29.2 67.9-52.4 107.6-69.2zM165.5 365.7c13.8-32.6 32-62.8 54.2-90.2 24.9 21.5 52.2 40.3 81.5 55.9-11.6 46.9-18.8 98.4-20.7 152.6H137c3-40.9 12.6-80.6 28.5-118.3zM137 540h143.5c1.9 54.2 9.1 105.7 20.7 152.6a444.07 444.07 0 00-81.5 55.9A373.86 373.86 0 01137 540zm228.7 318.5c-39.7-16.8-75.8-40-107.6-69.2 18.5-15.8 38.4-29.7 59.4-41.8 15.7 45 35.8 84.1 59.2 115.4-3.7-1.4-7.4-2.9-11-4.4zm292.6 0c-3.7 1.5-7.3 3-11 4.4 23.4-31.3 43.5-70.4 59.2-115.4 21 12 40.9 26 59.4 41.8a373.81 373.81 0 01-107.6 69.2z"></path></svg></span></span><span>English</span></button><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div><button type="button" class="ant-btn css-plsjn ant-btn-text ant-btn-color-default ant-btn-variant-text ant-btn-icon-only" style="margin-left:6px"><span class="ant-btn-icon"><span role="img" aria-label="sun" style="font-size:20px" class="anticon anticon-sun"><svg fill-rule="evenodd" viewBox="64 64 896 896" focusable="false" data-icon="sun" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M548 818v126a16 16 0 01-16 16h-40a16 16 0 01-16-16V818c15.85 1.64 27.84 2.46 36 2.46 8.15 0 20.16-.82 36-2.46m205.25-115.66l89.1 89.1a16 16 0 010 22.62l-28.29 28.29a16 16 0 01-22.62 0l-89.1-89.1c12.37-10.04 21.43-17.95 27.2-23.71 5.76-5.77 13.67-14.84 23.71-27.2m-482.5 0c10.04 12.36 17.95 21.43 23.71 27.2 5.77 5.76 14.84 13.67 27.2 23.71l-89.1 89.1a16 16 0 01-22.62 0l-28.29-28.29a16 16 0 010-22.63zM512 278c129.24 0 234 104.77 234 234S641.24 746 512 746 278 641.24 278 512s104.77-234 234-234m0 72c-89.47 0-162 72.53-162 162s72.53 162 162 162 162-72.53 162-162-72.53-162-162-162M206 476c-1.64 15.85-2.46 27.84-2.46 36 0 8.15.82 20.16 2.46 36H80a16 16 0 01-16-16v-40a16 16 0 0116-16zm738 0a16 16 0 0116 16v40a16 16 0 01-16 16H818c1.64-15.85 2.46-27.84 2.46-36 0-8.15-.82-20.16-2.46-36zM814.06 180.65l28.29 28.29a16 16 0 010 22.63l-89.1 89.09c-10.04-12.37-17.95-21.43-23.71-27.2-5.77-5.76-14.84-13.67-27.2-23.71l89.1-89.1a16 16 0 0122.62 0m-581.5 0l89.1 89.1c-12.37 10.04-21.43 17.95-27.2 23.71-5.76 5.77-13.67 14.84-23.71 27.2l-89.1-89.1a16 16 0 010-22.62l28.29-28.29a16 16 0 0122.62 0M532 64a16 16 0 0116 16v126c-15.85-1.64-27.84-2.46-36-2.46-8.15 0-20.16.82-36 2.46V80a16 16 0 0116-16z"></path></svg></span></span></button></div></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ROLL/docs/Overview"><span title="Overview" class="linkLabel_WmDU">Overview</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/ROLL/docs/Getting Started/Installation/image_address"><span title="Getting Started" class="categoryLinkLabel_W154">Getting Started</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/Getting Started/Installation/image_address"><span title="Installation" class="categoryLinkLabel_W154">Installation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/Getting Started/Quick Start/aliyun_serverless_devpod_quick_start"><span title="Quick Start" class="categoryLinkLabel_W154">Quick Start</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/Getting Started/Debugging Guide/debug_guide"><span title="Debugging Guide" class="categoryLinkLabel_W154">Debugging Guide</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/Getting Started/FAQ/qa_issues"><span title="FAQ" class="categoryLinkLabel_W154">FAQ</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/ROLL/docs/User Guides/Configuration/config_guide"><span title="User Guides" class="categoryLinkLabel_W154">User Guides</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/User Guides/Configuration/config_guide"><span title="Configuration" class="categoryLinkLabel_W154">Configuration</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/User Guides/Pipeline/agent_pipeline_start"><span title="Pipeline" class="categoryLinkLabel_W154">Pipeline</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/User Guides/Algorithms/GRPO"><span title="Algorithms" class="categoryLinkLabel_W154">Algorithms</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/User Guides/Agentic/Tool_Use"><span title="Agentic" class="categoryLinkLabel_W154">Agentic</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/async_parallel_rollout"><span title="Advanced Features" class="categoryLinkLabel_W154">Advanced Features</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/async_parallel_rollout"><span title="Agentic Asynchronous Parallel Rollout" class="linkLabel_WmDU">Agentic Asynchronous Parallel Rollout</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/async_training"><span title="ROLL Asynchronous Training User Guide" class="linkLabel_WmDU">ROLL Asynchronous Training User Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/checkpoint_and_resume"><span title="Checkpoint Saving and Resuming Guide" class="linkLabel_WmDU">Checkpoint Saving and Resuming Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/dynamic_batching"><span title="ROLL Dynamic Batching" class="linkLabel_WmDU">ROLL Dynamic Batching</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/megatron_convert_2_hf"><span title="Converting MCoreAdapter Models to Hugging Face Format" class="linkLabel_WmDU">Converting MCoreAdapter Models to Hugging Face Format</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/offload_reload_control"><span title="GPU Time-Division Multiplexing Control Guide" class="linkLabel_WmDU">GPU Time-Division Multiplexing Control Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ROLL/docs/User Guides/Advanced Features/sequence_packing"><span title="SEQUENCE PACKING IN ROLL" class="linkLabel_WmDU">SEQUENCE PACKING IN ROLL</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/User Guides/Tracker &amp; Metrics/trackers_and_metrics"><span title="Tracker &amp; Metrics" class="categoryLinkLabel_W154">Tracker &amp; Metrics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/User Guides/Hardware Support/ascend_usage"><span title="Hardware Support" class="categoryLinkLabel_W154">Hardware Support</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/ROLL/docs/Development/Architecture/AgenticPipeline"><span title="Development" class="categoryLinkLabel_W154">Development</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/Development/Architecture/AgenticPipeline"><span title="Architecture" class="categoryLinkLabel_W154">Architecture</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/Development/Developer Guide/support_new_models"><span title="Developer Guide" class="categoryLinkLabel_W154">Developer Guide</span></a></div></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ROLL/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">User Guides</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Advanced Features</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">SEQUENCE PACKING IN ROLL</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>SEQUENCE PACKING IN ROLL</h1></header>
<p>The ROLL framework now supports <strong>Sequence Packing</strong>, a feature that eliminates padding tokens by packing variable-length sequences together, thereby improving computational efficiency. This document provides a detailed explanation of the implementation rationale and configuration methods for this feature.</p>
<blockquote>
<p><strong>Note</strong>: Currently, only <code>megatron_strategy</code> supports <code>sequence_packing</code>.</p>
</blockquote>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-introduction">1. Introduction<a href="#1-introduction" class="hash-link" aria-label="Direct link to 1. Introduction" title="Direct link to 1. Introduction" translate="no">​</a></h2>
<p>In reinforcement learning (RL) training scenarios, rollout data typically exhibits a long-tailed distribution. In conventional training pipelines, samples within a micro-batch are padded to a fixed maximum sequence length before being grouped into a batch for training. This approach wastes significant computational resources on processing padding tokens and slows down training.</p>
<p>To address this issue, ROLL introduces <strong>Sequence Packing</strong>, which:</p>
<ul>
<li class="">Packs sequences of varying lengths within each micro-batch to eliminate padding tokens.</li>
<li class="">Employs optimized packing algorithms to improve packing efficiency, reduce the number of micro-batches, and accelerate training.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-implementation-principles">2. Implementation Principles<a href="#2-implementation-principles" class="hash-link" aria-label="Direct link to 2. Implementation Principles" title="Direct link to 2. Implementation Principles" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="21-data-partitioning-hierarchy">2.1 Data Partitioning Hierarchy<a href="#21-data-partitioning-hierarchy" class="hash-link" aria-label="Direct link to 2.1 Data Partitioning Hierarchy" title="Direct link to 2.1 Data Partitioning Hierarchy" translate="no">​</a></h3>
<p>In distributed training, data is organized in the following hierarchical structure:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">GLOBAL BATCH (Global Batch)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── DP RANK 0 → BATCH 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── MINI BATCH 0 (used for one gradient update)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├── MICRO BATCH 0 (smallest computation unit)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├── MICRO BATCH 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       └── ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── DP RANK 1 → BATCH 1  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── MINI BATCH 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       ├── MICRO BATCH 0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       └── ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── ...</span><br></span></code></pre></div></div>
<ul>
<li class=""><strong>GLOBAL BATCH</strong>: The complete rollout results generated by <code>actor_infer</code>.</li>
<li class=""><strong>BATCH</strong>: A subset of the Global Batch assigned to a specific Data Parallel (DP) rank.</li>
<li class=""><strong>MINI BATCH</strong>: A portion of a Batch used for a single gradient update (considering gradient accumulation).</li>
<li class=""><strong>MICRO BATCH</strong>: The smallest computational unit derived from a Mini Batch, used in a single forward/backward pass.</li>
</ul>
<p>In standard training, all samples within a micro-batch are padded to a fixed length, leading to substantial computational waste. Sequence Packing solves this by packing sequences at the micro-batch level.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="22-core-mechanism-of-sequence-packing">2.2 Core Mechanism of Sequence Packing<a href="#22-core-mechanism-of-sequence-packing" class="hash-link" aria-label="Direct link to 2.2 Core Mechanism of Sequence Packing" title="Direct link to 2.2 Core Mechanism of Sequence Packing" translate="no">​</a></h3>
<p>The primary goal of Sequence Packing is to eliminate padding tokens while ensuring correct and efficient execution under complex distributed training configurations—particularly when Context Parallelism (CP) and Tensor Parallelism (TP) are enabled. To achieve this, the packing process must satisfy specific alignment constraints critical for both correctness and performance.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="221-alignment-requirement-multiple-of-2--cp_size--tp_size">2.2.1 Alignment Requirement: Multiple of 2 × CP_SIZE × TP_SIZE<a href="#221-alignment-requirement-multiple-of-2--cp_size--tp_size" class="hash-link" aria-label="Direct link to 2.2.1 Alignment Requirement: Multiple of 2 × CP_SIZE × TP_SIZE" title="Direct link to 2.2.1 Alignment Requirement: Multiple of 2 × CP_SIZE × TP_SIZE" translate="no">​</a></h4>
<p>When Context Parallelism (CP) and Tensor Parallelism (TP) are enabled, the packed sequence length <strong>must be a multiple of <code>2 × CP_SIZE × TP_SIZE</code></strong>.</p>
<p>This requirement stems from the needs of both parallelism strategies:</p>
<ol>
<li class="">
<p><strong>TENSOR PARALLELISM (TP)</strong>: When Sequence Parallelism is enabled, sequences are split across TP ranks during the forward pass. Thus, the sequence length must be divisible by <code>TP_SIZE</code>.</p>
</li>
<li class="">
<p><strong>CONTEXT PARALLELISM (CP)</strong>: To achieve load balancing in CP, sequences must be logically divided into <code>2 × CP_SIZE</code> chunks. Hence, the sequence length must also be divisible by <code>2 × CP_SIZE</code>.</p>
</li>
</ol>
<p>Combining these two requirements, the sequence length must be a multiple of <strong><code>2 × CP_SIZE × TP_SIZE</code></strong> to ensure compatibility with both TP and CP.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="222-why-the-factor-of-2-detailed-explanation-of-cp-load-balancing">2.2.2 Why the Factor of 2? Detailed Explanation of CP Load Balancing<a href="#222-why-the-factor-of-2-detailed-explanation-of-cp-load-balancing" class="hash-link" aria-label="Direct link to 2.2.2 Why the Factor of 2? Detailed Explanation of CP Load Balancing" title="Direct link to 2.2.2 Why the Factor of 2? Detailed Explanation of CP Load Balancing" translate="no">​</a></h4>
<p>In Context Parallel (CP) training, the asymmetric nature of causal attention leads to severe load imbalance.</p>
<p><strong>Root Cause – Asymmetry in Causal Attention</strong></p>
<p>Consider a sequence of length 6: <code>[0, 1, 2, 3, 4, 5]</code>, with <code>CP=2</code>:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Full causal attention mask:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     0  1  2  3  4  5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">0  [ 1  0  0  0  0  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1  [ 1  1  0  0  0  0 ]  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2  [ 1  1  1  0  0  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3  [ 1  1  1  1  0  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">4  [ 1  1  1  1  1  0 ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">5  [ 1  1  1  1  1  1 ]</span><br></span></code></pre></div></div>
<p><strong>Problem with Naive Partitioning</strong>:</p>
<p>If the sequence is simply split evenly:</p>
<ul>
<li class="">CP0 handles: <code>[0, 1, 2]</code></li>
<li class="">CP1 handles: <code>[3, 4, 5]</code></li>
</ul>
<p>The actual computational loads become:</p>
<ul>
<li class=""><strong>CP0</strong>: Only computes attention weights for its own positions (6 weight computations).</li>
<li class=""><strong>CP1</strong>: Must compute attention weights from its positions to all preceding positions (15 weight computations).</li>
</ul>
<p><strong>Load ratio: 6:15 = 2:5</strong> — CP1 bears 2.5× more computation than CP0!</p>
<p><strong>Solution – 2×CP Interleaved Chunking</strong></p>
<p>Megatron-Core resolves this by splitting the sequence into <strong><code>2 × CP</code></strong> chunks and applying an interleaved assignment strategy:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Original sequence: [0, 1, 2, 3, 4, 5]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Split into 4 chunks: |[0,1]|[2,3]|[4,5]|[p,p]|  (padded to multiple of 4)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Interleaved assignment:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- Chunk 0 [0,1] → CP0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- Chunk 1 [2,3] → CP1  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- Chunk 2 [4,5] → CP1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- Chunk 3 [p,p] → CP0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Final assignment:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- CP0: [0,1] + [p,p]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">- CP1: [2,3] + [4,5]</span><br></span></code></pre></div></div>
<p>This carefully designed assignment balances the computational load between CP ranks, avoiding performance bottlenecks.</p>
<p>Thus, <strong>the factor of 2 is essential for CP load balancing</strong>, ensuring roughly equal workloads across CP ranks under causal attention.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="223-complete-packing-example">2.2.3 Complete Packing Example<a href="#223-complete-packing-example" class="hash-link" aria-label="Direct link to 2.2.3 Complete Packing Example" title="Direct link to 2.2.3 Complete Packing Example" translate="no">​</a></h4>
<p>Assume a micro-batch contains the following samples (original max sequence length = 8):</p>
<table><thead><tr><th>Sample ID</th><th>Original Sequence</th><th>Valid Length</th></tr></thead><tbody><tr><td>0</td><td><code>[0, 0, p, p, p, p, p, p]</code></td><td>2</td></tr><tr><td>1</td><td><code>[1, 1, 1, 1, p, p, p, p]</code></td><td>4</td></tr><tr><td>2</td><td><code>[2, 2, 2, 2, 2, 2, p, p]</code></td><td>6</td></tr><tr><td>3</td><td><code>[3, p, p, p, p, p, p, p]</code></td><td>1</td></tr></tbody></table>
<p>Configuration: <code>CP_SIZE=2</code>, <code>TP_SIZE=1</code></p>
<p><strong>Step 1: Remove original padding</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 0: [0, 0]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 1: [1, 1, 1, 1]  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 2: [2, 2, 2, 2, 2, 2]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 3: [3]</span><br></span></code></pre></div></div>
<p><strong>Step 2: Re-pad to alignment boundary</strong></p>
<ul>
<li class="">Alignment factor = 2 × CP_SIZE × TP_SIZE = 2 × 2 × 1 = 4</li>
</ul>
<p>Re-padded sequences:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 0: [0, 0, p, p] → length 4</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 1: [1, 1, 1, 1] → length 4  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 2: [2, 2, 2, 2, 2, 2, p, p] → length 8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Sample 3: [3, p, p, p] → length 4</span><br></span></code></pre></div></div>
<p><strong>Step 3: Detailed CP Chunking Process</strong></p>
<p>With <code>CP_SIZE=2</code>, each sequence is logically split into <strong><code>2 × CP_SIZE = 4</code></strong> segments and assigned via interleaving:</p>
<p>For any sequence of length L under <code>CP_SIZE=2</code>:</p>
<ul>
<li class="">Split into 4 consecutive segments: seg0, seg1, seg2, seg3</li>
<li class="">Each segment has length L/4</li>
<li class="">Assignment rule:<!-- -->
<ul>
<li class=""><strong>CP0</strong>: seg0 + seg3</li>
<li class=""><strong>CP1</strong>: seg1 + seg2</li>
</ul>
</li>
</ul>
<p>Applied to our example:</p>
<ul>
<li class="">
<p><strong>Sample 0</strong> <code>[0, 0, p, p]</code> (length 4):</p>
<ul>
<li class="">seg0: <code>[0]</code>, seg1: <code>[0]</code>, seg2: <code>[p]</code>, seg3: <code>[p]</code></li>
<li class="">CP0 gets: seg0 + seg3 = <code>[0] + [p]</code> → processes <code>[0, p]</code></li>
<li class="">CP1 gets: seg1 + seg2 = <code>[0] + [p]</code> → processes <code>[0, p]</code></li>
</ul>
</li>
<li class="">
<p><strong>Sample 1</strong> <code>[1, 1, 1, 1]</code> (length 4):</p>
<ul>
<li class="">seg0: <code>[1]</code>, seg1: <code>[1]</code>, seg2: <code>[1]</code>, seg3: <code>[1]</code></li>
<li class="">CP0: <code>[1] + [1]</code> → <code>[1, 1]</code></li>
<li class="">CP1: <code>[1] + [1]</code> → <code>[1, 1]</code></li>
</ul>
</li>
<li class="">
<p><strong>Sample 2</strong> <code>[2, 2, 2, 2, 2, 2, p, p]</code> (length 8):</p>
<ul>
<li class="">seg0: <code>[2, 2]</code>, seg1: <code>[2, 2]</code>, seg2: <code>[2, 2]</code>, seg3: <code>[p, p]</code></li>
<li class="">CP0: <code>[2, 2] + [p, p]</code> → <code>[2, 2, p, p]</code></li>
<li class="">CP1: <code>[2, 2] + [2, 2]</code> → <code>[2, 2, 2, 2]</code></li>
</ul>
</li>
<li class="">
<p><strong>Sample 3</strong> <code>[3, p, p, p]</code> (length 4):</p>
<ul>
<li class="">seg0: <code>[3]</code>, seg1: <code>[p]</code>, seg2: <code>[p]</code>, seg3: <code>[p]</code></li>
<li class="">CP0: <code>[3] + [p]</code> → <code>[3, p]</code></li>
<li class="">CP1: <code>[p] + [p]</code> → <code>[p, p]</code></li>
</ul>
</li>
</ul>
<p><strong>Step 4: Final Packed Input per CP Rank</strong></p>
<ul>
<li class=""><strong>CP0’s full input</strong>: <code>[0, p, 1, 1, 2, 2, p, p, 3, p]</code></li>
<li class=""><strong>CP1’s full input</strong>: <code>[0, p, 1, 1, 2, 2, 2, 2, p, p]</code></li>
</ul>
<p><strong>Step 5: Cumulative Sequence Lengths</strong></p>
<p>Padded cumulative lengths: <code>[0, 4, 8, 16, 20]</code></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="23-loss-computation-workflow">2.3 Loss Computation Workflow<a href="#23-loss-computation-workflow" class="hash-link" aria-label="Direct link to 2.3 Loss Computation Workflow" title="Direct link to 2.3 Loss Computation Workflow" translate="no">​</a></h3>
<p>Under Sequence Packing, loss calculation requires special handling:</p>
<ol>
<li class="">
<p><strong>Unpack Model Outputs</strong>: Use <code>_unpack_sequences</code> to restore individual sequences from the packed output.</p>
<ul>
<li class="">Compute start/end positions of each sequence on the current CP rank using <code>cu_seqlens_padded</code>.</li>
<li class=""><code>seq_starts = cu_seqlens_padded[:-1] // cp_size</code></li>
<li class=""><code>seq_ends = cu_seqlens_padded[1:] // cp_size</code></li>
</ul>
</li>
<li class="">
<p><strong>Per-Sequence Loss Calculation</strong>:</p>
<ul>
<li class="">Apply the loss function to each unpacked sequence individually.</li>
<li class="">Adjust original data to match the actual sequence length using <code>adjust_sequence_length</code>.</li>
<li class="">Accumulate losses from all sequences.</li>
</ul>
</li>
<li class="">
<p><strong>Result Aggregation</strong>:</p>
<ul>
<li class="">Sum all per-sequence losses to obtain the total loss.</li>
<li class="">Aggregate metrics across sequences.</li>
<li class="">Apply loss scaling if enabled.</li>
</ul>
</li>
</ol>
<p>This per-sequence approach ensures correct loss computation even under complex combinations of CP, TP, and packing.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="24-load-balancing-optimization">2.4 Load Balancing Optimization<a href="#24-load-balancing-optimization" class="hash-link" aria-label="Direct link to 2.4 Load Balancing Optimization" title="Direct link to 2.4 Load Balancing Optimization" translate="no">​</a></h3>
<p>To maximize the effectiveness of Sequence Packing, ROLL applies the <strong>Karmarkar-Karp algorithm</strong> at multiple levels for load balancing.</p>
<p><strong>Karmarkar-Karp Algorithm Overview</strong>:
A classical multi-way partitioning algorithm that divides a set of numbers into <em>k</em> subsets with sums as balanced as possible. In Sequence Packing, it ensures computational loads across processing units remain balanced, preventing bottlenecks.</p>
<p>Key optimizations include:</p>
<ul>
<li class=""><strong>GLOBAL BATCH → DP RANK Load Balancing</strong>: Ensures each DP rank receives a similar total number of tokens.</li>
<li class=""><strong>MINI BATCH → MICRO BATCH Load Balancing</strong>: Balances computational load across micro-batches.</li>
</ul>
<p>Implementation details and responsibility allocation are described in Section 3.2.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-implementation-workflow">3. Implementation Workflow<a href="#3-implementation-workflow" class="hash-link" aria-label="Direct link to 3. Implementation Workflow" title="Direct link to 3. Implementation Workflow" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="31-core-packing-and-unpacking-logic">3.1 Core Packing and Unpacking Logic<a href="#31-core-packing-and-unpacking-logic" class="hash-link" aria-label="Direct link to 3.1 Core Packing and Unpacking Logic" title="Direct link to 3.1 Core Packing and Unpacking Logic" translate="no">​</a></h3>
<p>Packing logic resides primarily in the strategy layer. When <code>use_sequence_packing</code> is enabled, the strategy automatically packs micro-batches and unpacks logits for loss computation.</p>
<p><strong>Core packing function <code>_pack_sequences</code> performs</strong>:</p>
<ol>
<li class="">Removes original padding and extracts valid tokens.</li>
<li class="">Computes cumulative sequence lengths (both original and padded).</li>
<li class="">Re-pads sequences to a multiple of <code>2 * cp_size * tp_size</code>.</li>
<li class="">Handles CP chunking and assignment.</li>
<li class="">Concatenates sequences and creates <code>PackedSeqParams</code>.</li>
</ol>
<p><strong>Loss computation</strong> is handled by <code>loss_wrapper</code>, which unpacks outputs and computes per-sequence losses.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="32-load-balancing-responsibility-allocation">3.2 Load Balancing Responsibility Allocation<a href="#32-load-balancing-responsibility-allocation" class="hash-link" aria-label="Direct link to 3.2 Load Balancing Responsibility Allocation" title="Direct link to 3.2 Load Balancing Responsibility Allocation" translate="no">​</a></h3>
<p>Load balancing in ROLL follows a clear division of responsibilities:</p>
<ol>
<li class="">
<p><strong>GLOBAL BATCH → DP RANK Load Balancing</strong>:</p>
<ul>
<li class=""><strong>Responsible Module</strong>: Pipeline layer (<code>batch_balance</code> function)</li>
<li class=""><strong>Objective</strong>: Equalize total token count per DP rank</li>
<li class=""><strong>Method</strong>: Apply Karmarkar-Karp algorithm before data distribution</li>
</ul>
</li>
<li class="">
<p><strong>MINI BATCH → MICRO BATCH Load Balancing</strong>:</p>
<ul>
<li class=""><strong>Responsible Module</strong>: Strategy layer (<code>make_micro_batch_iter_for_sequence_packing</code>)</li>
<li class=""><strong>Objective</strong>: Balance computational load across micro-batches</li>
<li class=""><strong>Method</strong>: Apply Karmarkar-Karp during micro-batch generation</li>
</ul>
</li>
<li class="">
<p><strong>Preservation of Randomness</strong>:</p>
<ul>
<li class="">The division from Batch → Mini Batch retains randomness (for shuffling) and thus does <strong>not</strong> apply load balancing.</li>
</ul>
</li>
</ol>
<p>This layered optimization ensures balanced workloads from global to local levels, maximizing hardware utilization.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-configuration-parameters">4. Configuration Parameters<a href="#4-configuration-parameters" class="hash-link" aria-label="Direct link to 4. Configuration Parameters" title="Direct link to 4. Configuration Parameters" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="41-how-to-enable-sequence-packing">4.1 How to Enable Sequence Packing<a href="#41-how-to-enable-sequence-packing" class="hash-link" aria-label="Direct link to 4.1 How to Enable Sequence Packing" title="Direct link to 4.1 How to Enable Sequence Packing" translate="no">​</a></h3>
<p>To use Sequence Packing, simply set <code>use_sequence_packing: true</code> in your configuration file.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="42-parameter-details-plain-language">4.2 Parameter Details (Plain Language)<a href="#42-parameter-details-plain-language" class="hash-link" aria-label="Direct link to 4.2 Parameter Details (Plain Language)" title="Direct link to 4.2 Parameter Details (Plain Language)" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="algorithm-packing-algorithm"><code>algorithm</code> (Packing Algorithm)<a href="#algorithm-packing-algorithm" class="hash-link" aria-label="Direct link to algorithm-packing-algorithm" title="Direct link to algorithm-packing-algorithm" translate="no">​</a></h4>
<ul>
<li class=""><strong><code>none</code></strong>: Default simple packing—sequences are packed in their original order.</li>
<li class=""><strong><code>load_balance</code></strong>: Intelligent load-balanced packing—reorders data to balance computational load across micro-batches. <strong>Recommended</strong>.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="max_packed_sequence_length_train-max-packed-length-for-training"><code>max_packed_sequence_length_train</code> (Max Packed Length for Training)<a href="#max_packed_sequence_length_train-max-packed-length-for-training" class="hash-link" aria-label="Direct link to max_packed_sequence_length_train-max-packed-length-for-training" title="Direct link to max_packed_sequence_length_train-max-packed-length-for-training" translate="no">​</a></h4>
<ul>
<li class="">Controls the maximum allowed length of a packed sequence during training.</li>
<li class="">E.g., setting to 8192 means no packed sequence will exceed 8192 tokens.</li>
<li class="">Choose a reasonable value to avoid out-of-memory errors while maintaining packing efficiency.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="max_packed_sequence_length_forward-max-packed-length-for-inference"><code>max_packed_sequence_length_forward</code> (Max Packed Length for Inference)<a href="#max_packed_sequence_length_forward-max-packed-length-for-inference" class="hash-link" aria-label="Direct link to max_packed_sequence_length_forward-max-packed-length-for-inference" title="Direct link to max_packed_sequence_length_forward-max-packed-length-for-inference" translate="no">​</a></h4>
<ul>
<li class="">Same as above, but applied during inference.</li>
<li class="">Typically set to the same value as the training parameter.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="min_num_micro_batches_train-minimum-micro-batches-for-training"><code>min_num_micro_batches_train</code> (Minimum Micro-Batches for Training)<a href="#min_num_micro_batches_train-minimum-micro-batches-for-training" class="hash-link" aria-label="Direct link to min_num_micro_batches_train-minimum-micro-batches-for-training" title="Direct link to min_num_micro_batches_train-minimum-micro-batches-for-training" translate="no">​</a></h4>
<ul>
<li class="">Specifies the minimum number of micro-batches per mini-batch during training.</li>
<li class="">Setting to 1 means no constraint—the system auto-determines optimal splitting.</li>
<li class="">Increase this value if facing GPU memory issues to reduce micro-batch size.</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="min_num_micro_batches_forward-minimum-micro-batches-for-inference"><code>min_num_micro_batches_forward</code> (Minimum Micro-Batches for Inference)<a href="#min_num_micro_batches_forward-minimum-micro-batches-for-inference" class="hash-link" aria-label="Direct link to min_num_micro_batches_forward-minimum-micro-batches-for-inference" title="Direct link to min_num_micro_batches_forward-minimum-micro-batches-for-inference" translate="no">​</a></h4>
<ul>
<li class="">Same as above, but for inference.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="43-full-configuration-example">4.3 Full Configuration Example<a href="#43-full-configuration-example" class="hash-link" aria-label="Direct link to 4.3 Full Configuration Example" title="Direct link to 4.3 Full Configuration Example" translate="no">​</a></h3>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">actor_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Enable sequence packing</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">use_sequence_packing</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token boolean important">True</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Sequence packing configuration</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">sequence_packing_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use load-balancing algorithm for better performance</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">algorithm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> load_balance</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Max packed sequence length during training</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">max_packed_sequence_length_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">8192</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Max packed sequence length during inference</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">max_packed_sequence_length_forward</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">8192</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Minimum 1 micro-batch during training (no constraint)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">min_num_micro_batches_train</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Minimum 1 micro-batch during inference</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">min_num_micro_batches_forward</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Sequence packing requires megatron strategy</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> megatron_train</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="44-usage-recommendations">4.4 Usage Recommendations<a href="#44-usage-recommendations" class="hash-link" aria-label="Direct link to 4.4 Usage Recommendations" title="Direct link to 4.4 Usage Recommendations" translate="no">​</a></h3>
<ol>
<li class=""><strong>Mandatory Condition</strong>: Only supported under <code>megatron_train</code> or <code>megatron_infer</code> strategies.</li>
<li class=""><strong>Recommended Setting</strong>: Use <code>algorithm: load_balance</code> for optimal performance.</li>
<li class=""><strong>Length Tuning</strong>: Set <code>max_packed_sequence_length</code> based on your GPU memory capacity—typically equal to the model’s maximum supported sequence length.</li>
<li class=""><strong>Custom Loss Functions</strong>: If using a custom loss function with sequence packing, refer to the custom loss documentation and ensure <code>apply_loss_scale</code> is correctly configured.</li>
</ol>
<p>With proper configuration, Sequence Packing significantly boosts training efficiency—especially in RL scenarios with highly variable sequence lengths—while maintaining model performance.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Advanced Features/sequence_packing.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2026-02-09T13:05:30.000Z" itemprop="dateModified">Feb 9, 2026</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ROLL/docs/User Guides/Advanced Features/offload_reload_control"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">GPU Time-Division Multiplexing Control Guide</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ROLL/docs/User Guides/Tracker &amp; Metrics/trackers_and_metrics"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Trackers and Metrics</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-introduction" class="table-of-contents__link toc-highlight">1. Introduction</a></li><li><a href="#2-implementation-principles" class="table-of-contents__link toc-highlight">2. Implementation Principles</a><ul><li><a href="#21-data-partitioning-hierarchy" class="table-of-contents__link toc-highlight">2.1 Data Partitioning Hierarchy</a></li><li><a href="#22-core-mechanism-of-sequence-packing" class="table-of-contents__link toc-highlight">2.2 Core Mechanism of Sequence Packing</a></li><li><a href="#23-loss-computation-workflow" class="table-of-contents__link toc-highlight">2.3 Loss Computation Workflow</a></li><li><a href="#24-load-balancing-optimization" class="table-of-contents__link toc-highlight">2.4 Load Balancing Optimization</a></li></ul></li><li><a href="#3-implementation-workflow" class="table-of-contents__link toc-highlight">3. Implementation Workflow</a><ul><li><a href="#31-core-packing-and-unpacking-logic" class="table-of-contents__link toc-highlight">3.1 Core Packing and Unpacking Logic</a></li><li><a href="#32-load-balancing-responsibility-allocation" class="table-of-contents__link toc-highlight">3.2 Load Balancing Responsibility Allocation</a></li></ul></li><li><a href="#4-configuration-parameters" class="table-of-contents__link toc-highlight">4. Configuration Parameters</a><ul><li><a href="#41-how-to-enable-sequence-packing" class="table-of-contents__link toc-highlight">4.1 How to Enable Sequence Packing</a></li><li><a href="#42-parameter-details-plain-language" class="table-of-contents__link toc-highlight">4.2 Parameter Details (Plain Language)</a></li><li><a href="#43-full-configuration-example" class="table-of-contents__link toc-highlight">4.3 Full Configuration Example</a></li><li><a href="#44-usage-recommendations" class="table-of-contents__link toc-highlight">4.4 Usage Recommendations</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Examples</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ROLL/docs/Getting Started/Quick Start/single_node_quick_start">Single Node Quick Start</a></li><li class="footer__item"><a class="footer__link-item" href="/ROLL/docs/User Guides/Configuration/config_guide">Config Guide</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Alibaba.</div></div></div></footer></div>
</body>
</html>