<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-English/UserGuide/backend/fp8_rollout" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">FP8 Quantization Configuration Guide | ROLL</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://alibaba.github.io/ROLL/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://alibaba.github.io/ROLL/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/fp8_rollout"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="FP8 Quantization Configuration Guide | ROLL"><meta data-rh="true" name="description" content="This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage."><meta data-rh="true" property="og:description" content="This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage."><link data-rh="true" rel="icon" href="https://img.alicdn.com/imgextra/i4/O1CN01bo6EZl2192CAIjFwE_!!6000000006941-2-tps-465-367.png"><link data-rh="true" rel="canonical" href="https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/fp8_rollout"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/fp8_rollout" hreflang="en"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/fp8_rollout" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"FP8 Quantization Configuration Guide","item":"https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/fp8_rollout"}]}</script><link rel="stylesheet" href="/ROLL/assets/css/styles.7a49f82a.css">
<script src="/ROLL/assets/js/runtime~main.3cecb864.js" defer="defer"></script>
<script src="/ROLL/assets/js/main.8175232d.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"dark"),document.documentElement.setAttribute("data-theme-choice",t||"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbar_MONK"><div class="navbar__inner"><div class="logoWrap_HHlA navbar__items"><div class="logo_Ufb2"><div class="ant-image css-10y1chj" style="width:40px;height:32px"><img alt="ROLL" class="ant-image-img css-10y1chj" style="height:32px" src="/ROLL/img/logo.png" width="40" height="32"></div></div><div><div class="title_E_95">ROLL</div><div class="subTitle_M9Ik">like a Reinforcement Learning Algorithm Developer</div></div></div><div class="navbar__items navbar__items--right"><a href="/ROLL/" class="ant-btn css-10y1chj ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false"><span>Home</span></a><a href="/ROLL/#core" class="ant-btn css-10y1chj ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false"><span>Core Algorithms</span></a><a href="/ROLL/#research" class="ant-btn css-10y1chj ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false"><span>Research Community</span></a><a href="/ROLL/docs/English/start" class="ant-btn css-10y1chj ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3 primary_vbUC" tabindex="0" aria-disabled="false"><span>API Docs</span></a><a href="https://github.com/alibaba/ROLL" class="ant-btn css-10y1chj ant-btn-text ant-btn-color-default ant-btn-variant-text btn_xSL3" tabindex="0" aria-disabled="false"><span>Github</span><span role="img" aria-label="export" class="anticon anticon-export"><svg fill-rule="evenodd" viewBox="64 64 896 896" focusable="false" data-icon="export" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M880 912H144c-17.7 0-32-14.3-32-32V144c0-17.7 14.3-32 32-32h360c4.4 0 8 3.6 8 8v56c0 4.4-3.6 8-8 8H184v656h656V520c0-4.4 3.6-8 8-8h56c4.4 0 8 3.6 8 8v360c0 17.7-14.3 32-32 32zM770.87 199.13l-52.2-52.2a8.01 8.01 0 014.7-13.6l179.4-21c5.1-.6 9.5 3.7 8.9 8.9l-21 179.4c-.8 6.6-8.9 9.4-13.6 4.7l-52.4-52.4-256.2 256.2a8.03 8.03 0 01-11.3 0l-42.4-42.4a8.03 8.03 0 010-11.3l256.1-256.3z"></path></svg></span></a><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div><button type="button" class="ant-btn css-10y1chj ant-btn-text ant-btn-color-default ant-btn-variant-text ant-btn-icon-only" style="margin-left:6px"><span class="ant-btn-icon"><span role="img" aria-label="sun" style="font-size:20px" class="anticon anticon-sun"><svg fill-rule="evenodd" viewBox="64 64 896 896" focusable="false" data-icon="sun" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M548 818v126a16 16 0 01-16 16h-40a16 16 0 01-16-16V818c15.85 1.64 27.84 2.46 36 2.46 8.15 0 20.16-.82 36-2.46m205.25-115.66l89.1 89.1a16 16 0 010 22.62l-28.29 28.29a16 16 0 01-22.62 0l-89.1-89.1c12.37-10.04 21.43-17.95 27.2-23.71 5.76-5.77 13.67-14.84 23.71-27.2m-482.5 0c10.04 12.36 17.95 21.43 23.71 27.2 5.77 5.76 14.84 13.67 27.2 23.71l-89.1 89.1a16 16 0 01-22.62 0l-28.29-28.29a16 16 0 010-22.63zM512 278c129.24 0 234 104.77 234 234S641.24 746 512 746 278 641.24 278 512s104.77-234 234-234m0 72c-89.47 0-162 72.53-162 162s72.53 162 162 162 162-72.53 162-162-72.53-162-162-162M206 476c-1.64 15.85-2.46 27.84-2.46 36 0 8.15.82 20.16 2.46 36H80a16 16 0 01-16-16v-40a16 16 0 0116-16zm738 0a16 16 0 0116 16v40a16 16 0 01-16 16H818c1.64-15.85 2.46-27.84 2.46-36 0-8.15-.82-20.16-2.46-36zM814.06 180.65l28.29 28.29a16 16 0 010 22.63l-89.1 89.09c-10.04-12.37-17.95-21.43-23.71-27.2-5.77-5.76-14.84-13.67-27.2-23.71l89.1-89.1a16 16 0 0122.62 0m-581.5 0l89.1 89.1c-12.37 10.04-21.43 17.95-27.2 23.71-5.76 5.77-13.67 14.84-23.71 27.2l-89.1-89.1a16 16 0 010-22.62l28.29-28.29a16 16 0 0122.62 0M532 64a16 16 0 0116 16v126c-15.85-1.64-27.84-2.46-36-2.46-8.15 0-20.16.82-36 2.46V80a16 16 0 0116-16z"></path></svg></span></span></button></div></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/ROLL/docs/English/DesignImplementation/AgenticPipeline"><span title="English" class="categoryLinkLabel_W154">English</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/DesignImplementation/AgenticPipeline"><span title="DesignImplementation" class="categoryLinkLabel_W154">DesignImplementation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/DevelopmentGuide/support_new_models_en"><span title="DevelopmentGuide" class="categoryLinkLabel_W154">DevelopmentGuide</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/QuickStart/config_guide"><span title="QuickStart" class="categoryLinkLabel_W154">QuickStart</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/ROLL/docs/English/UserGuide/trackers_and_metrics"><span title="UserGuide" class="categoryLinkLabel_W154">UserGuide</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/trackers_and_metrics"><span title="Trackers and Metrics" class="linkLabel_WmDU">Trackers and Metrics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/offload_reload_control"><span title="GPU Time-Division Multiplexing Control Guide" class="linkLabel_WmDU">GPU Time-Division Multiplexing Control Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/megatron_convert_2_hf"><span title="Converting MCoreAdapter Models to Hugging Face Format" class="linkLabel_WmDU">Converting MCoreAdapter Models to Hugging Face Format</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/device_mapping"><span title="ROLL Resource Configuration" class="linkLabel_WmDU">ROLL Resource Configuration</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/checkpoint_and_resume"><span title="Checkpoint Saving and Resuming Guide" class="linkLabel_WmDU">Checkpoint Saving and Resuming Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/async_training"><span title="ROLL Asynchronous Training User Guide" class="linkLabel_WmDU">ROLL Asynchronous Training User Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/async_parallel_rollout"><span title="Agentic Asynchronous Parallel Rollout" class="linkLabel_WmDU">Agentic Asynchronous Parallel Rollout</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/UserGuide/agentic/agentic_engineer_practice"><span title="agentic" class="categoryLinkLabel_W154">agentic</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/UserGuide/algorithms/offpolicy_setting"><span title="algorithms" class="categoryLinkLabel_W154">algorithms</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/UserGuide/ascend/ascend_usage"><span title="ascend" class="categoryLinkLabel_W154">ascend</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/ROLL/docs/English/UserGuide/backend/vllm"><span title="backend" class="categoryLinkLabel_W154">backend</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/backend/vllm"><span title="vLLM Inference Backend Configuration Guide" class="linkLabel_WmDU">vLLM Inference Backend Configuration Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/backend/sglang"><span title="SGLang Inference Backend Configuration Guide" class="linkLabel_WmDU">SGLang Inference Backend Configuration Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/backend/megatron"><span title="Megatron Inference and Training Backend Configuration Guide" class="linkLabel_WmDU">Megatron Inference and Training Backend Configuration Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/backend/lora"><span title="LoRA Fine-tuning Configuration Guide" class="linkLabel_WmDU">LoRA Fine-tuning Configuration Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ROLL/docs/English/UserGuide/backend/fp8_rollout"><span title="FP8 Quantization Configuration Guide" class="linkLabel_WmDU">FP8 Quantization Configuration Guide</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/UserGuide/backend/deepspeed"><span title="DeepSpeed Training Backend Configuration Guide" class="linkLabel_WmDU">DeepSpeed Training Backend Configuration Guide</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/UserGuide/pipeline/vl_rlvr_pipeline_start"><span title="pipeline" class="categoryLinkLabel_W154">pipeline</span></a></div></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/start"><span title="start" class="linkLabel_WmDU">start</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ROLL/docs/"><span title="index" class="linkLabel_WmDU">index</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/ROLL/docs/简体中文/使用指南/agentic/Tool_Use"><span title="简体中文" class="categoryLinkLabel_W154">简体中文</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ROLL/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">English</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">UserGuide</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">backend</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">FP8 Quantization Configuration Guide</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>FP8 Quantization Configuration Guide</h1></header>
<p>This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview" translate="no">​</a></h2>
<p>FP8 quantization is an efficient numerical precision optimization technique that can significantly reduce model VRAM footprint and improve inference speed. ROLL supports FP8 quantization configuration for actor_infer and llm_judge components.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="actor_infer-fp8-configuration">actor_infer FP8 Configuration<a href="#actor_infer-fp8-configuration" class="hash-link" aria-label="Direct link to actor_infer FP8 Configuration" title="Direct link to actor_infer FP8 Configuration" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="basic-configuration">Basic Configuration<a href="#basic-configuration" class="hash-link" aria-label="Direct link to Basic Configuration" title="Direct link to Basic Configuration" translate="no">​</a></h3>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">actor_infer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">quantization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="dense-model-configuration">Dense Model Configuration<a href="#dense-model-configuration" class="hash-link" aria-label="Direct link to Dense Model Configuration" title="Direct link to Dense Model Configuration" translate="no">​</a></h3>
<p>For Dense models, configuration requirements differ based on quantization method:</p>
<p><strong>Dense + Per Tensor Quantization (Default)</strong></p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">actor_infer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">quantization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span></code></pre></div></div>
<p><strong>Dense + Per Block Quantization</strong></p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">actor_infer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">quantization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">hf_overrides</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token key atrule">quantization_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">activation_scheme</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> dynamic</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">fmt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> e4m3</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">quant_method</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">weight_block_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">128</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">128</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Required: per block quantization</span><br></span></code></pre></div></div>
<p><strong>Configuration Description:</strong></p>
<ul>
<li class=""><code>activation_scheme: dynamic</code>: Use dynamic activation scheme</li>
<li class=""><code>fmt: e4m3</code>: Specify FP8 format as E4M3</li>
<li class=""><code>quant_method: fp8</code>: Set quantization method to FP8</li>
<li class=""><code>weight_block_size: [128, 128]</code>: Required for per block quantization, specifies weight block size</li>
</ul>
<p><strong>Note:</strong> When specifying <code>weight_block_size</code>, you must also provide <code>activation_scheme</code>, <code>fmt</code>, and <code>quant_method</code> parameters, otherwise an error will occur.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="moe-model-configuration">MoE Model Configuration<a href="#moe-model-configuration" class="hash-link" aria-label="Direct link to MoE Model Configuration" title="Direct link to MoE Model Configuration" translate="no">​</a></h3>
<p>For MoE (Mixture of Experts) models, <code>hf_overrides/quantization_config</code> must be configured, and only per block quantization is supported:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">actor_infer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">quantization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">hf_overrides</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token key atrule">quantization_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">activation_scheme</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> dynamic</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">fmt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> e4m3</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">quant_method</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">weight_block_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">128</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">128</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># Required: MoE models must use per block quantization</span><br></span></code></pre></div></div>
<p><strong>Note:</strong> MoE models must use per block quantization. The <code>weight_block_size</code> parameter is required, and you must also provide <code>activation_scheme</code>, <code>fmt</code>, and <code>quant_method</code> parameters.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="llm_judge-fp8-configuration">llm_judge FP8 Configuration<a href="#llm_judge-fp8-configuration" class="hash-link" aria-label="Direct link to llm_judge FP8 Configuration" title="Direct link to llm_judge FP8 Configuration" translate="no">​</a></h2>
<p>LLM as judge model also supports FP8 quantization. Note that the judge model requires independent GPU resources and cannot share GPU with actor_infer:</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">llm_judge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token comment" style="color:rgb(98, 114, 164)"># NOTE: llm as judge also needs GPU, cannot share GPU with actor infer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">worker_cls</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> roll.pipeline.rlvr.rewards.llm_judge_reward_worker.LLMJudgeRewardWorker</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">judge_prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> Qwen2.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">7B</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">Instruct</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">RLVR</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">prompt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">judge_model_type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> inference</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">tag_included</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">RLVR</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">gpu_memory_utilization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">0.8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">quantization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">max_model_len</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">8000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">load_format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> auto</span><br></span></code></pre></div></div>
<p><strong>Configuration Description:</strong></p>
<ul>
<li class=""><code>gpu_memory_utilization: 0.8</code>: Set VRAM utilization to 80%</li>
<li class=""><code>quantization: fp8</code>: Enable FP8 quantization</li>
<li class=""><code>max_model_len: 8000</code>: Maximum model length limit</li>
<li class=""><code>load_format: auto</code>: Automatically select loading format</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="configuration-notes">Configuration Notes<a href="#configuration-notes" class="hash-link" aria-label="Direct link to Configuration Notes" title="Direct link to Configuration Notes" translate="no">​</a></h2>
<ol>
<li class=""><strong>GPU Resource Isolation</strong>: llm_judge requires independent GPU and cannot share with actor_infer</li>
<li class=""><strong>MoE Model Limitations</strong>: MoE models must use per block quantization, per tensor quantization is not supported</li>
<li class=""><strong>Memory Optimization</strong>: FP8 quantization can significantly reduce memory usage, recommended for VRAM-constrained scenarios</li>
<li class=""><strong>Performance Trade-off</strong>: While FP8 quantization improves performance, it may slightly affect model accuracy, requiring trade-offs based on specific scenarios</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="complete-example">Complete Example<a href="#complete-example" class="hash-link" aria-label="Direct link to Complete Example" title="Direct link to Complete Example" translate="no">​</a></h2>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Configuration example: FP8 quantization for actor_infer and llm_judge</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">actor_infer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">quantization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">hf_overrides</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token key atrule">quantization_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">activation_scheme</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> dynamic</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">fmt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> e4m3</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">quant_method</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          </span><span class="token key atrule">weight_block_size</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">128</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token number">128</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token key atrule">llm_judge</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">worker_cls</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> roll.pipeline.rlvr.rewards.llm_judge_reward_worker.LLMJudgeRewardWorker</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">judge_prompt</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> Qwen2.5</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">7B</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">Instruct</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">RLVR</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">prompt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">judge_model_type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> inference</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">tag_included</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">RLVR</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain">  </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">strategy_args</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> vllm</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">strategy_config</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">gpu_memory_utilization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">0.8</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">quantization</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> fp8</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">max_model_len</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">8000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">load_format</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> auto</span><br></span></code></pre></div></div>
<p>With the above configuration, you can successfully enable FP8 quantization in ROLL to achieve better inference performance and VRAM efficiency.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/backend/fp8_rollout.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-11-07T06:53:09.000Z" itemprop="dateModified">Nov 7, 2025</time></b></span></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ROLL/docs/English/UserGuide/backend/deepspeed"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">DeepSpeed Training Backend Configuration Guide</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ROLL/docs/English/UserGuide/backend/lora"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">LoRA Fine-tuning Configuration Guide</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#actor_infer-fp8-configuration" class="table-of-contents__link toc-highlight">actor_infer FP8 Configuration</a><ul><li><a href="#basic-configuration" class="table-of-contents__link toc-highlight">Basic Configuration</a></li><li><a href="#dense-model-configuration" class="table-of-contents__link toc-highlight">Dense Model Configuration</a></li><li><a href="#moe-model-configuration" class="table-of-contents__link toc-highlight">MoE Model Configuration</a></li></ul></li><li><a href="#llm_judge-fp8-configuration" class="table-of-contents__link toc-highlight">llm_judge FP8 Configuration</a></li><li><a href="#configuration-notes" class="table-of-contents__link toc-highlight">Configuration Notes</a></li><li><a href="#complete-example" class="table-of-contents__link toc-highlight">Complete Example</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Examples</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ROLL/docs/简体中文/快速开始/single_node_quick_start_cn">ROLL单机实践手册</a></li><li class="footer__item"><a class="footer__link-item" href="/ROLL/docs/简体中文/快速开始/config_guide_cn">配置指南</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Alibaba.</div></div></div></footer></div>
</body>
</html>