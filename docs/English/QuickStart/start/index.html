<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-English/QuickStart/start">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">start | ROLL</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://alibaba.github.io/ROLL/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://alibaba.github.io/ROLL/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://alibaba.github.io/ROLL/docs/English/QuickStart/start"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="start | ROLL"><meta data-rh="true" name="description" content="ğŸš€ An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models ğŸš€"><meta data-rh="true" property="og:description" content="ğŸš€ An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models ğŸš€"><link data-rh="true" rel="icon" href="/ROLL/img/logo.png"><link data-rh="true" rel="canonical" href="https://alibaba.github.io/ROLL/docs/English/QuickStart/start"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/English/QuickStart/start" hreflang="en"><link data-rh="true" rel="alternate" href="https://alibaba.github.io/ROLL/docs/English/QuickStart/start" hreflang="x-default"><link rel="stylesheet" href="/ROLL/assets/css/styles.ee92b8a2.css">
<link rel="preload" href="/ROLL/assets/js/runtime~main.c12cdc20.js" as="script">
<link rel="preload" href="/ROLL/assets/js/main.9e947d7a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ROLL/"><div class="navbar__logo"><img src="/ROLL/img/logo.png" alt="ROLL Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/ROLL/img/logo.png" alt="ROLL Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">ROLL</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/ROLL/docs/English/DesignImplementation/AgenticPipeline">æ–‡æ¡£</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/ROLL/docs/English/DesignImplementation/AgenticPipeline">English</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/DesignImplementation/AgenticPipeline">DesignImplementation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/DevelopmentGuide/support_new_models_en">DevelopmentGuide</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/ROLL/docs/English/QuickStart/config_guide">QuickStart</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/config_guide">Configuration Guide</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/config_system">ROLL Configuration System Detailed Explanation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/debug_guide">ROLL Debugging Guide</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/image_address">Image Provided</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/installation">Installation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/multi_nodes_quick_start">Quick Start: Multi-Node Deployment Guide</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/qa_issues">Frequently Asked Questions (Q&amp;A)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/ROLL/docs/English/QuickStart/single_node_quick_start">Quick Start: Single-Node Deployment Guide</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ROLL/docs/English/QuickStart/start">start</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/ROLL/docs/English/UserGuide/agentic_async_parallel_rollout">UserGuide</a></div></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ROLL/docs/">index</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ROLL/docs/ç®€ä½“ä¸­æ–‡/ä½¿ç”¨æŒ‡å—/agentic_async_parallel_rollout">ç®€ä½“ä¸­æ–‡</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/ROLL/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">English</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">QuickStart</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">start</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>start</h1></header><div align="center"><img loading="lazy" src="https://img.alicdn.com/imgextra/i2/O1CN01R6uYoU1VrrET7d1G6_!!6000000002707-0-tps-1292-407.jpg" width="40%" alt="ROLL Logo" class="img_ev3q"><h1>ROLL: Reinforcement Learning Optimization for Large-Scale Learning</h1><h4>ğŸš€ An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models ğŸš€</h4><p><a href="https://github.com/alibaba/ROLL/blob/main/LICENSE" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://img.shields.io/badge/license-Apache%202.0-blue.svg" alt="License" class="img_ev3q"></a><a href="https://github.com/alibaba/ROLL/issues" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://img.shields.io/github/issues/alibaba/ROLL" alt="GitHub issues" class="img_ev3q"></a><a href="https://github.com/alibaba/ROLL/stargazers" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://img.shields.io/github/stars/alibaba/ROLL?style=social" alt="Repo stars" class="img_ev3q"></a><a href="https://arxiv.org/abs/2506.06122" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://img.shields.io/static/v1?label=arXiv&amp;message=Paper&amp;color=red" class="img_ev3q"></a><a href="https://img.alicdn.com/imgextra/i4/O1CN01MICK0T28fHMzy5P84_!!6000000007959-2-tps-756-850.png" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://img.shields.io/badge/WeChat-green?logo=wechat" alt="WeChat QR" class="img_ev3q"></a></p></div><p>ROLL is an efficient and user-friendly RL library designed for Large Language Models (LLMs) utilizing Large Scale GPU resources. It significantly enhances LLM performance in key areas such as human preference alignment, complex reasoning, and multi-turn agentic interaction scenarios.</p><p>Leveraging a multi-role distributed architecture with Ray for flexible resource allocation and heterogeneous task scheduling, ROLL integrates cutting-edge technologies like Megatron-Core, SGLang and vLLM to accelerate model training and inference.</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="-news">ğŸ“¢ News<a href="#-news" class="hash-link" aria-label="Direct link to ğŸ“¢ News" title="Direct link to ğŸ“¢ News">â€‹</a></h2><table><thead><tr><th align="left">ğŸ“£   Updates</th></tr></thead><tbody><tr><td align="left"><strong>[07/31/2025]</strong> ğŸ‰ Refactor agentic rl design. Support agentic rl <a href="/ROLL/docs/English/QuickStart/examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_async.yaml">async training</a>. Explore the new capabilities!</td></tr><tr><td align="left"><strong>[07/31/2025]</strong> ğŸ‰ Support <a href="/ROLL/docs/English/QuickStart/examples/qwen2.5-7B-distill_megatron/run_distill_pipeline.sh">DistillPipeline</a>/<a href="/ROLL/docs/English/QuickStart/examples/qwen2.5-3B-dpo_megatron/run_dpo_pipeline.sh">DpoPipeline</a>. Support <a href="/ROLL/docs/English/QuickStart/examples/qwen2.5-7B-rlvr_megatron/rlvr_lora_zero3.yaml">lora</a>. Support <a href="https://arxiv.org/abs/2507.18071" target="_blank" rel="noopener noreferrer">GSPO</a></td></tr><tr><td align="left"><strong>[06/25/2025]</strong> ğŸ‰ Support thread env for env scaling and support qwen2.5 VL agentic pipeline.</td></tr><tr><td align="left"><strong>[06/13/2025]</strong> ğŸ‰ Support Qwen2.5 VL rlvr pipeline and upgrade mcore to 0.12 version.</td></tr><tr><td align="left"><strong>[06/09/2025]</strong> ğŸ‰ ROLL tech report is now available! Access the report <a href="https://arxiv.org/abs/2506.06122" target="_blank" rel="noopener noreferrer">here</a>.</td></tr><tr><td align="left"><strong>[05/30/2025]</strong> ğŸ‰ Training RLVR and Agentic RL with ROLL is now available! Explore the new capabilities.</td></tr></tbody></table><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ï¸-architecture-at-a-glance">ğŸ—ºï¸ Architecture at a Glance<a href="#ï¸-architecture-at-a-glance" class="hash-link" aria-label="Direct link to ğŸ—ºï¸ Architecture at a Glance" title="Direct link to ğŸ—ºï¸ Architecture at a Glance">â€‹</a></h2><p>New to <strong>ROLL</strong>? The interactive mind-map below gives you a birdâ€™s-eye view of the libraryâ€™s core modules, data flow, and training pipelines. Start here for a quick orientation before diving into the detailed guides.</p><p align="center"><a href="https://img.alicdn.com/imgextra/i4/O1CN01SF8A1S1OKEqcHRJe0_!!6000000001686-2-tps-3674-1719.png" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://img.alicdn.com/imgextra/i4/O1CN01SF8A1S1OKEqcHRJe0_!!6000000001686-2-tps-3674-1719.png" width="100%" alt="ROLL Architecture Mind-map" class="img_ev3q"></a><br><sub>Click the image to view the <b>HD</b> version</sub></p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="-get-started">ğŸš€ Get Started<a href="#-get-started" class="hash-link" aria-label="Direct link to ğŸš€ Get Started" title="Direct link to ğŸš€ Get Started">â€‹</a></h2><p><a href="https://alibaba.github.io/ROLL/" target="_blank" rel="noopener noreferrer">Documents</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="quick-start">Quick Start<a href="#quick-start" class="hash-link" aria-label="Direct link to Quick Start" title="Direct link to Quick Start">â€‹</a></h3><p><a href="https://alibaba.github.io/ROLL/docs/English/StepByStep/alicloud_pipeline_quick_start_en" target="_blank" rel="noopener noreferrer">Quick Start based on alicloud</a><br>
<a href="https://alibaba.github.io/ROLL/docs/English/QuickStart/installation" target="_blank" rel="noopener noreferrer">Installation</a><br>
<a href="https://alibaba.github.io/ROLL/docs/English/QuickStart/config_guide" target="_blank" rel="noopener noreferrer">Config guide</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-by-step">Step By Step<a href="#step-by-step" class="hash-link" aria-label="Direct link to Step By Step" title="Direct link to Step By Step">â€‹</a></h3><p><a href="https://alibaba.github.io/ROLL/docs/English/StepByStep/rlvr_pipeline_start" target="_blank" rel="noopener noreferrer">RLVR Pipeline</a><br>
<a href="https://alibaba.github.io/ROLL/docs/English/StepByStep/agent_pipeline_start" target="_blank" rel="noopener noreferrer">Agentic RL Pipeline</a></p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="-key-features">âœ¨ Key Features<a href="#-key-features" class="hash-link" aria-label="Direct link to âœ¨ Key Features" title="Direct link to âœ¨ Key Features">â€‹</a></h2><p>ROLL is engineered to empower a diverse range of users in the LLM and RL landscape.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-for-tech-pioneers-eg-large-ai-labs-hyperscalers"><ins>ğŸ¯ For Tech Pioneers (e.g., Large AI Labs, Hyperscalers)</ins><a href="#-for-tech-pioneers-eg-large-ai-labs-hyperscalers" class="hash-link" aria-label="Direct link to -for-tech-pioneers-eg-large-ai-labs-hyperscalers" title="Direct link to -for-tech-pioneers-eg-large-ai-labs-hyperscalers">â€‹</a></h3><p>Seeking to lead the LLM community with large-scale GPU clusters? ROLL offers:</p><ul><li>ğŸš€ <strong>Fast and Cost-Effective</strong>: Fully exploits high-performance hardware, expediting RL training and achieving considerable reductions in training cost and time on large GPU clusters.</li><li>ğŸ”— <strong>Scalability and Fault Tolerance</strong>: Supports a wide range of LLM training and serving optimization techniques, enabling scalable training of models up to 200B+ parameters across thousands of GPUs. Features an efficient checkpoint and resumption mechanism for minimal downtime.</li><li>ğŸ› ï¸ <strong>Flexible Hardware Usage</strong>: Supports RL training across various hardware types. Users can choose between colocation or disaggregation, and configure synchronous or asynchronous execution modes to fully leverage different hardware architectures.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-for-product-developers"><ins>ğŸ§‘â€ğŸ’» For Product Developers</ins><a href="#-for-product-developers" class="hash-link" aria-label="Direct link to -for-product-developers" title="Direct link to -for-product-developers">â€‹</a></h3><p>Focused on enhancing in-house LLMs with human alignment, reasoning, and tool use? ROLL provides:</p><ul><li>ğŸ”§ <strong>Diverse and Extensible Rewards/Environments</strong>: Implements a suite of <code>Reward Worker</code>s and <code>Environment Worker</code>s. Easily customize your own rewards and environments based on our existing implementations.</li><li>ğŸ§­ <strong>Compositional Sample-Reward Route</strong>: Provides a user-friendly interface to control prompt sampling ratios across tasks and dynamically route samples to appropriate <code>Reward Worker</code>s (e.g., mathematical verifiers, sandbox environments, LLM-as-a-judge). Essential for optimizing multi-capability production-level LLMs.</li><li>âš™ï¸ <strong>Easy Device-Reward Mapping</strong>: Develops an interface for easy configuration of device mapping for <code>Reward Worker</code>s, isolating reward computation to prevent interference and performance bottlenecks in multi-task RL training.</li><li>ğŸ“š <strong>Rich Training Recipes</strong>: Offers a variety of RL algorithms (like GRPO/PPO/reinforce++/TOPR/RAFT++), LLMs, tasks, and datasets to reduce engineering effort for new training features.</li><li>ğŸ† <strong>Superior Performance</strong>: Includes a set of tuned training configurations that achieve satisfactory performance across many tasks, alleviating laborious hyperparameter searches.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="-for-algorithm-researchers"><ins>ğŸ”¬ For Algorithm Researchers</ins><a href="#-for-algorithm-researchers" class="hash-link" aria-label="Direct link to -for-algorithm-researchers" title="Direct link to -for-algorithm-researchers">â€‹</a></h3><p>Need flexible, fine-grained control for RL experiments, often with limited GPU access? ROLL delivers:</p><ul><li>ğŸ’¡ <strong>Constrained Device Execution</strong>: Enables efficient training on limited GPU resources (including single-GPU setups) via memory optimization techniques, facilitating rapid trial-and-error and timely feedback.</li><li>ğŸ§© <strong>Pluggable RLVR &amp; Agentic RL Pipeline</strong>: Abstracts RL training pipeline stages (RLVR and Agentic RL) at an appropriate granularity, enabling agile experimentation. Flexibly orchestrate stages to implement and customize diverse RL algorithms.</li><li>ğŸ“Š <strong>Transparent Experimentation</strong>: Provides comprehensive logging and monitoring capabilities for easy tracking and analysis of experiments.</li><li>âš–ï¸ <strong>Fair Academic Baselines</strong>: Offers classical algorithms, models, and tasks to facilitate fair baseline comparisons on standard benchmarks.</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ï¸-advanced-rl-tuning-with-roll-optimizing-llm-performance">ğŸ› ï¸ Advanced RL Tuning with ROLL: Optimizing LLM Performance<a href="#ï¸-advanced-rl-tuning-with-roll-optimizing-llm-performance" class="hash-link" aria-label="Direct link to ğŸ› ï¸ Advanced RL Tuning with ROLL: Optimizing LLM Performance" title="Direct link to ğŸ› ï¸ Advanced RL Tuning with ROLL: Optimizing LLM Performance">â€‹</a></h2><p>Training LLMs with Reinforcement Learning presents unique challenges due to vast action spaces, complex reward landscapes, and the need for stable, efficient learning. ROLL incorporates several advanced techniques and parameter configurations, empowering users to fine-tune their LLM RL pipelines for optimal performance.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stabilizing-training-through-clipping-and-normalization"><ins>Stabilizing Training through Clipping and Normalization</ins><a href="#stabilizing-training-through-clipping-and-normalization" class="hash-link" aria-label="Direct link to stabilizing-training-through-clipping-and-normalization" title="Direct link to stabilizing-training-through-clipping-and-normalization">â€‹</a></h3><p>LLM outputs can be highly variable, leading to unstable gradients if not managed carefully. ROLL offers robust mechanisms:</p><ul><li><strong>Value and Advantage Clipping (<code>value_clip</code>, <code>advantage_clip</code>)</strong>: These parameters help constrain the updates to the value function and the magnitude of advantages. For instance, setting <code>advantage_clip</code> prevents excessively large advantage values from destabilizing policy updates, which is crucial when rewards are sparse or noisy.</li><li><strong>Dual Clip Loss (<code>dual_clip_loss</code>)</strong>: A common technique in PPO, this further refines the clipping mechanism in the loss function to ensure policy updates remain within a trusted region, promoting stable learning.</li><li><strong>Reward Clipping and Normalization (<code>reward_clip</code>, <code>reward_norm</code>, <code>reward_scale</code>, <code>reward_shift</code>)</strong>: Rewards from LLM evaluations (human or automated) can vary significantly. <code>reward_clip</code> truncates extreme reward values. While <code>reward_norm</code> (if enabled) standardizes rewards (e.g., to zero mean and unit variance), making the learning process less sensitive to the absolute scale of rewards and improving convergence across different tasks or reward functions. ROLL&#x27;s flexible <code>Reward Worker</code> infrastructure seamlessly integrates with these normalization strategies.</li><li><strong>Advantage Whitening (<code>whiten_advantages</code>)</strong>: Normalizing advantage estimates across a batch (subtracting the mean and dividing by the standard deviation) reduces variance and stabilizes policy gradient updates. This is particularly beneficial in LLM RL where advantage estimates can be noisy.</li></ul><p>These techniques collectively enhance training stability, prevent policy collapse, and allow for smoother convergence, especially important for large-scale models and complex, open-ended generation tasks. This aligns with ROLL&#x27;s <strong>Superior Performance</strong> through tuned configurations and <strong>Scalability</strong> for large models.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="intelligent-data-handling-for-sample-efficiency"><ins>Intelligent Data Handling for Sample Efficiency</ins><a href="#intelligent-data-handling-for-sample-efficiency" class="hash-link" aria-label="Direct link to intelligent-data-handling-for-sample-efficiency" title="Direct link to intelligent-data-handling-for-sample-efficiency">â€‹</a></h3><p>Not all generated data is equally useful for learning. ROLL provides fine-grained control over data processing:</p><ul><li><strong>Data Masking (<code>max_len_mask</code>, <code>difficulty_mask</code>)</strong>:<ul><li><code>max_len_mask</code>: Ignores or down-weights parts of sequences that exceed a defined maximum length, preventing excessively long and potentially low-quality generations from dominating the training signal.</li><li><code>difficulty_mask</code> (with <code>difficulty_low_threshold</code>, <code>difficulty_high_threshold</code>): This powerful feature allows filtering samples based on their perceived difficulty (e.g., estimated by a reward model or success rate). By focusing on samples that are neither too easy (low learning signal) nor too hard (potentially noisy or unlearnable signal), ROLL can significantly improve sample efficiency. This is a practical implementation of strategies like &quot;dynamical sampling&quot; mentioned in advanced RL literature, supported by ROLL&#x27;s <strong>Compositional Sample-Reward Route</strong> and efficient <strong>Worker Scheduler</strong> design.</li></ul></li><li><strong>Error Filtering (<code>error_max_len_clip</code>)</strong>: While not enabled by default in this config, options exist to manage errors in generations, for example, by clipping or assigning specific penalties.</li><li><strong>Loss Weighting (<code>difficulty_loss_weight</code>, <code>length_loss_weight</code>)</strong>: Although disabled here, ROLL&#x27;s architecture supports future extensions for weighting the loss based on sample characteristics like difficulty or length, allowing for more nuanced control over the learning process.</li></ul><p>These data handling strategies lead to more efficient use of computational resources by focusing on the most informative samples, speeding up training and potentially leading to better final model performance. This contributes to ROLL&#x27;s <strong>Fast and Cost-Effective</strong> training.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="refining-rewards-and-policy-regularization"><ins>Refining Rewards and Policy Regularization</ins><a href="#refining-rewards-and-policy-regularization" class="hash-link" aria-label="Direct link to refining-rewards-and-policy-regularization" title="Direct link to refining-rewards-and-policy-regularization">â€‹</a></h3><p>Guiding the LLM effectively requires careful reward design and policy regularization:</p><ul><li><strong>Token-Level KL Regularization (<code>add_token_level_kl</code>)</strong>: While not active in this specific configuration, adding a token-level KL divergence penalty to the reward (or as part of the loss) is a common and effective technique. It encourages the learned policy to stay close to a reference policy (e.g., the initial SFT model). This helps maintain generation quality, prevents catastrophic forgetting of general language abilities, and ensures the LLM doesn&#x27;t stray too far into undesirable parts of the policy space while optimizing for the RL objective. ROLL&#x27;s integration with powerful training backends like <strong>Megatron-Core</strong> facilitates such complex loss computations efficiently.</li></ul><p>Proper regularization is key to balancing exploration with exploitation and ensuring that the LLM improves on the target task without degrading its overall language capabilities. This aligns with ROLL&#x27;s goal of enhancing LLM performance in areas like <strong>human preference alignment</strong> and <strong>complex reasoning</strong>.</p><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="-upcoming-features">ğŸ”® Upcoming Features<a href="#-upcoming-features" class="hash-link" aria-label="Direct link to ğŸ”® Upcoming Features" title="Direct link to ğŸ”® Upcoming Features">â€‹</a></h2><p>We are continuously working to expand ROLL&#x27;s capabilities:</p><ul><li>ğŸ–¼ï¸ <strong>Qwen2.5 VL RL pipeline</strong>: Enhancing support for Vision-Language models.</li><li>â±ï¸ <strong>One-Step Async pipeline</strong>: For even more efficient and streamlined asynchronous operations.</li><li>âš™ï¸ <strong>FSDP2</strong>: Integrating the latest Fully Sharded Data Parallel techniques.</li><li>ğŸ” <strong>Support DeepseekV3</strong>: Adding compatibility for the newest Deepseek models.</li></ul><hr><h2 class="anchor anchorWithStickyNavbar_LWe7" id="-notable-work-based-on-roll">ğŸ† Notable work based on ROLL<a href="#-notable-work-based-on-roll" class="hash-link" aria-label="Direct link to ğŸ† Notable work based on ROLL" title="Direct link to ğŸ† Notable work based on ROLL">â€‹</a></h2><ul><li><a href="https://www.arxiv.org/abs/2507.22879" target="_blank" rel="noopener noreferrer">RecGPT</a>: a next-generation, LLM-driven framework that places user intent at the core of recommender systems, fostering a more sustainable and mutually beneficial ecosystem.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="-citation-and-acknowledgement">ğŸ™ Citation and Acknowledgement<a href="#-citation-and-acknowledgement" class="hash-link" aria-label="Direct link to ğŸ™ Citation and Acknowledgement" title="Direct link to ğŸ™ Citation and Acknowledgement">â€‹</a></h2><p>ROLL is inspired by the design of OpenRLHF, VeRL, Nemo-Aligner, and RAGEN.
The project is developed by Alibaba TAOBAO &amp; TMALL Group and Alibaba Group. The code is distributed under the Apache License (Version 2.0). This product contains various third-party components under other open-source licenses. See the <code>NOTICE</code> file for more information.</p><p>The following repositories have been used in ROLL, either in their close-to-original form or as an inspiration:</p><ul><li><a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noopener noreferrer">NVIDIA/Megatron-LM</a></li><li><a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener noreferrer">microsoft/DeepSpeed</a></li><li><a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener noreferrer">sgl-project/sglang</a></li><li><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vllm-project/vllm</a></li></ul><p>If you use ROLL in your research or project, please consider citing us:</p><div class="language-bibtex codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bibtex codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">@article{wang2025reinforcement,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  title={Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  author={Wang, Weixun and Xiong, Shaopan and Chen, Gengru and Gao, Wei and Guo, Sheng and He, Yancheng and Huang, Ju and Liu, Jiaheng and Li, Zhendong and Li, Xiaoyang and others},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  journal={arXiv preprint arXiv:2506.06122},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  year={2025}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><hr><div align="center">We welcome contributions from the community! ğŸ¤</div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/QuickStart/start.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2025-08-20T11:46:44.000Z">Aug 20, 2025</time></b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/ROLL/docs/English/QuickStart/single_node_quick_start"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Quick Start: Single-Node Deployment Guide</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ROLL/docs/English/UserGuide/agentic_async_parallel_rollout"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Agentic Asynchronous Parallel Rollout</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#-news" class="table-of-contents__link toc-highlight">ğŸ“¢ News</a></li><li><a href="#ï¸-architecture-at-a-glance" class="table-of-contents__link toc-highlight">ğŸ—ºï¸ Architecture at a Glance</a></li><li><a href="#-get-started" class="table-of-contents__link toc-highlight">ğŸš€ Get Started</a><ul><li><a href="#quick-start" class="table-of-contents__link toc-highlight">Quick Start</a></li><li><a href="#step-by-step" class="table-of-contents__link toc-highlight">Step By Step</a></li></ul></li><li><a href="#-key-features" class="table-of-contents__link toc-highlight">âœ¨ Key Features</a><ul><li><a href="#-for-tech-pioneers-eg-large-ai-labs-hyperscalers" class="table-of-contents__link toc-highlight"><ins>ğŸ¯ For Tech Pioneers (e.g., Large AI Labs, Hyperscalers)</ins></a></li><li><a href="#-for-product-developers" class="table-of-contents__link toc-highlight"><ins>ğŸ§‘â€ğŸ’» For Product Developers</ins></a></li><li><a href="#-for-algorithm-researchers" class="table-of-contents__link toc-highlight"><ins>ğŸ”¬ For Algorithm Researchers</ins></a></li></ul></li><li><a href="#ï¸-advanced-rl-tuning-with-roll-optimizing-llm-performance" class="table-of-contents__link toc-highlight">ğŸ› ï¸ Advanced RL Tuning with ROLL: Optimizing LLM Performance</a><ul><li><a href="#stabilizing-training-through-clipping-and-normalization" class="table-of-contents__link toc-highlight"><ins>Stabilizing Training through Clipping and Normalization</ins></a></li><li><a href="#intelligent-data-handling-for-sample-efficiency" class="table-of-contents__link toc-highlight"><ins>Intelligent Data Handling for Sample Efficiency</ins></a></li><li><a href="#refining-rewards-and-policy-regularization" class="table-of-contents__link toc-highlight"><ins>Refining Rewards and Policy Regularization</ins></a></li></ul></li><li><a href="#-upcoming-features" class="table-of-contents__link toc-highlight">ğŸ”® Upcoming Features</a></li><li><a href="#-notable-work-based-on-roll" class="table-of-contents__link toc-highlight">ğŸ† Notable work based on ROLL</a></li><li><a href="#-citation-and-acknowledgement" class="table-of-contents__link toc-highlight">ğŸ™ Citation and Acknowledgement</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Examples</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/ROLL/docs/ç®€ä½“ä¸­æ–‡/å¿«é€Ÿå¼€å§‹/single_node_quick_start_cn">ROLLå•æœºå®è·µæ‰‹å†Œ</a></li><li class="footer__item"><a class="footer__link-item" href="/ROLL/docs/ç®€ä½“ä¸­æ–‡/å¿«é€Ÿå¼€å§‹/config_guide_cn">é…ç½®æŒ‡å—</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/alibaba/ROLL" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2025 Alibaba.</div></div></div></footer></div>
<script src="/ROLL/assets/js/runtime~main.c12cdc20.js"></script>
<script src="/ROLL/assets/js/main.9e947d7a.js"></script>
</body>
</html>