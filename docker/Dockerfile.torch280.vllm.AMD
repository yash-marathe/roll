# ==============================================================================
# Dockerfile.torch280.vllm.AMD
# ROCm-based PyTorch + vLLM image for AMD GPUs (MI300X / gfx942)
# Base: rocm/pytorch-training:v26.1
#   - ROCm 7.1.0  |  PyTorch 2.10.0.dev+rocm7.1  |  Python 3.10
#   - Flash Attention 2.8.3  |  Triton 3.4.0  |  Transformer Engine 2.4.0
# ==============================================================================

FROM rocm/pytorch-training:v26.1

# ── shell & frontend ─────────────────────────────────────────────────────────
SHELL ["/bin/bash", "-o", "pipefail", "-c"]
ENV DEBIAN_FRONTEND=noninteractive

# ── ROCm / HIP environment variables ─────────────────────────────────────────
ENV ROCM_PATH=/opt/rocm \
    HIP_PATH=/opt/rocm \
    HIP_CLANG_PATH=/opt/rocm/llvm/bin \
    ROCM_HOME=/opt/rocm \
    # Target architecture: MI300X (gfx942). Adjust for other GPUs.
    PYTORCH_ROCM_ARCH=gfx942 \
    AMDGPU_TARGETS=gfx942 \
    HCC_AMDGPU_TARGET=gfx942 \
    HSA_OVERRIDE_GFX_VERSION=9.4.2 \
    # RCCL / collective-communication tuning
    HSA_NO_SCRATCH_RECLAIM=1 \
    NCCL_DEBUG=WARN \
    # HIP runtime behaviour
    HIP_FORCE_DEV_KERNARG=1 \
    GPU_MAX_HW_QUEUES=4 \
    # Ensure ROCm libs are on the search path
    LD_LIBRARY_PATH=/opt/rocm/lib:${LD_LIBRARY_PATH} \
    PATH=/opt/rocm/bin:/opt/rocm/llvm/bin:${PATH} \
    PYTHONPATH=""

# ── install uv (fast Python package manager) ─────────────────────────────────
COPY --from=ghcr.io/astral-sh/uv:0.10.2 /uv /uvx /usr/local/bin/
# The base image uses /opt/venv (Python 3.10, ROCm torch 2.10.0.dev+rocm7.1).
ENV VIRTUAL_ENV=/opt/venv \
    PATH=/opt/venv/bin:${PATH} \
    UV_LINK_MODE=copy \
    UV_COMPILE_BYTECODE=1

# ── protect the ROCm torch from being overwritten ────────────────────────────
# Transitive deps (ray → cupy-cuda12x → nvidia-*) will pull in CUDA torch
# from PyPI, overwriting the base image's ROCm build.  UV_EXCLUDE tells the
# resolver to skip these packages entirely during every uv pip install.
RUN printf '%s\n' \
        "torch" \
        "torchvision" \
        "torchaudio" \
        "nvidia-cublas-cu12" \
        "nvidia-cuda-cupti-cu12" \
        "nvidia-cuda-nvrtc-cu12" \
        "nvidia-cuda-runtime-cu12" \
        "nvidia-cudnn-cu12" \
        "nvidia-cufft-cu12" \
        "nvidia-curand-cu12" \
        "nvidia-cusolver-cu12" \
        "nvidia-cusparse-cu12" \
        "nvidia-cusparselt-cu12" \
        "nvidia-nccl-cu12" \
        "nvidia-nvjitlink-cu12" \
        "nvidia-nvtx-cu12" \
    > /tmp/uv-exclude.txt
ENV UV_EXCLUDE=/tmp/uv-exclude.txt

# ── system packages ──────────────────────────────────────────────────────────
RUN apt-get update && apt-get install -y --no-install-recommends \
        git \
        cmake \
        ninja-build \
        zip \
    && rm -rf /var/lib/apt/lists/*

# ── Python core dependencies ─────────────────────────────────────────────────
# Grouped by category; every package pinned to the latest compatible version.
RUN uv pip install \
        # --- ML / inference framework deps ---
        "ray[cgraph]==2.53.0" \
        "pydantic==2.12.5" \
        "huggingface-hub==1.4.1" \
        "tensordict==0.11.0" \
        "modelscope==1.34.0" \
        "loralib==0.1.2" \
        # --- RL / gym ---
        "gym==0.26.2" \
        "gymnasium[toy-text]==1.2.3" \
        "gym_sokoban" \
        # --- math / evaluation ---
        # NOTE: math-verify pulls latex2sympy2_extended which supersedes latex2sympy2.
        # Do NOT install latex2sympy2 alongside math-verify (antlr4 version conflict).
        "math-verify==0.9.0" \
        "pycocotools==2.0.11" \
        # --- experiment tracking ---
        "swanlab==0.7.8" \
        # --- config / utilities ---
        "hydra-core==1.3.2" \
        "isort==7.0.0" \
        "jsonlines==4.0.0" \
        "pyext==0.5" \
        "dacite==1.9.2" \
        "codetiming==1.4.0" \
        # --- build helpers (needed for vllm) ---
        "setuptools>=77.0.3,<80.0.0" \
        "setuptools-scm>=8" \
        "packaging>=24.2" \
        "wheel"

# ── build & install vLLM from source (ROCm 7.1 + Python 3.10) ───────────────
# No pre-built wheel exists for ROCm 7.1 / cp310; build from the v0.15.1 tag.
ARG VLLM_VERSION=v0.15.1
RUN git clone --depth 1 --branch ${VLLM_VERSION} \
        https://github.com/vllm-project/vllm.git /tmp/vllm \
    && cd /tmp/vllm && git fetch --tags --depth 1

WORKDIR /tmp/vllm

# Strip torch lines from vLLM's own requirement files as a second safeguard.
RUN python use_existing_torch.py

# Install ROCm-specific deps (rocm.txt → common.txt).
RUN uv pip install -r requirements/rocm.txt

# Install AMD SMI bindings shipped with ROCm.
RUN uv pip install /opt/rocm/share/amd_smi || true

# Verify the ROCm torch survived intact.
RUN python -c "import torch; assert torch.version.hip, \
    f'Expected ROCm torch but got cuda={torch.version.cuda}'; \
    print(f'OK: torch {torch.__version__}  hip={torch.version.hip}')"

# Build vLLM wheel and install it (not editable, so /tmp/vllm can be deleted).
# VLLM_TARGET_DEVICE=rocm forces the ROCm code path in setup.py.
RUN --mount=type=cache,target=/root/.cache/uv \
    VLLM_TARGET_DEVICE=rocm \
    MAX_JOBS="$(nproc)" \
    python setup.py bdist_wheel --dist-dir=dist \
    && uv pip install dist/*.whl

WORKDIR /

# ── clone ROLL & install mcore_adapter ───────────────────────────────────────
RUN git clone --depth 1 https://github.com/alibaba/ROLL.git /app/ROLL

WORKDIR /app/ROLL
RUN uv pip install ./mcore_adapter
ENV PYTHONPATH=/app/ROLL:${PYTHONPATH}

# ── post-install patches (conditional — skip if file/pattern is absent) ───────
RUN SITE=$(python -c "import site; print(site.getsitepackages()[0])") \
    # 1. Raise Flash Attention version cap in Transformer Engine (older TE only).
    && { [ -f "${SITE}/transformer_engine/pytorch/attention.py" ] \
         && sed -i 's/_flash_attn_max_version = PkgVersion("2\.7\.3")/_flash_attn_max_version = PkgVersion("3.0.0.post1")/' \
                "${SITE}/transformer_engine/pytorch/attention.py" \
         || echo "SKIP: transformer_engine/pytorch/attention.py not found (v2.6+ uses attention/ package)"; } \
    # 2. Disable standalone-compile guard in vLLM compilation backend.
    && { [ -f "${SITE}/vllm/compilation/backends.py" ] \
         && sed -i -e '/^[[:space:]]*if envs.VLLM_USE_STANDALONE_COMPILE and is_torch_equal_or_newer(/ {' \
                -e 'N' \
                -e 's/^\([[:space:]]*\).*\n.*/\1if False:/' \
                -e '}' \
                "${SITE}/vllm/compilation/backends.py" \
         || echo "SKIP: vllm/compilation/backends.py not found"; } \
    # 3. Fix walrus-operator precedence in Ray's AMD GPU accelerator (older ray only).
    && { [ -f "${SITE}/ray/_private/accelerators/amd_gpu.py" ] \
         && grep -q 'cuda_val := os.environ.get' "${SITE}/ray/_private/accelerators/amd_gpu.py" \
         && sed -i \
             -e '/^[[:space:]]*if cuda_val := os.environ.get(CUDA_VISIBLE_DEVICES_ENV_VAR, None) is not None:/ s/^\([[:space:]]*if \)cuda_val := os.environ.get(CUDA_VISIBLE_DEVICES_ENV_VAR, None) is not None:/\1(cuda_val := os.environ.get(CUDA_VISIBLE_DEVICES_ENV_VAR, None)) is not None:/' \
             -e '/^[[:space:]]*if hip_val := os.environ.get(HIP_VISIBLE_DEVICES_ENV_VAR, None) is None:/ s/^\([[:space:]]*if \)hip_val:= os.environ.get(HIP_VISIBLE_DEVICES_ENV_VAR, None) is None:/\1(hip_val := os.environ.get(HIP_VISIBLE_DEVICES_ENV_VAR, None)) is None:/' \
             "${SITE}/ray/_private/accelerators/amd_gpu.py" \
         || echo "SKIP: Ray walrus-operator bug not present in this version"; }

# ── cleanup ──────────────────────────────────────────────────────────────────
RUN rm -rf /tmp/vllm /tmp/uv-exclude.txt /root/.cache/pip
ENV UV_EXCLUDE=

WORKDIR /app/ROLL
