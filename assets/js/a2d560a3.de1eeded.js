"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[8253],{3476:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var a=r(8168),t=(r(6540),r(5680));const i={},l="Group Relative Policy Optimization (GRPO)",o={unversionedId:"English/UserGuide/algorithms/GRPO",id:"English/UserGuide/algorithms/GRPO",title:"Group Relative Policy Optimization (GRPO)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/GRPO.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/GRPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/GRPO",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/GRPO.md",tags:[],version:"current",lastUpdatedAt:1756200489,formattedLastUpdatedAt:"Aug 26, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Agentic Asynchronous Parallel Rollout",permalink:"/ROLL/docs/English/UserGuide/agentic_async_parallel_rollout"},next:{title:"Group Sequence Policy Optimization (GSPO)",permalink:"/ROLL/docs/English/UserGuide/algorithms/GSPO"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"GRPO Configuration Parameters",id:"grpo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Differences Between GRPO and PPO",id:"differences-between-grpo-and-ppo",level:2},{value:"Reference Example",id:"reference-example",level:2}],g={toc:p},m="wrapper";function u({components:e,...n}){return(0,t.yg)(m,(0,a.A)({},g,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"group-relative-policy-optimization-grpo"},"Group Relative Policy Optimization (GRPO)"),(0,t.yg)("h2",{id:"introduction"},"Introduction"),(0,t.yg)("p",null,"Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that simplifies the training process by eliminating the need for a value function (critic) model. GRPO works as follows:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Group Sampling"),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.'),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Reward Assignment"),": Each solution is evaluated and assigned a reward based on its correctness or quality."),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Baseline Calculation"),": The average reward of the group serves as the baseline."),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Policy Update"),": The model updates its parameters by comparing each solution's reward to the group baseline, reinforcing solutions that are better than average and suppressing those that are worse than average.")),(0,t.yg)("p",null,"This approach reduces computational overhead by avoiding training a separate value estimation model, making the learning process more efficient."),(0,t.yg)("h2",{id:"grpo-configuration-parameters"},"GRPO Configuration Parameters"),(0,t.yg)("p",null,"In ROLL, the GRPO algorithm-specific configuration parameters are as follows (",(0,t.yg)("inlineCode",{parentName:"p"},"roll.pipeline.rlvr.rlvr_config.RLVRConfig"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'# grpo\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "grpo"\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# ppo related\n# advantage\nwhiten_advantages: true\nadvantage_clip: 2.0\ndual_clip_loss: true\n\n# clip\nreward_clip: 10\n# normalize\nreward_norm: null\nreward_shift: false\nreward_scale: false\n\n# reward\nadd_token_level_kl: false\n')),(0,t.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of prompts per rollout_batch_size"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group"),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"prompt_length"),": Maximum length of prompts"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"response_length"),": Maximum length of responses"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, set to "grpo"'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization rounds per batch of samples"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL divergence loss"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": KL-loss coefficient"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"loss_agg_mode"),': Loss aggregation mode, default is "seq-mean-token-sum", Literal','["token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"]')),(0,t.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,t.yg)("p",null,"The following parameters are common in PPO but also apply to GRPO:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dual_clip_loss"),": Whether to use dual clipping loss"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_clip"),": Reward value clipping range"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_norm"),": Reward normalization type"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_shift"),": Whether to only subtract mean in reward normalization"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_scale"),": Whether to only divide by standard deviation in reward normalization"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"add_token_level_kl"),": Whether to add token-level KL penalty")),(0,t.yg)("h2",{id:"differences-between-grpo-and-ppo"},"Differences Between GRPO and PPO"),(0,t.yg)("p",null,"The main differences between GRPO and traditional PPO algorithms are:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"No Critic Model Required"),": GRPO does not require training a separate value network (critic)"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Group Sampling"),": GRPO generates multiple completions (responses) for each prompt, rather than evaluating one rollout for each input"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Relative Rewards"),": Within each group, completions are scored and normalized based on group performance"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"KL Loss"),": GRPO performs regularization by directly adding the KL divergence between the training policy and reference policy to the loss function")),(0,t.yg)("h2",{id:"reference-example"},"Reference Example"),(0,t.yg)("p",null,"You can refer to the following configuration file to set up GRPO training:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_grpo.yaml"))))}u.isMDXComponent=!0},5680:(e,n,r)=>{r.d(n,{xA:()=>g,yg:()=>d});var a=r(6540);function t(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),r.push.apply(r,a)}return r}function l(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach(function(n){t(e,n,r[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))})}return e}function o(e,n){if(null==e)return{};var r,a,t=function(e,n){if(null==e)return{};var r,a,t={},i=Object.keys(e);for(a=0;a<i.length;a++)r=i[a],n.indexOf(r)>=0||(t[r]=e[r]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)r=i[a],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(t[r]=e[r])}return t}var s=a.createContext({}),p=function(e){var n=a.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):l(l({},n),e)),r},g=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},c=a.forwardRef(function(e,n){var r=e.components,t=e.mdxType,i=e.originalType,s=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),m=p(r),c=t,d=m["".concat(s,".").concat(c)]||m[c]||u[c]||i;return r?a.createElement(d,l(l({ref:n},g),{},{components:r})):a.createElement(d,l({ref:n},g))});function d(e,n){var r=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=r.length,l=new Array(i);l[0]=c;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o[m]="string"==typeof e?e:t,l[1]=o;for(var p=2;p<i;p++)l[p]=r[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,r)}c.displayName="MDXCreateElement"}}]);