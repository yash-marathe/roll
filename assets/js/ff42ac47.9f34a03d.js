"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[2478],{416:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>g,frontMatter:()=>r,metadata:()=>o,toc:()=>s});var t=a(8168),i=(a(6540),a(5680));const r={},l="Reward Feedback Learning (Reward FL)",o={unversionedId:"English/UserGuide/algorithms/Reward_FL",id:"English/UserGuide/algorithms/Reward_FL",title:"Reward Feedback Learning (Reward FL)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/Reward_FL.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/Reward_FL",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reward_FL",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/Reward_FL.md",tags:[],version:"current",lastUpdatedAt:1761894972,formattedLastUpdatedAt:"Oct 31, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Reinforce++",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus"},next:{title:"TOPR (Tapered Off-Policy REINFORCE)",permalink:"/ROLL/docs/English/UserGuide/algorithms/TOPR"}},d={},s=[{value:"Introduction",id:"introduction",level:2},{value:"Reward FL Configuration Parameters",id:"reward-fl-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"Wan2_2 Related Parameters",id:"wan2_2-related-parameters",level:3},{value:"Note",id:"note",level:2},{value:"Refernece Model",id:"refernece-model",level:2},{value:"Preprocess checkpoints",id:"preprocess-checkpoints",level:2},{value:"Setup environments",id:"setup-environments",level:2},{value:"Reference Example",id:"reference-example",level:2},{value:"Reference",id:"reference",level:2}],p={toc:s},m="wrapper";function g({components:e,...n}){return(0,i.yg)(m,(0,t.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"reward-feedback-learning-reward-fl"},"Reward Feedback Learning (Reward FL)"),(0,i.yg)("h2",{id:"introduction"},"Introduction"),(0,i.yg)("p",null,"Reward Feedback Learning (Reward FL) is a reinforcement learning algorithm that optimize diffusion models against a scorer. Reward Fl works as follows:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Sampling"),": For a given prompt and first frame latent, the model generates a corresponding video."),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Reward Assignment"),": Each video is evaluated and assigned a reward based on its face informations."),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Model Update"),": The model updates its parameters based on reward signals from the generated videos, reinforcing strategies that obtain higher rewards.")),(0,i.yg)("h2",{id:"reward-fl-configuration-parameters"},"Reward FL Configuration Parameters"),(0,i.yg)("p",null,"In ROLL, the Reward FL algorithm-specific configuration parameters are as follows (",(0,i.yg)("inlineCode",{parentName:"p"},"roll.pipeline.diffusion.reward_fl.reward_fl_config.RewardFLConfig"),"):"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'# reward fl\nlearning_rate: 2e-6\nlr_scheduler_type: constant\nper_device_train_batch_size: 1\ngradient_accumulation_steps: 1\nwarmup_steps: 10\nnum_train_epochs: 1\n\nmodel_name: "wan2_2"\n\n# wan2_2 related\nmodel_paths: ./examples/wan2.2-14B-reward_fl_ds/wan22_paths.json\nreward_model_path: /data/models/antelopev2/\ntokenizer_path: /data/models/Wan-AI/Wan2.1-T2V-1.3B/google/umt5-xxl/\nmodel_id_with_origin_paths: null\ntrainable_models: dit2\nuse_gradient_checkpointing_offload: true\nextra_inputs: input_image\nmax_timestep_boundary: 1.0\nmin_timestep_boundary: 0.9\nnum_inference_steps: 8\n')),(0,i.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"num_train_epochs"),": Number of optimization rounds per batch of samples"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"train_batch_size"),": Batch size for one train step.In deepspeed training the global train batch size is ",(0,i.yg)("inlineCode",{parentName:"li"},"per_device_train_batch_size")," ","*"," ",(0,i.yg)("inlineCode",{parentName:"li"},"gradient_accumulation_steps")," ","*"," world_size"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"learning_rate"),": Learning rate"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"per_device_train_batch_size"),": Training batch size per device"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gradient_accumulation_steps"),": Gradient accumulation steps"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"weight_decay"),": Weight decay coefficient"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"warmup_steps"),": Learning rate warmup steps"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"lr_scheduler_type"),": Learning rate scheduler type")),(0,i.yg)("h3",{id:"wan2_2-related-parameters"},"Wan2_2 Related Parameters"),(0,i.yg)("p",null,"The following parameters related to Wan2_2 are as follows:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"model_paths"),": Model path of json file, e.g., ",(0,i.yg)("inlineCode",{parentName:"li"},"wan22_paths.json"),", including high_noise_model, low_noise_model, text_encoder, vae."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"tokenizer_path"),": Tokenizer path. Leave empty to auto-download."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"reward_model_path"),": Reward model path, e.g., face model."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"max_timestep_boundary"),": Maximum value of the timestep interval, ranging from 0 to 1. Default is 1. This needs to be manually set only when training mixed models with multiple DiTs, for example, ",(0,i.yg)("a",{parentName:"li",href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"},"Wan-AI/Wan2.2-I2V-A14B"),"."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"min_timestep_boundary"),": Minimum value of the timestep interval, ranging from 0 to 1. Default is 1. This needs to be manually set only when training mixed models with multiple DiTs, for example, ",(0,i.yg)("a",{parentName:"li",href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"},"Wan-AI/Wan2.2-I2V-A14B"),"."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"model_id_with_origin_paths"),": Model ID with origin paths, e.g., Wan-AI/Wan2.1-T2V-1.3B:diffusion_pytorch_model*.safetensors. Comma-separated."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"trainable_models"),": Models to train, e.g., dit, vae, text_encoder."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"extra_inputs"),": Additional model inputs, comma-separated."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"use_gradient_checkpointing_offload"),": Whether to offload gradient checkpointing to CPU memory."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"num_inference_steps"),": Number of inference steps, default is 8 for the distilled wan2_2 model.")),(0,i.yg)("h2",{id:"note"},"Note"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"The reward model is constructed based on facial information, Please ensure that the first frame of the video contains a human face."),(0,i.yg)("li",{parentName:"ul"},"Download the reward model(antelopev2.zip) and unzip the onnx files to ",(0,i.yg)("inlineCode",{parentName:"li"},"reward_model_path")," directory."),(0,i.yg)("li",{parentName:"ul"},"Download the official Wan2.2 pipeline and Distilled Wan2.2 DiT safetensors. Put them in the ",(0,i.yg)("inlineCode",{parentName:"li"},"model_paths")," directory, e.g., ",(0,i.yg)("inlineCode",{parentName:"li"},"wan22_paths.json")," file."),(0,i.yg)("li",{parentName:"ul"},"According to the data/example_video_dataset/metadata.csv file, adapt your video dataset to the corresponding format")),(0,i.yg)("h2",{id:"refernece-model"},"Refernece Model"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"Official Wan2.2 pipeline"),": ",(0,i.yg)("a",{parentName:"li",href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"},"Wan-AI/Wan2.2-I2V-A14B")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"Distilled Wan2.2 DiT safetensors"),": ",(0,i.yg)("a",{parentName:"li",href:"https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main"},"lightx2v/Wan2.2-Lightning")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"Reward Model"),": ",(0,i.yg)("a",{parentName:"li",href:"https://github.com/deepinsight/insightface/releases/download/v0.7/antelopev2.zip"},"deepinsight/insightface")," ")),(0,i.yg)("h2",{id:"preprocess-checkpoints"},"Preprocess checkpoints"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Run ",(0,i.yg)("inlineCode",{parentName:"li"},"merge_model.py")," to merge multiple files of ",(0,i.yg)("inlineCode",{parentName:"li"},"Official Wan2.2 pipeline")," high noise model and low noise model into one file, respectively."),(0,i.yg)("li",{parentName:"ul"},"Run ",(0,i.yg)("inlineCode",{parentName:"li"},"merge_lora.py")," to merge ",(0,i.yg)("inlineCode",{parentName:"li"},"Distilled Wan2.2 DiT safetensors")," lora to the base model of ",(0,i.yg)("inlineCode",{parentName:"li"},"Official Wan2.2 pipeline")," high noise model and low noise model, respectively.")),(0,i.yg)("h2",{id:"setup-environments"},"Setup environments"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"pip install -r requirements_torch260_diffsynth.txt\n")),(0,i.yg)("h2",{id:"reference-example"},"Reference Example"),(0,i.yg)("p",null,"You can refer to the following configuration file to set up Reward FL training:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_reward_fl.yaml"))),(0,i.yg)("p",null,"Run ",(0,i.yg)("inlineCode",{parentName:"p"},"run_reward_fl_ds_pipeline.sh")," to get start."),(0,i.yg)("h2",{id:"reference"},"Reference"),(0,i.yg)("p",null,"[1]",": Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization. ",(0,i.yg)("a",{parentName:"p",href:"https://arxiv.org/abs/2510.14255"},"https://arxiv.org/abs/2510.14255")))}g.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>p,yg:()=>u});var t=a(6540);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,t)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach(function(n){i(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function o(e,n){if(null==e)return{};var a,t,i=function(e,n){if(null==e)return{};var a,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var d=t.createContext({}),s=function(e){var n=t.useContext(d),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},p=function(e){var n=s(e.components);return t.createElement(d.Provider,{value:n},e.children)},m="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef(function(e,n){var a=e.components,i=e.mdxType,r=e.originalType,d=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),m=s(a),c=i,u=m["".concat(d,".").concat(c)]||m[c]||g[c]||r;return a?t.createElement(u,l(l({ref:n},p),{},{components:a})):t.createElement(u,l({ref:n},p))});function u(e,n){var a=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=a.length,l=new Array(r);l[0]=c;var o={};for(var d in n)hasOwnProperty.call(n,d)&&(o[d]=n[d]);o.originalType=e,o[m]="string"==typeof e?e:i,l[1]=o;for(var s=2;s<r;s++)l[s]=a[s];return t.createElement.apply(null,l)}return t.createElement.apply(null,a)}c.displayName="MDXCreateElement"}}]);