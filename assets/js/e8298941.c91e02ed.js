"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[8419],{5680:(e,n,r)=>{r.d(n,{xA:()=>u,yg:()=>d});var a=r(6540);function t(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function l(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),r.push.apply(r,a)}return r}function i(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?l(Object(r),!0).forEach(function(n){t(e,n,r[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):l(Object(r)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))})}return e}function o(e,n){if(null==e)return{};var r,a,t=function(e,n){if(null==e)return{};var r,a,t={},l=Object.keys(e);for(a=0;a<l.length;a++)r=l[a],n.indexOf(r)>=0||(t[r]=e[r]);return t}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)r=l[a],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(t[r]=e[r])}return t}var s=a.createContext({}),p=function(e){var n=a.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):i(i({},n),e)),r},u=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},c="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef(function(e,n){var r=e.components,t=e.mdxType,l=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),c=p(r),m=t,d=c["".concat(s,".").concat(m)]||c[m]||g[m]||l;return r?a.createElement(d,i(i({ref:n},u),{},{components:r})):a.createElement(d,i({ref:n},u))});function d(e,n){var r=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var l=r.length,i=new Array(l);i[0]=m;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o[c]="string"==typeof e?e:t,i[1]=o;for(var p=2;p<l;p++)i[p]=r[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}m.displayName="MDXCreateElement"},9573:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>g,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var a=r(8168),t=(r(6540),r(5680));const l={},i="Reinforce++",o={unversionedId:"English/UserGuide/algorithms/Reinforce_Plus_Plus",id:"English/UserGuide/algorithms/Reinforce_Plus_Plus",title:"Reinforce++",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/Reinforce_Plus_Plus",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus.md",tags:[],version:"current",lastUpdatedAt:1758773047,formattedLastUpdatedAt:"Sep 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"RAFT++ (Reward rAnked Fine-Tuning)",permalink:"/ROLL/docs/English/UserGuide/algorithms/RAFT_Plus_Plus"},next:{title:"Reward Feedback Learning (Reward FL)",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reward_FL"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Reinforce++ Configuration Parameters",id:"reinforce-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}],u={toc:p},c="wrapper";function g({components:e,...n}){return(0,t.yg)(c,(0,a.A)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"reinforce"},"Reinforce++"),(0,t.yg)("h2",{id:"introduction"},"Introduction"),(0,t.yg)("p",null,"Reinforce++ is a policy gradient-based reinforcement learning algorithm that is an enhanced version of the classic REINFORCE algorithm. Reinforce++ works as follows:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Group Sampling"),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.'),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Reward Calculation"),": Each solution is evaluated and assigned a reward based on its correctness or quality."),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Policy Update"),": The model updates its parameters based on reward signals and generated sequences, reinforcing strategies that obtain higher rewards.")),(0,t.yg)("h2",{id:"reinforce-configuration-parameters"},"Reinforce++ Configuration Parameters"),(0,t.yg)("p",null,"In ROLL, the Reinforce++ algorithm-specific configuration parameters are as follows (",(0,t.yg)("inlineCode",{parentName:"p"},"roll.pipeline.rlvr.rlvr_config.RLVRConfig"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'# Reinforce++ core config\nadv_estimator: "reinforce"\n\n# normalize\nnorm_mean_type: batch\nnorm_std_type: batch\n\n# reward\nadd_token_level_kl: false\n\n# advantage\nwhiten_advantages: false\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# advantage\nadvantage_clip: 2.0\ndual_clip_loss: true\n# clip\nreward_clip: 10\n\n')),(0,t.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, set to "reinforce", which is the core configuration of the Reinforce++ algorithm'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"norm_mean_type"),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"norm_std_type"),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"add_token_level_kl"),": Whether to add token-level KL penalty, default value is false"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values, default value is false")),(0,t.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,t.yg)("p",null,"The following parameters are common configuration items for PPO-class algorithms:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of prompts per rollout_batch_size, default value is 64"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group"),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group), default value is 8"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"prompt_length"),": Maximum length of prompts, default value is 2048"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"response_length"),": Maximum length of responses, default value is 4096"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization rounds per batch of samples, default value is 1"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL divergence loss, default value is true"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": KL-loss coefficient, default value is 0.001"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"loss_agg_mode"),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range, default value is 2.0"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dual_clip_loss"),": Whether to use dual clipping loss, default value is true"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_clip"),": Reward value clipping range, default value is 10")),(0,t.yg)("h2",{id:"reference-example"},"Reference Example"),(0,t.yg)("p",null,"You can refer to the following configuration file to set up Reinforce++ training:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_reinforce_pp.yaml"))),(0,t.yg)("h2",{id:"references"},"References"),(0,t.yg)("p",null,"[1]"," ",(0,t.yg)("a",{parentName:"p",href:"https://arxiv.org/abs/2504.11343"},"https://arxiv.org/abs/2504.11343")))}g.isMDXComponent=!0}}]);