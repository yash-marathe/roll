"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[6037],{22153:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>t,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"User Guides/Algorithms/TOPR","title":"TOPR (Tapered Off-Policy REINFORCE)","description":"Introduction","source":"@site/docs/User Guides/Algorithms/TOPR.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/TOPR","permalink":"/ROLL/docs/User Guides/Algorithms/TOPR","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/TOPR.md","tags":[],"version":"current","lastUpdatedAt":1764581625000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Reward Feedback Learning (Reward FL)","permalink":"/ROLL/docs/User Guides/Algorithms/Reward_FL"},"next":{"title":"Tool Use Guide","permalink":"/ROLL/docs/User Guides/Agentic/Tool_Use"}}');var l=s(74848),r=s(28453);const o={},a="TOPR (Tapered Off-Policy REINFORCE)",t={},c=[{value:"Introduction",id:"introduction",level:2},{value:"TOPR Configuration Parameters",id:"topr-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"topr-tapered-off-policy-reinforce",children:"TOPR (Tapered Off-Policy REINFORCE)"})}),"\n",(0,l.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,l.jsx)(n.p,{children:"TOPR (Tapered Off-Policy REINFORCE) is a stable and efficient reinforcement learning algorithm designed for large language models. TOPR improves training stability and efficiency by combining off-policy mechanisms with tapering techniques. TOPR works as follows:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Off-policy Mechanism"}),": Utilizes historical data for training to improve sample efficiency."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Tapering Technique"}),": Stabilizes the training process by gradually reducing dependence on old policies."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Policy Update"}),": Updates policy parameters using a loss function that combines positive and negative samples."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"topr-configuration-parameters",children:"TOPR Configuration Parameters"}),"\n",(0,l.jsxs)(n.p,{children:["In ROLL, the TOPR algorithm-specific configuration parameters are as follows (",(0,l.jsx)(n.code,{children:"roll.pipeline.rlvr.rlvr_config.RLVRConfig"}),"):"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:'# TOPR core config\n# TOPR\nrl_loss_coef: 0.0\npositive_loss_coef: x_1 # x_1 > 0.0\nuse_topr_neg_loss_coef: x_2 # x_2 > 0.0\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 512  # prompt\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "gae"\nnum_return_sequences_in_group: 1\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n\nwhiten_advantages: true\nadvantage_clip: 2.0\nreward_clip: ~\ndual_clip_loss: true\nlambd: 0.95\ngamma: 1\npg_clip: 0.2\nvalue_clip: ~\nkl_penalty: "kl"\ntarget_kl: ~\ninit_kl_coef: 0.2\nkl_horizon: 10000\nadd_token_level_kl: false\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n'})}),"\n",(0,l.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"rl_loss_coef"}),": Reinforcement learning loss term coefficient, default value is 0.0"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"positive_loss_coef"}),": Positive sample loss term coefficient, needs to be set to a value greater than 0.0"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"use_topr_neg_loss_coef"}),": Negative sample loss term coefficient, needs to be set to a value greater than 0.0"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,l.jsx)(n.p,{children:"The following parameters are common configuration items for PPO-class algorithms:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"rollout_batch_size"}),": Number of prompts per rollout_batch_size, default value is 512"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"prompt_length"}),": Maximum length of prompts, default value is 2048"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"response_length"}),": Maximum length of responses, default value is 4096"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, optional values are "gae", "reinforce", "grpo", default value is "gae"']}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"num_return_sequences_in_group"}),": Number of responses generated per prompt (group size), default value is 1"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization rounds per batch of samples, default value is 1"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"use_kl_loss"}),": Whether to use KL divergence loss, default value is true"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"kl_loss_coef"}),": KL-loss coefficient, default value is 0.001"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"loss_agg_mode"}),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"']}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values, default value is true"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range, default value is 2.0"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"reward_clip"}),": Reward value clipping range, default value is ~ (means not set)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"dual_clip_loss"}),": Whether to use dual clipping loss, default value is true"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"lambd"}),": Lambda parameter in GAE estimator, used to trade off bias and variance, default value is 0.95"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"gamma"}),": Discount factor, default value is 1"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"pg_clip"}),": PPO clipping range, default value is 0.2"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"value_clip"}),": Value function clipping range, default value is ~ (means not set)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"kl_penalty"}),': KL penalty options, optional values are "kl", "abs", "mse", "full", default value is "kl"']}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"target_kl"}),": Target KL value for adaptive KL control, default value is ~ (means not set)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"init_kl_coef"}),": Initial KL penalty coefficient, default value is 0.2"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"kl_horizon"}),": Range for adaptive KL control, default value is 10000"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"add_token_level_kl"}),": Whether to add token-level KL penalty, default value is false"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"norm_mean_type"}),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"norm_std_type"}),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"reference-example",children:"Reference Example"}),"\n",(0,l.jsx)(n.p,{children:"You can refer to the following configuration file to set up TOPR training:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"./examples/docs_examples/example_topr.yaml"})}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,l.jsxs)(n.p,{children:["[1] Roux, N. L.; Bellemare, M. G.; Lebensold, J.; Bergeron, A.; Greaves, J.; Fr\xe9chette, A.; Pelletier, C.; Thibodeau-Laufer, E.; Toth, S.; Work, S. Tapered Off-Policy REINFORCE: Stable and Efficient Reinforcement Learning for LLMs. arXiv March 19, 2025. ",(0,l.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2503.14286",children:"https://doi.org/10.48550/arXiv.2503.14286"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(96540);const l={},r=i.createContext(l);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);