"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[8129],{28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>t});var s=r(96540);const i={},l=s.createContext(i);function o(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(l.Provider,{value:n},e.children)}},45947:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"User Guides/Algorithms/Reinforce_Plus_Plus","title":"Reinforce++","description":"Introduction","source":"@site/docs/User Guides/Algorithms/Reinforce_Plus_Plus.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/Reinforce_Plus_Plus","permalink":"/ROLL/docs/User Guides/Algorithms/Reinforce_Plus_Plus","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/Reinforce_Plus_Plus.md","tags":[],"version":"current","lastUpdatedAt":1764639784000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"RAFT++ (Reward rAnked Fine-Tuning)","permalink":"/ROLL/docs/User Guides/Algorithms/RAFT_Plus_Plus"},"next":{"title":"Reward Feedback Learning (Reward FL)","permalink":"/ROLL/docs/User Guides/Algorithms/Reward_FL"}}');var i=r(74848),l=r(28453);const o={},t="Reinforce++",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Reinforce++ Configuration Parameters",id:"reinforce-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"reinforce",children:"Reinforce++"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Reinforce++ is a policy gradient-based reinforcement learning algorithm that is an enhanced version of the classic REINFORCE algorithm. Reinforce++ works as follows:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Group Sampling"}),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reward Calculation"}),": Each solution is evaluated and assigned a reward based on its correctness or quality."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Policy Update"}),": The model updates its parameters based on reward signals and generated sequences, reinforcing strategies that obtain higher rewards."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"reinforce-configuration-parameters",children:"Reinforce++ Configuration Parameters"}),"\n",(0,i.jsxs)(n.p,{children:["In ROLL, the Reinforce++ algorithm-specific configuration parameters are as follows (",(0,i.jsx)(n.code,{children:"roll.pipeline.rlvr.rlvr_config.RLVRConfig"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Reinforce++ core config\nadv_estimator: "reinforce"\n\n# normalize\nnorm_mean_type: batch\nnorm_std_type: batch\n\n# reward\nadd_token_level_kl: false\n\n# advantage\nwhiten_advantages: false\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# advantage\nadvantage_clip: 2.0\ndual_clip_loss: true\n# clip\nreward_clip: 10\n\n'})}),"\n",(0,i.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, set to "reinforce", which is the core configuration of the Reinforce++ algorithm']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"norm_mean_type"}),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"norm_std_type"}),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"add_token_level_kl"}),": Whether to add token-level KL penalty, default value is false"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values, default value is false"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,i.jsx)(n.p,{children:"The following parameters are common configuration items for PPO-class algorithms:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"rollout_batch_size"}),": Number of prompts per rollout_batch_size, default value is 64"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num_return_sequences_in_group"}),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group), default value is 8"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt_length"}),": Maximum length of prompts, default value is 2048"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"response_length"}),": Maximum length of responses, default value is 4096"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization rounds per batch of samples, default value is 1"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"use_kl_loss"}),": Whether to use KL divergence loss, default value is true"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"kl_loss_coef"}),": KL-loss coefficient, default value is 0.001"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"loss_agg_mode"}),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range, default value is 2.0"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dual_clip_loss"}),": Whether to use dual clipping loss, default value is true"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"reward_clip"}),": Reward value clipping range, default value is 10"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"reference-example",children:"Reference Example"}),"\n",(0,i.jsx)(n.p,{children:"You can refer to the following configuration file to set up Reinforce++ training:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"./examples/docs_examples/example_reinforce_pp.yaml"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.p,{children:["[1] ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2504.11343",children:"https://arxiv.org/abs/2504.11343"})]})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);