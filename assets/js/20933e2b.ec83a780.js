"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[6749],{5680:(e,n,r)=>{r.d(n,{xA:()=>u,yg:()=>c});var a=r(6540);function t(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),r.push.apply(r,a)}return r}function l(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach(function(n){t(e,n,r[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))})}return e}function o(e,n){if(null==e)return{};var r,a,t=function(e,n){if(null==e)return{};var r,a,t={},i=Object.keys(e);for(a=0;a<i.length;a++)r=i[a],n.indexOf(r)>=0||(t[r]=e[r]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)r=i[a],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(t[r]=e[r])}return t}var s=a.createContext({}),p=function(e){var n=a.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):l(l({},n),e)),r},u=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef(function(e,n){var r=e.components,t=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),g=p(r),d=t,c=g["".concat(s,".").concat(d)]||g[d]||m[d]||i;return r?a.createElement(c,l(l({ref:n},u),{},{components:r})):a.createElement(c,l({ref:n},u))});function c(e,n){var r=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=r.length,l=new Array(i);l[0]=d;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o[g]="string"==typeof e?e:t,l[1]=o;for(var p=2;p<i;p++)l[p]=r[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,r)}d.displayName="MDXCreateElement"},9009:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var a=r(8168),t=(r(6540),r(5680));const i={},l="RAFT++ (Reward rAnked Fine-Tuning)",o={unversionedId:"English/UserGuide/algorithms/RAFT_Plus_Plus",id:"English/UserGuide/algorithms/RAFT_Plus_Plus",title:"RAFT++ (Reward rAnked Fine-Tuning)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/RAFT_Plus_Plus.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/RAFT_Plus_Plus",permalink:"/ROLL/docs/English/UserGuide/algorithms/RAFT_Plus_Plus",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/RAFT_Plus_Plus.md",tags:[],version:"current",lastUpdatedAt:1758784255,formattedLastUpdatedAt:"Sep 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Proximal Policy Optimization (PPO)",permalink:"/ROLL/docs/English/UserGuide/algorithms/PPO"},next:{title:"Reinforce++",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"RAFT++ Configuration Parameters",id:"raft-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}],u={toc:p},g="wrapper";function m({components:e,...n}){return(0,t.yg)(g,(0,a.A)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"raft-reward-ranked-fine-tuning"},"RAFT++ (Reward rAnked Fine-Tuning)"),(0,t.yg)("h2",{id:"introduction"},"Introduction"),(0,t.yg)("p",null,"RAFT++ (Reward rAnked Fine-Tuning) is a ranking-based reinforcement learning algorithm that optimizes policies by comparing rewards of different responses. RAFT++ works as follows:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Group Sampling"),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.'),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Reward Ranking"),": Each solution is evaluated and assigned a reward based on its correctness or quality, then ranked according to the rewards."),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Policy Update"),": The model updates its parameters by comparing rewards of different solutions within the group, reinforcing strategies that obtain higher rewards.")),(0,t.yg)("h2",{id:"raft-configuration-parameters"},"RAFT++ Configuration Parameters"),(0,t.yg)("p",null,"In ROLL, the RAFT++ algorithm-specific configuration parameters are as follows (",(0,t.yg)("inlineCode",{parentName:"p"},"roll.pipeline.rlvr.rlvr_config.RLVRConfig"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'# RAFT++ core config\nadv_estimator: "grpo"\n\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n\n# advantage\nwhiten_advantages: false\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# advantage\nadvantage_clip: 2.0\ndual_clip_loss: true\n# clip\nreward_clip: 10\n\n# reward\nadd_token_level_kl: false\n')),(0,t.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, set to "reinforce", which is the core configuration of RAFT++ algorithm'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"norm_mean_type"),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"norm_std_type"),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values, default value is false")),(0,t.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,t.yg)("p",null,"The following parameters are common configuration items for PPO-class algorithms:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of prompts per rollout_batch_size, default value is 64"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group"),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group), default value is 8"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"prompt_length"),": Maximum length of prompts, default value is 2048"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"response_length"),": Maximum length of responses, default value is 4096"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization rounds per batch of samples, default value is 1"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL divergence loss, default value is true"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": KL-loss coefficient, default value is 0.001"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"loss_agg_mode"),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range, default value is 2.0"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dual_clip_loss"),": Whether to use dual clipping loss, default value is true"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_clip"),": Reward value clipping range, default value is 10"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"add_token_level_kl"),": Whether to add token-level KL penalty, default value is false")),(0,t.yg)("h2",{id:"reference-example"},"Reference Example"),(0,t.yg)("p",null,"You can refer to the following configuration file to set up RAFT++ training:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_raft_pp.yaml"))),(0,t.yg)("h2",{id:"references"},"References"),(0,t.yg)("p",null,"[1]"," Xiong, W.; Yao, J.; Xu, Y.; Pang, B.; Wang, L.; Sahoo, D.; Li, J.; Jiang, N.; Zhang, T.; Xiong, C.; Dong, H. A Minimalist Approach to LLM Reasoning: From Rejection Sampling to Reinforce. arXiv April 15, 2025. ",(0,t.yg)("a",{parentName:"p",href:"https://doi.org/10.48550/arXiv.2504.11343"},"https://doi.org/10.48550/arXiv.2504.11343"),"."))}m.isMDXComponent=!0}}]);