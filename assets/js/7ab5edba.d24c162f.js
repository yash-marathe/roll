"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[1064],{5680:(e,n,t)=>{t.d(n,{xA:()=>s,yg:()=>d});var a=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var u=a.createContext({}),g=function(e){var n=a.useContext(u),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},s=function(e){var n=g(e.components);return a.createElement(u.Provider,{value:n},e.children)},c="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef(function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,u=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),c=g(t),m=i,d=c["".concat(u,".").concat(m)]||c[m]||p[m]||r;return t?a.createElement(d,o(o({ref:n},s),{},{components:t})):a.createElement(d,o({ref:n},s))});function d(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=m;var l={};for(var u in n)hasOwnProperty.call(n,u)&&(l[u]=n[u]);l.originalType=e,l[c]="string"==typeof e?e:i,o[1]=l;for(var g=2;g<r;g++)o[g]=t[g];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},7215:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>g});var a=t(8168),i=(t(6540),t(5680));const r={},o="FP8 Quantization Configuration Guide",l={unversionedId:"English/UserGuide/backend/fp8_rollout",id:"English/UserGuide/backend/fp8_rollout",title:"FP8 Quantization Configuration Guide",description:"This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage.",source:"@site/docs/English/UserGuide/backend/fp8_rollout.md",sourceDirName:"English/UserGuide/backend",slug:"/English/UserGuide/backend/fp8_rollout",permalink:"/ROLL/docs/English/UserGuide/backend/fp8_rollout",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/backend/fp8_rollout.md",tags:[],version:"current",lastUpdatedAt:1761894972,formattedLastUpdatedAt:"Oct 31, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"DeepSpeed Training Backend Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/deepspeed"},next:{title:"LoRA Fine-tuning Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/lora"}},u={},g=[{value:"Overview",id:"overview",level:2},{value:"actor_infer FP8 Configuration",id:"actor_infer-fp8-configuration",level:2},{value:"Basic Configuration",id:"basic-configuration",level:3},{value:"Dense Model Configuration",id:"dense-model-configuration",level:3},{value:"MoE Model Configuration",id:"moe-model-configuration",level:3},{value:"llm_judge FP8 Configuration",id:"llm_judge-fp8-configuration",level:2},{value:"Configuration Notes",id:"configuration-notes",level:2},{value:"Complete Example",id:"complete-example",level:2}],s={toc:g},c="wrapper";function p({components:e,...n}){return(0,i.yg)(c,(0,a.A)({},s,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"fp8-quantization-configuration-guide"},"FP8 Quantization Configuration Guide"),(0,i.yg)("p",null,"This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage."),(0,i.yg)("h2",{id:"overview"},"Overview"),(0,i.yg)("p",null,"FP8 quantization is an efficient numerical precision optimization technique that can significantly reduce model VRAM footprint and improve inference speed. ROLL supports FP8 quantization configuration for actor_infer and llm_judge components."),(0,i.yg)("h2",{id:"actor_infer-fp8-configuration"},"actor_infer FP8 Configuration"),(0,i.yg)("h3",{id:"basic-configuration"},"Basic Configuration"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n")),(0,i.yg)("h3",{id:"dense-model-configuration"},"Dense Model Configuration"),(0,i.yg)("p",null,"For Dense models, configuration requirements differ based on quantization method:"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Dense + Per Tensor Quantization (Default)")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Dense + Per Block Quantization")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n      hf_overrides:\n        quantization_config:\n          activation_scheme: dynamic\n          fmt: e4m3\n          quant_method: fp8\n          weight_block_size: [128, 128]  # Required: per block quantization\n")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Configuration Description:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"activation_scheme: dynamic"),": Use dynamic activation scheme"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"fmt: e4m3"),": Specify FP8 format as E4M3"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"quant_method: fp8"),": Set quantization method to FP8"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"weight_block_size: [128, 128]"),": Required for per block quantization, specifies weight block size")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Note:")," When specifying ",(0,i.yg)("inlineCode",{parentName:"p"},"weight_block_size"),", you must also provide ",(0,i.yg)("inlineCode",{parentName:"p"},"activation_scheme"),", ",(0,i.yg)("inlineCode",{parentName:"p"},"fmt"),", and ",(0,i.yg)("inlineCode",{parentName:"p"},"quant_method")," parameters, otherwise an error will occur."),(0,i.yg)("h3",{id:"moe-model-configuration"},"MoE Model Configuration"),(0,i.yg)("p",null,"For MoE (Mixture of Experts) models, ",(0,i.yg)("inlineCode",{parentName:"p"},"hf_overrides/quantization_config")," must be configured, and only per block quantization is supported:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n      hf_overrides:\n        quantization_config:\n          activation_scheme: dynamic\n          fmt: e4m3\n          quant_method: fp8\n          weight_block_size: [128, 128]  # Required: MoE models must use per block quantization\n")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Note:")," MoE models must use per block quantization. The ",(0,i.yg)("inlineCode",{parentName:"p"},"weight_block_size")," parameter is required, and you must also provide ",(0,i.yg)("inlineCode",{parentName:"p"},"activation_scheme"),", ",(0,i.yg)("inlineCode",{parentName:"p"},"fmt"),", and ",(0,i.yg)("inlineCode",{parentName:"p"},"quant_method")," parameters."),(0,i.yg)("h2",{id:"llm_judge-fp8-configuration"},"llm_judge FP8 Configuration"),(0,i.yg)("p",null,"LLM as judge model also supports FP8 quantization. Note that the judge model requires independent GPU resources and cannot share GPU with actor_infer:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"llm_judge:\n  # NOTE: llm as judge also needs GPU, cannot share GPU with actor infer\n  worker_cls: roll.pipeline.rlvr.rewards.llm_judge_reward_worker.LLMJudgeRewardWorker\n  judge_prompt: Qwen2.5-7B-Instruct-RLVR-prompt\n  judge_model_type: inference\n  tag_included: [RLVR]  \n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      quantization: fp8\n      max_model_len: 8000\n      load_format: auto\n")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Configuration Description:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gpu_memory_utilization: 0.8"),": Set VRAM utilization to 80%"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"quantization: fp8"),": Enable FP8 quantization"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"max_model_len: 8000"),": Maximum model length limit"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"load_format: auto"),": Automatically select loading format")),(0,i.yg)("h2",{id:"configuration-notes"},"Configuration Notes"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"GPU Resource Isolation"),": llm_judge requires independent GPU and cannot share with actor_infer"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"MoE Model Limitations"),": MoE models must use per block quantization, per tensor quantization is not supported"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Memory Optimization"),": FP8 quantization can significantly reduce memory usage, recommended for VRAM-constrained scenarios"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Performance Trade-off"),": While FP8 quantization improves performance, it may slightly affect model accuracy, requiring trade-offs based on specific scenarios")),(0,i.yg)("h2",{id:"complete-example"},"Complete Example"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"# Configuration example: FP8 quantization for actor_infer and llm_judge\nactor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n      hf_overrides:\n        quantization_config:\n          activation_scheme: dynamic\n          fmt: e4m3\n          quant_method: fp8\n          weight_block_size: [128, 128]\n\nllm_judge:\n  worker_cls: roll.pipeline.rlvr.rewards.llm_judge_reward_worker.LLMJudgeRewardWorker\n  judge_prompt: Qwen2.5-7B-Instruct-RLVR-prompt\n  judge_model_type: inference\n  tag_included: [RLVR]  \n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      quantization: fp8\n      max_model_len: 8000\n      load_format: auto\n")),(0,i.yg)("p",null,"With the above configuration, you can successfully enable FP8 quantization in ROLL to achieve better inference performance and VRAM efficiency."))}p.isMDXComponent=!0}}]);