"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[1504],{2402:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>g});var t=a(8168),r=(a(6540),a(5680));const i={},o="LoRA Fine-tuning Configuration Guide",l={unversionedId:"English/UserGuide/backend/lora",id:"English/UserGuide/backend/lora",title:"LoRA Fine-tuning Configuration Guide",description:"LoRA (Low-Rank Adaptation) is an efficient parameter-efficient fine-tuning method that achieves parameter-efficient fine-tuning by adding low-rank matrices to pre-trained models. This document will provide detailed instructions on how to configure and use LoRA fine-tuning in the ROLL framework.",source:"@site/docs/English/UserGuide/backend/lora.md",sourceDirName:"English/UserGuide/backend",slug:"/English/UserGuide/backend/lora",permalink:"/ROLL/docs/English/UserGuide/backend/lora",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/backend/lora.md",tags:[],version:"current",lastUpdatedAt:1755444305,formattedLastUpdatedAt:"Aug 17, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"DeepSpeed Training Backend Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/deepspeed"},next:{title:"Megatron Inference and Training Backend Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/megatron"}},p={},g=[{value:"LoRA Introduction",id:"lora-introduction",level:2},{value:"Configuring LoRA Fine-tuning",id:"configuring-lora-fine-tuning",level:2},{value:"Configuration Example",id:"configuration-example",level:3},{value:"Configuration Parameter Details",id:"configuration-parameter-details",level:3},{value:"LoRA Compatibility with Training Backends",id:"lora-compatibility-with-training-backends",level:2},{value:"Performance Optimization Recommendations",id:"performance-optimization-recommendations",level:2},{value:"Notes",id:"notes",level:2}],m={toc:g},s="wrapper";function u({components:e,...n}){return(0,r.yg)(s,(0,t.A)({},m,n,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"lora-fine-tuning-configuration-guide"},"LoRA Fine-tuning Configuration Guide"),(0,r.yg)("p",null,"LoRA (Low-Rank Adaptation) is an efficient parameter-efficient fine-tuning method that achieves parameter-efficient fine-tuning by adding low-rank matrices to pre-trained models. This document will provide detailed instructions on how to configure and use LoRA fine-tuning in the ROLL framework."),(0,r.yg)("h2",{id:"lora-introduction"},"LoRA Introduction"),(0,r.yg)("p",null,"LoRA achieves parameter-efficient fine-tuning through the following approaches:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Low-Rank Matrix Decomposition"),": Decompose weight update matrices into the product of two low-rank matrices"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Parameter Efficiency"),": Train only a small number of additional parameters instead of all model parameters"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Easy Deployment"),": Fine-tuned models can be easily merged into the original model")),(0,r.yg)("h2",{id:"configuring-lora-fine-tuning"},"Configuring LoRA Fine-tuning"),(0,r.yg)("p",null,"In the ROLL framework, LoRA fine-tuning can be configured by setting relevant parameters in the YAML configuration file."),(0,r.yg)("h3",{id:"configuration-example"},"Configuration Example"),(0,r.yg)("p",null,"The following is a typical LoRA configuration example (from ",(0,r.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-7B-rlvr_megatron/rlvl_lora_zero3.yaml"),"):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"# LoRA global configuration\nlora_target: o_proj,q_proj,k_proj,v_proj\nlora_rank: 32\nlora_alpha: 32\n\nactor_train:\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: true\n    dtype: bf16\n    lora_target: ${lora_target}\n    lora_rank: ${lora_rank}\n    lora_alpha: ${lora_alpha}\n    model_type: ~\n  training_args:\n    learning_rate: 1.0e-5\n    weight_decay: 0\n    per_device_train_batch_size: 1\n    gradient_accumulation_steps: 32\n    warmup_steps: 20\n    num_train_epochs: 50\n  strategy_args:\n    strategy_name: deepspeed_train\n    strategy_config: ${deepspeed_zero3}\n  device_mapping: list(range(0,16))\n  infer_batch_size: 4\n\nactor_infer:\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: true\n    dtype: bf16\n    lora_target: ${lora_target}\n    lora_rank: ${lora_rank}\n    lora_alpha: ${lora_alpha}\n  generating_args:\n    max_new_tokens: ${response_length}\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: ${num_return_sequences_in_group}\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.6\n      enforce_eager: false\n      block_size: 16\n      max_model_len: 8000\n  device_mapping: list(range(0,12))\n  infer_batch_size: 1\n")),(0,r.yg)("h3",{id:"configuration-parameter-details"},"Configuration Parameter Details"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"lora_target"),": Specify the model layers to apply LoRA"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"For example: ",(0,r.yg)("inlineCode",{parentName:"li"},"o_proj,q_proj,k_proj,v_proj")," means applying LoRA to the output projection and query, key, value projection layers in the attention mechanism"),(0,r.yg)("li",{parentName:"ul"},"Can be adjusted according to the specific model structure"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"lora_rank"),": Rank of the LoRA matrix"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Controls the size of the LoRA matrix"),(0,r.yg)("li",{parentName:"ul"},"Smaller ranks can reduce the number of parameters but may affect performance"),(0,r.yg)("li",{parentName:"ul"},"Usually set to 8, 16, 32, 64, etc."))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"lora_alpha"),": LoRA scaling factor"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Controls the magnitude of LoRA updates"),(0,r.yg)("li",{parentName:"ul"},"Usually set to the same as ",(0,r.yg)("inlineCode",{parentName:"li"},"lora_rank")," or its multiple"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"LoRA Parameters in model_args"),":"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"lora_target"),": Specify the layers to apply LoRA"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"lora_rank"),": Rank of the LoRA matrix"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"lora_alpha"),": LoRA scaling factor")))),(0,r.yg)("h2",{id:"lora-compatibility-with-training-backends"},"LoRA Compatibility with Training Backends"),(0,r.yg)("p",null,"Currently, LoRA fine-tuning only supports the DeepSpeed training backend:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_train:\n  strategy_args:\n    strategy_name: deepspeed_train  # LoRA only supports deepspeed_train\n")),(0,r.yg)("p",null,"This is because DeepSpeed provides optimization features that integrate well with LoRA."),(0,r.yg)("h2",{id:"performance-optimization-recommendations"},"Performance Optimization Recommendations"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Selecting Appropriate LoRA Layers"),":"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Applying LoRA to attention mechanism-related layers usually works well"),(0,r.yg)("li",{parentName:"ul"},"The best LoRA layer combination can be determined through experimentation"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Adjusting LoRA Parameters"),":"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"lora_rank"),": Adjust according to model size and task complexity"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"lora_alpha"),": Usually set to ",(0,r.yg)("inlineCode",{parentName:"li"},"lora_rank")," or its multiple"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Learning Rate Setting"),":"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"LoRA fine-tuning usually requires a higher learning rate"),(0,r.yg)("li",{parentName:"ul"},"Set to ",(0,r.yg)("inlineCode",{parentName:"li"},"1.0e-5")," in the example")))),(0,r.yg)("h2",{id:"notes"},"Notes"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"LoRA fine-tuning currently only supports the DeepSpeed training backend"),(0,r.yg)("li",{parentName:"ol"},"Ensure the model supports LoRA fine-tuning"),(0,r.yg)("li",{parentName:"ol"},"Pay attention to compatibility with LoRA when using gradient checkpointing"),(0,r.yg)("li",{parentName:"ol"},"LoRA fine-tuning performance may differ from full parameter fine-tuning and needs to be evaluated according to specific tasks")),(0,r.yg)("p",null,"By properly configuring LoRA fine-tuning, you can significantly reduce the number of training parameters and computational resource consumption while maintaining model performance."))}u.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>m,yg:()=>d});var t=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach(function(n){r(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function l(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=t.createContext({}),g=function(e){var n=t.useContext(p),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},m=function(e){var n=g(e.components);return t.createElement(p.Provider,{value:n},e.children)},s="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef(function(e,n){var a=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),s=g(a),c=r,d=s["".concat(p,".").concat(c)]||s[c]||u[c]||i;return a?t.createElement(d,o(o({ref:n},m),{},{components:a})):t.createElement(d,o({ref:n},m))});function d(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=c;var l={};for(var p in n)hasOwnProperty.call(n,p)&&(l[p]=n[p]);l.originalType=e,l[s]="string"==typeof e?e:r,o[1]=l;for(var g=2;g<i;g++)o[g]=a[g];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}c.displayName="MDXCreateElement"}}]);