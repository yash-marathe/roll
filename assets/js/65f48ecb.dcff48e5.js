"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[7048],{28453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(96540);const s={},o=i.createContext(s);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(o.Provider,{value:n},e.children)}},70418:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"User Guides/Algorithms/GRPO","title":"Group Relative Policy Optimization (GRPO)","description":"Introduction","source":"@site/docs/User Guides/Algorithms/GRPO.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/GRPO","permalink":"/ROLL/docs/User Guides/Algorithms/GRPO","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/GRPO.md","tags":[],"version":"current","lastUpdatedAt":1764905933000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"RLVR Pipeline for VLM","permalink":"/ROLL/docs/User Guides/Pipeline/vl_rlvr_pipeline_start"},"next":{"title":"Group Sequence Policy Optimization (GSPO)","permalink":"/ROLL/docs/User Guides/Algorithms/GSPO"}}');var s=r(74848),o=r(28453);const t={},l="Group Relative Policy Optimization (GRPO)",a={},d=[{value:"Introduction",id:"introduction",level:2},{value:"GRPO Configuration Parameters",id:"grpo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Differences Between GRPO and PPO",id:"differences-between-grpo-and-ppo",level:2},{value:"Reference Example",id:"reference-example",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"group-relative-policy-optimization-grpo",children:"Group Relative Policy Optimization (GRPO)"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm that simplifies the training process by eliminating the need for a value function (critic) model. GRPO works as follows:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Group Sampling"}),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward Assignment"}),": Each solution is evaluated and assigned a reward based on its correctness or quality."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Baseline Calculation"}),": The average reward of the group serves as the baseline."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Policy Update"}),": The model updates its parameters by comparing each solution's reward to the group baseline, reinforcing solutions that are better than average and suppressing those that are worse than average."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This approach reduces computational overhead by avoiding training a separate value estimation model, making the learning process more efficient."}),"\n",(0,s.jsx)(n.h2,{id:"grpo-configuration-parameters",children:"GRPO Configuration Parameters"}),"\n",(0,s.jsxs)(n.p,{children:["In ROLL, the GRPO algorithm-specific configuration parameters are as follows (",(0,s.jsx)(n.code,{children:"roll.pipeline.rlvr.rlvr_config.RLVRConfig"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# grpo\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "grpo"\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# ppo related\n# advantage\nwhiten_advantages: true\nadvantage_clip: 2.0\ndual_clip_loss: true\n\n# clip\nreward_clip: 10\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n\n# reward\nadd_token_level_kl: false\n'})}),"\n",(0,s.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rollout_batch_size"}),": Number of prompts per rollout_batch_size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"num_return_sequences_in_group"}),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt_length"}),": Maximum length of prompts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"response_length"}),": Maximum length of responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, set to "grpo"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization rounds per batch of samples"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_kl_loss"}),": Whether to use KL divergence loss"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"kl_loss_coef"}),": KL-loss coefficient"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"loss_agg_mode"}),': Loss aggregation mode, default is "seq-mean-token-sum", Literal["token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"]']}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,s.jsx)(n.p,{children:"The following parameters are common in PPO but also apply to GRPO:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"dual_clip_loss"}),": Whether to use dual clipping loss"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"reward_clip"}),": Reward value clipping range"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"norm_mean_type"}),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"norm_std_type"}),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"add_token_level_kl"}),": Whether to add token-level KL penalty"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"differences-between-grpo-and-ppo",children:"Differences Between GRPO and PPO"}),"\n",(0,s.jsx)(n.p,{children:"The main differences between GRPO and traditional PPO algorithms are:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No Critic Model Required"}),": GRPO does not require training a separate value network (critic)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Group Sampling"}),": GRPO generates multiple completions (responses) for each prompt, rather than evaluating one rollout for each input"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Relative Rewards"}),": Within each group, completions are scored and normalized based on group performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"KL Loss"}),": GRPO performs regularization by directly adding the KL divergence between the training policy and reference policy to the loss function"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"reference-example",children:"Reference Example"}),"\n",(0,s.jsx)(n.p,{children:"You can refer to the following configuration file to set up GRPO training:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"./examples/docs_examples/example_grpo.yaml"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);