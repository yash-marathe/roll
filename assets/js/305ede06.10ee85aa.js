"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[8851],{9759:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Algorithms/GSPO","title":"Group Sequence Policy Optimization (GSPO)","description":"Introduction","source":"@site/docs/User Guides/Algorithms/GSPO.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/GSPO","permalink":"/ROLL/docs/User Guides/Algorithms/GSPO","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/GSPO.md","tags":[],"version":"current","lastUpdatedAt":1764581625000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Group Relative Policy Optimization (GRPO)","permalink":"/ROLL/docs/User Guides/Algorithms/GRPO"},"next":{"title":"Lite PPO","permalink":"/ROLL/docs/User Guides/Algorithms/LitePPO"}}');var t=i(74848),s=i(28453);const o={},l="Group Sequence Policy Optimization (GSPO)",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"GSPO Configuration Parameters",id:"gspo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Differences Between GSPO and GRPO",id:"differences-between-gspo-and-grpo",level:2},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"group-sequence-policy-optimization-gspo",children:"Group Sequence Policy Optimization (GSPO)"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Group Sequence Policy Optimization (GSPO) is a reinforcement learning algorithm proposed by Alibaba's Qwen team for training large language models[^1]. GSPO works as follows:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sequence-Level Optimization"}),": Unlike algorithms such as GRPO, GSPO performs importance ratio calculation, reward assignment, and optimization at the sequence level rather than the token level."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Group Sampling"}),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reward Assignment"}),": Each solution is evaluated and assigned a reward based on its correctness or quality."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Baseline Calculation"}),": The average reward of the group serves as the baseline."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Policy Update"}),": The model updates its parameters by comparing each solution's reward to the group baseline."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"gspo-configuration-parameters",children:"GSPO Configuration Parameters"}),"\n",(0,t.jsx)(n.p,{children:"In ROLL, the GSPO algorithm-specific configuration parameters are as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# GSPO related\nadv_estimator: "grpo"\nimportance_sampling: seq\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\n\n# ppo related\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# advantage\nwhiten_advantages: false\nadvantage_clip: 2.0\ndual_clip_loss: true\n# clip\nreward_clip: 10\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n\n# reward\nadd_token_level_kl: false\n'})}),"\n",(0,t.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, set to "grpo"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"importance_sampling"}),': Importance sampling method, set to "seq" for sequence-level sampling']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rollout_batch_size"}),": Number of prompts per rollout_batch_size"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"num_return_sequences_in_group"}),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"prompt_length"}),": Maximum length of prompts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"response_length"}),": Maximum length of responses"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,t.jsx)(n.p,{children:"The following parameters are common in PPO but also apply to GSPO:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization rounds per batch of samples"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"use_kl_loss"}),": Whether to use KL divergence loss"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"kl_loss_coef"}),": KL-loss coefficient"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"loss_agg_mode"}),': Loss aggregation mode, default is "seq-mean-token-sum", Literal["token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"]']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"dual_clip_loss"}),": Whether to use dual clipping loss"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"reward_clip"}),": Reward value clipping range"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"norm_mean_type"}),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"norm_std_type"}),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"add_token_level_kl"}),": Whether to add token-level KL penalty"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"differences-between-gspo-and-grpo",children:"Differences Between GSPO and GRPO"}),"\n",(0,t.jsx)(n.p,{children:"Main differences between GSPO and GRPO algorithms:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Comparison Dimension"}),(0,t.jsx)(n.th,{children:"GRPO (Group Relative Policy Optimization)"}),(0,t.jsx)(n.th,{children:"GSPO (Group Sequence Policy Optimization)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Optimization Granularity"})}),(0,t.jsx)(n.td,{children:"Token-level optimization"}),(0,t.jsx)(n.td,{children:"Sequence-level optimization, consistent with reward calculation granularity"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Importance Ratio Calculation"})}),(0,t.jsx)(n.td,{children:"Based on token-level probability ratio calculation, each token independently calculates importance weights"}),(0,t.jsx)(n.td,{children:"Based on sequence-level probability ratio calculation, using geometric averaging for smoothing, calculating the joint probability ratio for the entire sequence"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Mixture of Experts (MoE) Support"})}),(0,t.jsx)(n.td,{children:"Unstable training in MoE models, requiring additional techniques to maintain expert activation consistency"}),(0,t.jsx)(n.td,{children:"Naturally supports MoE model training without additional techniques, as it only focuses on sequence-level likelihood"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Variance Control"})}),(0,t.jsx)(n.td,{children:"Due to per-token importance weight calculation, high variance noise is easily introduced"}),(0,t.jsx)(n.td,{children:"Significantly reduces variance through sequence-level importance sampling and length normalization"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Clipping Mechanism"})}),(0,t.jsx)(n.td,{children:"Clipping at the token level, potentially leading to inconsistent gradient updates"}),(0,t.jsx)(n.td,{children:"Clipping at the sequence level, providing more consistent and stable gradient updates"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"reference-example",children:"Reference Example"}),"\n",(0,t.jsx)(n.p,{children:"You can refer to the following configuration file to set up GSPO training:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"./examples/docs_examples/example_gspo.yaml"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.p,{children:['[1]: Qwen Team. "Group Sequence Policy Optimization." arXiv preprint arXiv:2507.18071 (2025). ',(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2507.18071",children:"https://arxiv.org/abs/2507.18071"})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var r=i(96540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);