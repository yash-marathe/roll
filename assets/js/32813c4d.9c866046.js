"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[9123],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var o=i(96540);const t={},r=o.createContext(t);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(r.Provider,{value:n},e.children)}},63198:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"User Guides/Configuration/fp8_rollout","title":"FP8 Quantization Configuration Guide","description":"This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage.","source":"@site/docs/User Guides/Configuration/fp8_rollout.md","sourceDirName":"User Guides/Configuration","slug":"/User Guides/Configuration/fp8_rollout","permalink":"/ROLL/docs/User Guides/Configuration/fp8_rollout","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Configuration/fp8_rollout.md","tags":[],"version":"current","lastUpdatedAt":1764581625000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ROLL Resource Configuration","permalink":"/ROLL/docs/User Guides/Configuration/device_mapping"},"next":{"title":"LoRA Fine-tuning Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/lora"}}');var t=i(74848),r=i(28453);const a={},s="FP8 Quantization Configuration Guide",l={},c=[{value:"Overview",id:"overview",level:2},{value:"actor_infer FP8 Configuration",id:"actor_infer-fp8-configuration",level:2},{value:"Basic Configuration",id:"basic-configuration",level:3},{value:"Dense Model Configuration",id:"dense-model-configuration",level:3},{value:"MoE Model Configuration",id:"moe-model-configuration",level:3},{value:"llm_judge FP8 Configuration",id:"llm_judge-fp8-configuration",level:2},{value:"Configuration Notes",id:"configuration-notes",level:2},{value:"Complete Example",id:"complete-example",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"fp8-quantization-configuration-guide",children:"FP8 Quantization Configuration Guide"})}),"\n",(0,t.jsx)(n.p,{children:"This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"FP8 quantization is an efficient numerical precision optimization technique that can significantly reduce model VRAM footprint and improve inference speed. ROLL supports FP8 quantization configuration for actor_infer and llm_judge components."}),"\n",(0,t.jsx)(n.h2,{id:"actor_infer-fp8-configuration",children:"actor_infer FP8 Configuration"}),"\n",(0,t.jsx)(n.h3,{id:"basic-configuration",children:"Basic Configuration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n"})}),"\n",(0,t.jsx)(n.h3,{id:"dense-model-configuration",children:"Dense Model Configuration"}),"\n",(0,t.jsx)(n.p,{children:"For Dense models, configuration requirements differ based on quantization method:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Dense + Per Tensor Quantization (Default)"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Dense + Per Block Quantization"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n      hf_overrides:\n        quantization_config:\n          activation_scheme: dynamic\n          fmt: e4m3\n          quant_method: fp8\n          weight_block_size: [128, 128]  # Required: per block quantization\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Configuration Description:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"activation_scheme: dynamic"}),": Use dynamic activation scheme"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"fmt: e4m3"}),": Specify FP8 format as E4M3"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"quant_method: fp8"}),": Set quantization method to FP8"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"weight_block_size: [128, 128]"}),": Required for per block quantization, specifies weight block size"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," When specifying ",(0,t.jsx)(n.code,{children:"weight_block_size"}),", you must also provide ",(0,t.jsx)(n.code,{children:"activation_scheme"}),", ",(0,t.jsx)(n.code,{children:"fmt"}),", and ",(0,t.jsx)(n.code,{children:"quant_method"})," parameters, otherwise an error will occur."]}),"\n",(0,t.jsx)(n.h3,{id:"moe-model-configuration",children:"MoE Model Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["For MoE (Mixture of Experts) models, ",(0,t.jsx)(n.code,{children:"hf_overrides/quantization_config"})," must be configured, and only per block quantization is supported:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n      hf_overrides:\n        quantization_config:\n          activation_scheme: dynamic\n          fmt: e4m3\n          quant_method: fp8\n          weight_block_size: [128, 128]  # Required: MoE models must use per block quantization\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," MoE models must use per block quantization. The ",(0,t.jsx)(n.code,{children:"weight_block_size"})," parameter is required, and you must also provide ",(0,t.jsx)(n.code,{children:"activation_scheme"}),", ",(0,t.jsx)(n.code,{children:"fmt"}),", and ",(0,t.jsx)(n.code,{children:"quant_method"})," parameters."]}),"\n",(0,t.jsx)(n.h2,{id:"llm_judge-fp8-configuration",children:"llm_judge FP8 Configuration"}),"\n",(0,t.jsx)(n.p,{children:"LLM as judge model also supports FP8 quantization. Note that the judge model requires independent GPU resources and cannot share GPU with actor_infer:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"llm_judge:\n  # NOTE: llm as judge also needs GPU, cannot share GPU with actor infer\n  worker_cls: roll.pipeline.rlvr.rewards.llm_judge_reward_worker.LLMJudgeRewardWorker\n  judge_prompt: Qwen2.5-7B-Instruct-RLVR-prompt\n  judge_model_type: inference\n  tag_included: [RLVR]  \n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      quantization: fp8\n      max_model_len: 8000\n      load_format: auto\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Configuration Description:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"gpu_memory_utilization: 0.8"}),": Set VRAM utilization to 80%"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"quantization: fp8"}),": Enable FP8 quantization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_model_len: 8000"}),": Maximum model length limit"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"load_format: auto"}),": Automatically select loading format"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"configuration-notes",children:"Configuration Notes"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU Resource Isolation"}),": llm_judge requires independent GPU and cannot share with actor_infer"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"MoE Model Limitations"}),": MoE models must use per block quantization, per tensor quantization is not supported"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Optimization"}),": FP8 quantization can significantly reduce memory usage, recommended for VRAM-constrained scenarios"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance Trade-off"}),": While FP8 quantization improves performance, it may slightly affect model accuracy, requiring trade-offs based on specific scenarios"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Configuration example: FP8 quantization for actor_infer and llm_judge\nactor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      quantization: fp8\n      hf_overrides:\n        quantization_config:\n          activation_scheme: dynamic\n          fmt: e4m3\n          quant_method: fp8\n          weight_block_size: [128, 128]\n\nllm_judge:\n  worker_cls: roll.pipeline.rlvr.rewards.llm_judge_reward_worker.LLMJudgeRewardWorker\n  judge_prompt: Qwen2.5-7B-Instruct-RLVR-prompt\n  judge_model_type: inference\n  tag_included: [RLVR]  \n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      quantization: fp8\n      max_model_len: 8000\n      load_format: auto\n"})}),"\n",(0,t.jsx)(n.p,{children:"With the above configuration, you can successfully enable FP8 quantization in ROLL to achieve better inference performance and VRAM efficiency."})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);