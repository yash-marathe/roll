"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[8419],{5680:(e,r,n)=>{n.d(r,{xA:()=>p,yg:()=>m});var a=n(6540);function t(e,r,n){return r in e?Object.defineProperty(e,r,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[r]=n,e}function l(e,r){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);r&&(a=a.filter(function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable})),n.push.apply(n,a)}return n}function i(e){for(var r=1;r<arguments.length;r++){var n=null!=arguments[r]?arguments[r]:{};r%2?l(Object(n),!0).forEach(function(r){t(e,r,n[r])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach(function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(n,r))})}return e}function o(e,r){if(null==e)return{};var n,a,t=function(e,r){if(null==e)return{};var n,a,t={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],r.indexOf(n)>=0||(t[n]=e[n]);return t}(e,r);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],r.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(t[n]=e[n])}return t}var s=a.createContext({}),u=function(e){var r=a.useContext(s),n=r;return e&&(n="function"==typeof e?e(r):i(i({},r),e)),n},p=function(e){var r=u(e.components);return a.createElement(s.Provider,{value:r},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var r=e.children;return a.createElement(a.Fragment,{},r)}},g=a.forwardRef(function(e,r){var n=e.components,t=e.mdxType,l=e.originalType,s=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),c=u(n),g=t,m=c["".concat(s,".").concat(g)]||c[g]||d[g]||l;return n?a.createElement(m,i(i({ref:r},p),{},{components:n})):a.createElement(m,i({ref:r},p))});function m(e,r){var n=arguments,t=r&&r.mdxType;if("string"==typeof e||t){var l=n.length,i=new Array(l);i[0]=g;var o={};for(var s in r)hasOwnProperty.call(r,s)&&(o[s]=r[s]);o.originalType=e,o[c]="string"==typeof e?e:t,i[1]=o;for(var u=2;u<l;u++)i[u]=n[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},9573:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>u});var a=n(8168),t=(n(6540),n(5680));const l={},i="Reinforce++",o={unversionedId:"English/UserGuide/algorithms/Reinforce_Plus_Plus",id:"English/UserGuide/algorithms/Reinforce_Plus_Plus",title:"Reinforce++",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/Reinforce_Plus_Plus",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus.md",tags:[],version:"current",lastUpdatedAt:1755682791,formattedLastUpdatedAt:"Aug 20, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"RAFT++ (Reward rAnked Fine-Tuning)",permalink:"/ROLL/docs/English/UserGuide/algorithms/RAFT_Plus_Plus"},next:{title:"TOPR (Tapered Off-Policy REINFORCE)",permalink:"/ROLL/docs/English/UserGuide/algorithms/TOPR"}},s={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Reinforce++ Configuration Parameters",id:"reinforce-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}],p={toc:u},c="wrapper";function d({components:e,...r}){return(0,t.yg)(c,(0,a.A)({},p,r,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"reinforce"},"Reinforce++"),(0,t.yg)("h2",{id:"introduction"},"Introduction"),(0,t.yg)("p",null,"Reinforce++ is a policy gradient-based reinforcement learning algorithm that is an enhanced version of the classic REINFORCE algorithm. Reinforce++ works as follows:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Group Sampling"),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.'),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Reward Calculation"),": Each solution is evaluated and assigned a reward based on its correctness or quality."),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Policy Update"),": The model updates its parameters based on reward signals and generated sequences, reinforcing strategies that obtain higher rewards.")),(0,t.yg)("h2",{id:"reinforce-configuration-parameters"},"Reinforce++ Configuration Parameters"),(0,t.yg)("p",null,"In ROLL, the Reinforce++ algorithm-specific configuration parameters are as follows (",(0,t.yg)("inlineCode",{parentName:"p"},"roll.pipeline.rlvr.rlvr_config.RLVRConfig"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'# Reinforce++ core config\nadv_estimator: "reinforce"\n\n# normalize\nreward_norm: batch\nreward_shift: false\nreward_scale: false\n\n# reward\nadd_token_level_kl: false\n\n# advantage\nwhiten_advantages: false\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-sum"\n\n# advantage\nadvantage_clip: 2.0\ndual_clip_loss: true\n# clip\nreward_clip: 10\n\n')),(0,t.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, set to "reinforce", which is the core configuration of the Reinforce++ algorithm'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_norm"),': Reward normalization type, optional values are "batch", "group", "running", null, default value is "batch"'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_shift"),": Whether to only subtract mean in reward normalization, default value is false"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_scale"),": Whether to only divide by standard deviation in reward normalization, default value is false"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"add_token_level_kl"),": Whether to add token-level KL penalty, default value is false"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values, default value is false")),(0,t.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,t.yg)("p",null,"The following parameters are common configuration items for PPO-class algorithms:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of prompts per rollout_batch_size, default value is 64"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group"),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group), default value is 8"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"prompt_length"),": Maximum length of prompts, default value is 2048"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"response_length"),": Maximum length of responses, default value is 4096"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization rounds per batch of samples, default value is 1"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL divergence loss, default value is true"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": KL-loss coefficient, default value is 0.001"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"loss_agg_mode"),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"'),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range, default value is 2.0"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dual_clip_loss"),": Whether to use dual clipping loss, default value is true"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_clip"),": Reward value clipping range, default value is 10")),(0,t.yg)("h2",{id:"reference-example"},"Reference Example"),(0,t.yg)("p",null,"You can refer to the following configuration file to set up Reinforce++ training:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_reinforce_pp.yaml"))),(0,t.yg)("h2",{id:"references"},"References"),(0,t.yg)("p",null,"[1]"," ",(0,t.yg)("a",{parentName:"p",href:"https://arxiv.org/abs/2504.11343"},"https://arxiv.org/abs/2504.11343")))}d.isMDXComponent=!0}}]);