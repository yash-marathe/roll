"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[2906],{5680:(e,n,a)=>{a.d(n,{xA:()=>p,yg:()=>d});var t=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach(function(n){r(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function l(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=t.createContext({}),g=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},p=function(e){var n=g(e.components);return t.createElement(s.Provider,{value:n},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},m=t.forwardRef(function(e,n){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=g(a),m=r,d=u["".concat(s,".").concat(m)]||u[m]||c[m]||i;return a?t.createElement(d,o(o({ref:n},p),{},{components:a})):t.createElement(d,o({ref:n},p))});function d(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:r,o[1]=l;for(var g=2;g<i;g++)o[g]=a[g];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}m.displayName="MDXCreateElement"},8084:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>g});var t=a(8168),r=(a(6540),a(5680));const i={},o="TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)",l={unversionedId:"English/UserGuide/algorithms/agentic_StarPO",id:"English/UserGuide/algorithms/agentic_StarPO",title:"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/agentic_StarPO.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/agentic_StarPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/agentic_StarPO",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/agentic_StarPO.md",tags:[],version:"current",lastUpdatedAt:1756200599,formattedLastUpdatedAt:"Aug 26, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)",permalink:"/ROLL/docs/English/UserGuide/algorithms/agentic_GiGPO"},next:{title:"Agentic Asynchronous Training Feature Usage Guide",permalink:"/ROLL/docs/English/UserGuide/async_training_agentic"}},s={},g=[{value:"Introduction",id:"introduction",level:2},{value:"StarPO Configuration Parameters",id:"starpo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Environment Manager Parameters",id:"environment-manager-parameters",level:3},{value:"Reference Examples",id:"reference-examples",level:2},{value:"References",id:"references",level:2}],p={toc:g},u="wrapper";function c({components:e,...n}){return(0,r.yg)(u,(0,t.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"trajwiselearningstarpo-state-thinking-actions-reward-policy-optimization"},"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)"),(0,r.yg)("h2",{id:"introduction"},"Introduction"),(0,r.yg)("p",null,"StarPO (State-Thinking-Actions-Reward Policy Optimization) is a reinforcement learning algorithm for LLM agent training. It optimizes by treating the entire multi-turn interaction trajectory (including observations, reasoning traces, actions, and feedback) as a coherent unit, rather than independently processing each action as in traditional methods."),(0,r.yg)("p",null,"The core idea of StarPO is trajectory-level optimization, which alternates between two phases:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Rollout Phase"),": Generate reasoning-interaction trajectories"),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Update Phase"),": Optimize the model based on complete trajectories")),(0,r.yg)("h2",{id:"starpo-configuration-parameters"},"StarPO Configuration Parameters"),(0,r.yg)("p",null,"In ROLL, the core implementation of StarPO is located at ",(0,r.yg)("inlineCode",{parentName:"p"},"roll/pipeline/agentic/utils.py"),". The specific configuration parameters for the StarPO algorithm are as follows (",(0,r.yg)("inlineCode",{parentName:"p"},"roll.pipeline.agentic.agentic_config.AgenticConfig"),"):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'# StarPO core config\n# StarPO related\nadv_estimator: "reinforce"\n\n# rollout_batch_size is the number of trajectories\nrollout_batch_size: 1024\nval_batch_size: 1024\nsequence_length: 1024\n\nadvantage_clip: 0.2\nppo_epochs: 1\n\n# pg_clip: 0.1\n#dual_clip_loss: True\ninit_kl_coef: 0.0\nwhiten_advantages: true\nentropy_loss_coef: 0\nmax_grad_norm: 1.0\n\nreward_normalization:\n  grouping: traj_group_id # Can be tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by calculates reward/adv\n  method: mean # asym_clip / identity / mean_std / mean\n\ntrain_env_manager:\n  max_env_num_per_worker: 16\n  num_env_groups: 128\n  # under the same group, the env config and env seed are ensured to be equal\n  group_size: 8 # grpo\'s grpo\n  tags: [FrozenLake]\n  num_groups_partition: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation\n\nenv_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager\n')),(0,r.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, set to "reinforce", which is the core configuration of the StarPO algorithm'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"env_manager_cls"),": Environment manager class, StarPO needs to use ",(0,r.yg)("inlineCode",{parentName:"li"},"roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager"))),(0,r.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,r.yg)("p",null,"The following parameters are common configuration items for PPO-class algorithms:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of trajectories per rollout batch, default value is 1024"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"val_batch_size"),": Validation batch size, default value is 1024"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"sequence_length"),": Maximum sequence length, default value is 1024"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range, default value is 0.2"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization epochs per batch of samples, default value is 1"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"init_kl_coef"),": Initial coefficient for KL penalty, default value is 0.0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values, default value is true"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"entropy_loss_coef"),": Entropy loss coefficient, default value is 0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"max_grad_norm"),": Maximum norm for gradient clipping, default value is 1.0")),(0,r.yg)("h3",{id:"environment-manager-parameters"},"Environment Manager Parameters"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.max_env_num_per_worker"),": Maximum number of environments per worker, default value is 16"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.num_env_groups"),": Number of training environment groups, default value is 128"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.group_size"),": Number of environments per group, default value is 8"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.tags"),": List of environment tags, default value is ","[FrozenLake]"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.num_groups_partition"),": Group allocation for each environment type, default value is ","[128]")),(0,r.yg)("h2",{id:"reference-examples"},"Reference Examples"),(0,r.yg)("p",null,"You can refer to the following configuration files to set up StarPO training:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"./examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml"))),(0,r.yg)("h2",{id:"references"},"References"),(0,r.yg)("p",null,"[1]"," Liu, T.; Feng, L.; An, B. StarPO: State-Regularized Policy Optimization for LLM Agent Training. arXiv 2025, 2504.20073."))}c.isMDXComponent=!0}}]);