"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[4296],{1374:(e,i,a)=>{a.r(i),a.d(i,{assets:()=>p,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>l,toc:()=>g});var n=a(8168),t=(a(6540),a(5680));const r={},o="Off-Policy Algorithms Configuration Guide",l={unversionedId:"English/UserGuide/algorithms/offpolicy_setting",id:"English/UserGuide/algorithms/offpolicy_setting",title:"Off-Policy Algorithms Configuration Guide",description:"The ROLL framework supports multiple Off-Policy algorithm variants for reinforcement learning training. This document provides detailed configuration methods and usage examples for various algorithms.",source:"@site/docs/English/UserGuide/algorithms/offpolicy_setting.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/offpolicy_setting",permalink:"/ROLL/docs/English/UserGuide/algorithms/offpolicy_setting",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/offpolicy_setting.md",tags:[],version:"current",lastUpdatedAt:1761894972,formattedLastUpdatedAt:"Oct 31, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"TOPR (Tapered Off-Policy REINFORCE)",permalink:"/ROLL/docs/English/UserGuide/algorithms/TOPR"},next:{title:"ROLL x Ascend",permalink:"/ROLL/docs/English/UserGuide/ascend/ascend_usage"}},p={},g=[{value:"Supported Algorithm Variants",id:"supported-algorithm-variants",level:2},{value:"Basic Configuration",id:"basic-configuration",level:2},{value:"Core Parameters",id:"core-parameters",level:3},{value:"Worker Configuration",id:"worker-configuration",level:3},{value:"Detailed Algorithm Configuration",id:"detailed-algorithm-configuration",level:2},{value:"1. Vanilla Policy Gradient",id:"1-vanilla-policy-gradient",level:3},{value:"2. PPO (Proximal Policy Optimization)",id:"2-ppo-proximal-policy-optimization",level:3},{value:"3. TIS (Truncated Importance Sampling)",id:"3-tis-truncated-importance-sampling",level:3},{value:"4. TOPR (Tapered off-policy REINFORCE)",id:"4-topr-tapered-off-policy-reinforce",level:3},{value:"5. CISPO (Clipped Importance Sampling Policy Optimization)",id:"5-cispo-clipped-importance-sampling-policy-optimization",level:3},{value:"6. Kimi15",id:"6-kimi15",level:3},{value:"Complete Configuration Example",id:"complete-configuration-example",level:2},{value:"Key Configuration Points",id:"key-configuration-points",level:3},{value:"Usage",id:"usage",level:3},{value:"Algorithm Selection Recommendations",id:"algorithm-selection-recommendations",level:2},{value:"Selection Based on Task Characteristics",id:"selection-based-on-task-characteristics",level:3},{value:"Selection Based on Data Distribution",id:"selection-based-on-data-distribution",level:3},{value:"Monitoring and Debugging",id:"monitoring-and-debugging",level:2},{value:"Key Metrics",id:"key-metrics",level:3},{value:"Debugging Recommendations",id:"debugging-recommendations",level:3},{value:"Frequently Asked Questions",id:"frequently-asked-questions",level:2},{value:"Q: How to choose the appropriate pg_variant?",id:"q-how-to-choose-the-appropriate-pg_variant",level:3},{value:"Q: What is the computational complexity of each algorithm?",id:"q-what-is-the-computational-complexity-of-each-algorithm",level:3},{value:"Q: Can I switch algorithms during training?",id:"q-can-i-switch-algorithms-during-training",level:3},{value:"Q: How to adjust algorithm-specific parameters?",id:"q-how-to-adjust-algorithm-specific-parameters",level:3}],s={toc:g},m="wrapper";function c({components:e,...i}){return(0,t.yg)(m,(0,n.A)({},s,i,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"off-policy-algorithms-configuration-guide"},"Off-Policy Algorithms Configuration Guide"),(0,t.yg)("p",null,"The ROLL framework supports multiple Off-Policy algorithm variants for reinforcement learning training. This document provides detailed configuration methods and usage examples for various algorithms."),(0,t.yg)("h2",{id:"supported-algorithm-variants"},"Supported Algorithm Variants"),(0,t.yg)("p",null,"The ROLL framework currently supports the following Off-Policy algorithms:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"vanilla")," - Basic Policy Gradient algorithm"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"ppo")," - Proximal Policy Optimization"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"tis")," - Truncated Importance Sampling"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"topr")," - Tapered off-policy REINFORCE"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"cispo")," - Clipped Importance Sampling Policy Optimization"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"kimi15")," - Kimi15 algorithm")),(0,t.yg)("h2",{id:"basic-configuration"},"Basic Configuration"),(0,t.yg)("h3",{id:"core-parameters"},"Core Parameters"),(0,t.yg)("p",null,"Set the basic parameters for Off-Policy algorithms in the configuration file:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'# Select algorithm variant\npg_variant: topr  # Options: vanilla, tis, topr, cispo, kimi15, ppo\n\n# Training configuration\nmax_steps: 500\nsave_steps: 100\nlogging_steps: 1\neval_steps: 10\n\n# Data configuration\nrollout_batch_size: 128\nprompt_length: 2048\nresponse_length: 8192\nnum_return_sequences_in_group: 8\n\n# Common training parameters\nppo_epochs: 1\nadv_estimator: "reinforce"\nwhiten_advantages: true\n')),(0,t.yg)("h3",{id:"worker-configuration"},"Worker Configuration"),(0,t.yg)("p",null,"Use the specialized ActorPGWorker to handle Off-Policy algorithms:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_train:\n  worker_cls: roll.pipeline.rlvr.actor_pg_worker.ActorPGWorker\n  pg_variant: topr  # Keep consistent with global configuration\n  model_args:\n    flash_attn: fa2\n    disable_gradient_checkpointing: false\n    dtype: bf16\n  training_args:\n    learning_rate: 1.0e-6\n    weight_decay: 0\n    per_device_train_batch_size: 1\n    gradient_accumulation_steps: 64\n    warmup_steps: 20\n    num_train_epochs: 50\n  strategy_args:\n    strategy_name: megatron_train\n    strategy_config:\n      tensor_model_parallel_size: 1\n      pipeline_model_parallel_size: 1\n      use_distributed_optimizer: true\n      recompute_granularity: full\n  device_mapping: list(range(0,16))\n")),(0,t.yg)("h2",{id:"detailed-algorithm-configuration"},"Detailed Algorithm Configuration"),(0,t.yg)("h3",{id:"1-vanilla-policy-gradient"},"1. Vanilla Policy Gradient"),(0,t.yg)("p",null,"The most basic policy gradient algorithm, directly using the product of log probability and advantage function as loss."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration Features:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"No additional parameters required"),(0,t.yg)("li",{parentName:"ul"},"High computational efficiency"),(0,t.yg)("li",{parentName:"ul"},"Suitable for simple reinforcement learning tasks")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: vanilla\n\n# No additional configuration parameters needed\n")),(0,t.yg)("h3",{id:"2-ppo-proximal-policy-optimization"},"2. PPO (Proximal Policy Optimization)"),(0,t.yg)("p",null,"Proximal Policy Optimization algorithm that stabilizes training by clipping importance sampling ratios."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Parameters:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: ppo\n\n# PPO specific parameters\npg_clip: 0.2                    # Clipping range\npg_clip_low: 0.2               # Lower bound clipping (optional)\npg_clip_high: 0.2              # Upper bound clipping (optional)\nuse_pg_clip_range: false       # Whether to use asymmetric clipping\ndual_clip_loss: true           # Whether to enable dual clipping\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration Example:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: ppo\npg_clip: 0.2\ndual_clip_loss: true\n")),(0,t.yg)("h3",{id:"3-tis-truncated-importance-sampling"},"3. TIS (Truncated Importance Sampling)"),(0,t.yg)("p",null,"Truncated Importance Sampling algorithm that limits importance sampling ratios to the range ","[0, 1]","."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Parameters:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: tis\n\n# TIS specific parameters\ntis_lower_bound: 0.0           # Lower bound\ntis_upper_bound: 1.0           # Upper bound\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration Example:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: tis\ntis_lower_bound: 0.0\ntis_upper_bound: 1.0\n")),(0,t.yg)("h3",{id:"4-topr-tapered-off-policy-reinforce"},"4. TOPR (Tapered off-policy REINFORCE)"),(0,t.yg)("p",null,"Tapered off-policy reinforcement learning algorithm that adopts different update strategies based on positive and negative rewards."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Algorithm Features:")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Positive samples: Direct SFT update without importance sampling"),(0,t.yg)("li",{parentName:"ul"},"Negative samples: TIS update with importance sampling ratio limited to ","[0, 1]")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Parameters:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: topr\n\n# TOPR specific parameters\ntopr_positive_weight: 1.0      # Positive sample weight\ntopr_negative_weight: 1.0      # Negative sample weight\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration Example:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: topr\ntopr_positive_weight: 1.0\ntopr_negative_weight: 1.0\n")),(0,t.yg)("h3",{id:"5-cispo-clipped-importance-sampling-policy-optimization"},"5. CISPO (Clipped Importance Sampling Policy Optimization)"),(0,t.yg)("p",null,"Clipped Importance Sampling Policy Optimization algorithm that uses clipped importance sampling weights and stop-gradient operations."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Parameters:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: cispo\n\n# CISPO specific parameters\ncispo_epsilon_low: 0.1         # Lower bound clipping parameter\ncispo_epsilon_high: 0.1        # Upper bound clipping parameter\ncispo_use_unified_mask: false  # Whether to use unified mask\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration Example:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: cispo\ncispo_epsilon_low: 0.1\ncispo_epsilon_high: 0.1\ncispo_use_unified_mask: false\n")),(0,t.yg)("h3",{id:"6-kimi15"},"6. Kimi15"),(0,t.yg)("p",null,"Policy gradient algorithm based on KL regularization, adding KL divergence regularization term to the policy gradient term."),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Key Parameters:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: kimi15\n\n# Kimi15 specific parameters\nkimi15_tau: 0.1               # Regularization parameter\n")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration Example:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: kimi15\nkimi15_tau: 0.1\n")),(0,t.yg)("h2",{id:"complete-configuration-example"},"Complete Configuration Example"),(0,t.yg)("p",null,"For a complete RLVR Off-Policy configuration example, please refer to:"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Configuration File"),": ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-7B-rlvr-offpolicy/rlvr_config.yaml")),(0,t.yg)("p",null,"This configuration file contains all necessary parameter settings and supports switching between different algorithm variants by modifying the ",(0,t.yg)("inlineCode",{parentName:"p"},"pg_variant")," parameter:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"pg_variant: topr  # Options: topr, vanilla, tis, cispo, kimi15, ppo\n")),(0,t.yg)("h3",{id:"key-configuration-points"},"Key Configuration Points"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Worker Configuration"),": Use ",(0,t.yg)("inlineCode",{parentName:"li"},"ActorPGWorker")," class to handle Off-Policy algorithms"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Algorithm Selection"),": Specify algorithm variant through ",(0,t.yg)("inlineCode",{parentName:"li"},"pg_variant")," parameter"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Model Configuration"),": Support Megatron training and SGLang inference"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Reward Configuration"),": Include mathematical rule reward model configuration")),(0,t.yg)("h3",{id:"usage"},"Usage"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Copy the configuration file to your working directory"),(0,t.yg)("li",{parentName:"ol"},"Modify ",(0,t.yg)("inlineCode",{parentName:"li"},"pg_variant")," and other parameters as needed"),(0,t.yg)("li",{parentName:"ol"},"Run the training script:")),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"python examples/start_rlvr_pipeline.py --config-path your_config.yaml\n")),(0,t.yg)("h2",{id:"algorithm-selection-recommendations"},"Algorithm Selection Recommendations"),(0,t.yg)("h3",{id:"selection-based-on-task-characteristics"},"Selection Based on Task Characteristics"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Simple Tasks"),": Use ",(0,t.yg)("inlineCode",{parentName:"p"},"vanilla")," or ",(0,t.yg)("inlineCode",{parentName:"p"},"ppo")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Low computational overhead"),(0,t.yg)("li",{parentName:"ul"},"Fast convergence"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Complex Reasoning Tasks"),": Use ",(0,t.yg)("inlineCode",{parentName:"p"},"topr")," or ",(0,t.yg)("inlineCode",{parentName:"p"},"cispo")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Better stability"),(0,t.yg)("li",{parentName:"ul"},"Suitable for long sequence generation"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Tasks Requiring Exploration"),": Use ",(0,t.yg)("inlineCode",{parentName:"p"},"tis")," or ",(0,t.yg)("inlineCode",{parentName:"p"},"kimi15")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Better exploration capability"),(0,t.yg)("li",{parentName:"ul"},"Suitable for sparse reward environments")))),(0,t.yg)("h3",{id:"selection-based-on-data-distribution"},"Selection Based on Data Distribution"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Balanced Positive/Negative Samples"),": Use ",(0,t.yg)("inlineCode",{parentName:"li"},"ppo")," or ",(0,t.yg)("inlineCode",{parentName:"li"},"vanilla")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"More Negative Samples"),": Use ",(0,t.yg)("inlineCode",{parentName:"li"},"topr"),", can adjust negative sample weights"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Need Regularization"),": Use ",(0,t.yg)("inlineCode",{parentName:"li"},"kimi15"),", control regularization intensity through tau parameter")),(0,t.yg)("h2",{id:"monitoring-and-debugging"},"Monitoring and Debugging"),(0,t.yg)("h3",{id:"key-metrics"},"Key Metrics"),(0,t.yg)("p",null,"Different algorithms output different monitoring metrics:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Common Metrics"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"pg_loss"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"kl_loss"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"entropy_loss")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"PPO Specific"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"ppo_ratio_clipfrac"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"ppo_ratio_low_clipfrac"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"ppo_ratio_high_clipfrac")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"TIS Specific"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"tis_lower_clipfrac"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"tis_upper_clipfrac"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"tis_total_clipfrac")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"TOPR Specific"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"topr_positive_samples"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"topr_negative_samples"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"topr_negative_total_clipfrac")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"CISPO Specific"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"cispo_total_clipfrac"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"cispo_clipped_ratio")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Kimi15 Specific"),": ",(0,t.yg)("inlineCode",{parentName:"li"},"kimi15_policy_grad_magnitude"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"kimi15_kl_reg_magnitude"))),(0,t.yg)("h3",{id:"debugging-recommendations"},"Debugging Recommendations"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Monitor Clipping Ratios"),": High clipping ratios may indicate learning rate is too large"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Observe Sample Distribution"),": TOPR algorithm focuses on positive/negative sample ratios"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Adjust Hyperparameters"),": Tune algorithm-specific parameters based on task characteristics"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("strong",{parentName:"li"},"Use TensorBoard"),": Visualize metric changes during training")),(0,t.yg)("h2",{id:"frequently-asked-questions"},"Frequently Asked Questions"),(0,t.yg)("h3",{id:"q-how-to-choose-the-appropriate-pg_variant"},"Q: How to choose the appropriate pg_variant?"),(0,t.yg)("p",null,"A: It's recommended to start with ",(0,t.yg)("inlineCode",{parentName:"p"},"topr"),", as it performs well on most tasks. Then adjust based on specific task characteristics."),(0,t.yg)("h3",{id:"q-what-is-the-computational-complexity-of-each-algorithm"},"Q: What is the computational complexity of each algorithm?"),(0,t.yg)("p",null,"A: ",(0,t.yg)("inlineCode",{parentName:"p"},"vanilla")," < ",(0,t.yg)("inlineCode",{parentName:"p"},"ppo")," < ",(0,t.yg)("inlineCode",{parentName:"p"},"tis")," < ",(0,t.yg)("inlineCode",{parentName:"p"},"kimi15")," < ",(0,t.yg)("inlineCode",{parentName:"p"},"cispo")," < ",(0,t.yg)("inlineCode",{parentName:"p"},"topr")),(0,t.yg)("h3",{id:"q-can-i-switch-algorithms-during-training"},"Q: Can I switch algorithms during training?"),(0,t.yg)("p",null,"A: It's not recommended to switch algorithms during training, as this can cause training instability."),(0,t.yg)("h3",{id:"q-how-to-adjust-algorithm-specific-parameters"},"Q: How to adjust algorithm-specific parameters?"),(0,t.yg)("p",null,"A: Refer to the configuration examples for each algorithm and tune based on validation set performance. It's recommended to start with small adjustments."))}c.isMDXComponent=!0},5680:(e,i,a)=>{a.d(i,{xA:()=>s,yg:()=>y});var n=a(6540);function t(e,i,a){return i in e?Object.defineProperty(e,i,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[i]=a,e}function r(e,i){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);i&&(n=n.filter(function(i){return Object.getOwnPropertyDescriptor(e,i).enumerable})),a.push.apply(a,n)}return a}function o(e){for(var i=1;i<arguments.length;i++){var a=null!=arguments[i]?arguments[i]:{};i%2?r(Object(a),!0).forEach(function(i){t(e,i,a[i])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach(function(i){Object.defineProperty(e,i,Object.getOwnPropertyDescriptor(a,i))})}return e}function l(e,i){if(null==e)return{};var a,n,t=function(e,i){if(null==e)return{};var a,n,t={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],i.indexOf(a)>=0||(t[a]=e[a]);return t}(e,i);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],i.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var p=n.createContext({}),g=function(e){var i=n.useContext(p),a=i;return e&&(a="function"==typeof e?e(i):o(o({},i),e)),a},s=function(e){var i=g(e.components);return n.createElement(p.Provider,{value:i},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var i=e.children;return n.createElement(n.Fragment,{},i)}},u=n.forwardRef(function(e,i){var a=e.components,t=e.mdxType,r=e.originalType,p=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),m=g(a),u=t,y=m["".concat(p,".").concat(u)]||m[u]||c[u]||r;return a?n.createElement(y,o(o({ref:i},s),{},{components:a})):n.createElement(y,o({ref:i},s))});function y(e,i){var a=arguments,t=i&&i.mdxType;if("string"==typeof e||t){var r=a.length,o=new Array(r);o[0]=u;var l={};for(var p in i)hasOwnProperty.call(i,p)&&(l[p]=i[p]);l.originalType=e,l[m]="string"==typeof e?e:t,o[1]=l;for(var g=2;g<r;g++)o[g]=a[g];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"}}]);