"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[884],{357:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_pipeline_in_training-24ad92a3612a1c18937a60ddde03f385.png"},4986:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_pipeline_complete-3e33d8f8e3007b5184d9e0ba1badb7df.png"},5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>d});var r=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},u="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef(function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),m=i,d=u["".concat(s,".").concat(m)]||u[m]||g[m]||a;return t?r.createElement(d,o(o({ref:n},p),{},{components:t})):r.createElement(d,o({ref:n},p))});function d(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,o=new Array(a);o[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:i,o[1]=l;for(var c=2;c<a;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}m.displayName="MDXCreateElement"},8851:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>g,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var r=t(8168),i=(t(6540),t(5680));const a={},o="Quickstart: Singel Node Deployment Guide",l={unversionedId:"English/QuickStart/single_node_quick_start_en",id:"English/QuickStart/single_node_quick_start_en",title:"Quickstart: Singel Node Deployment Guide",description:"Environment Preparation",source:"@site/docs/English/QuickStart/single_node_quick_start_en.md",sourceDirName:"English/QuickStart",slug:"/English/QuickStart/single_node_quick_start_en",permalink:"/ROLL/docs/English/QuickStart/single_node_quick_start_en",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/QuickStart/single_node_quick_start_en.md",tags:[],version:"current",lastUpdatedAt:1755156639,formattedLastUpdatedAt:"Aug 14, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Quickstart: Multi Nodes Deployment Guide",permalink:"/ROLL/docs/English/QuickStart/multi_nodes_quick_start_en"},next:{title:"start",permalink:"/ROLL/docs/English/QuickStart/start"}},s={},c=[{value:"Environment Preparation",id:"environment-preparation",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"Pipeline Execution",id:"pipeline-execution",level:2},{value:"Reference: V100 Single-GPU Memory Configuration Optimization",id:"reference-v100-single-gpu-memory-configuration-optimization",level:2}],p={toc:c},u="wrapper";function g({components:e,...n}){return(0,i.yg)(u,(0,r.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"quickstart-singel-node-deployment-guide"},"Quickstart: Singel Node Deployment Guide"),(0,i.yg)("h2",{id:"environment-preparation"},"Environment Preparation"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Purchase a machine equipped with GPU and install GPU drivers simultaneously"),(0,i.yg)("li",{parentName:"ol"},"Connect remotely to the GPU instance and access the machine terminal"),(0,i.yg)("li",{parentName:"ol"},"Install Docker environment and NVIDIA Container Toolkit")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"curl -fsSL https://github.com/alibaba/ROLL/blob/main/scripts/install_docker_nvidia_container_toolkit.sh  | sudo bash   \n")),(0,i.yg)("h2",{id:"environment-configuration"},"Environment Configuration"),(0,i.yg)("p",null,"Choose your desired image from the ",(0,i.yg)("a",{parentName:"p",href:"https://alibaba.github.io/ROLL/docs/English/QuickStart/image_address"},"image addresses"),". The following example will use ",(0,i.yg)("em",{parentName:"p"},"torch2.6.0 + vLLM0.8.4"),"."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"# 1. Start a Docker container with GPU support, expose the port, and keep the container running.\nsudo docker run -dit \\\n  --gpus all \\\n  -p 9001:22 \\\n  --ipc=host \\\n  --shm-size=10gb \\\n  roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch260-vllm084 \\\n  /bin/bash\n\n# 2. Enter the Docker container\n#    You can find your running container's ID or name using `sudo docker ps`.\nsudo docker exec -it <container_id> /bin/bash\n\n# 3. Verify GPU visibility\nnvidia-smi\n\n# 4. Clone the project repo\ngit clone https://github.com/alibaba/ROLL.git\n\n# 5. Install dependencies (select the requirements file corresponding to your chosen image)\ncd ROLL\npip install -r requirements_torch260_vllm.txt -i https://mirrors.aliyun.com/pypi/simple/\n")),(0,i.yg)("h2",{id:"pipeline-execution"},"Pipeline Execution"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"bash examples/agentic_demo/run_agentic_pipeline_frozen_lake_single_node_demo.sh  \n")),(0,i.yg)("p",null,"Example Log Screenshots during Pipeline Execution:\n",(0,i.yg)("img",{alt:"log_pipeline_start",src:t(9243).A,width:"2868",height:"650"})),(0,i.yg)("p",null,(0,i.yg)("img",{alt:"log_pipeline_in_training",src:t(357).A,width:"2876",height:"904"})),(0,i.yg)("p",null,(0,i.yg)("img",{alt:"log_pipeline_complete",src:t(4986).A,width:"1904",height:"206"})),(0,i.yg)("h2",{id:"reference-v100-single-gpu-memory-configuration-optimization"},"Reference: V100 Single-GPU Memory Configuration Optimization"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"# Reduce the system's expected number of GPUs from 8 to your actual 1 V100\nnum_gpus_per_node: 1 \n# Training processes are now mapped only to GPU 0\nactor_train.device_mapping: list(range(0,1))\n# Inference processes are now mapped only to GPU 0\nactor_infer.device_mapping: list(range(0,1))\n# Reference model processes are now mapped only to GPU 0\nreference.device_mapping: list(range(0,1))\n\n# Significantly reduce the batch sizes for Rollout and Validation stages to prevent out-of-memory errors on a single GPU\nrollout_batch_size: 16\nval_batch_size: 16\n\n# V100 has better native support for FP16 than BF16 (unlike A100/H100). Switching to FP16 improves compatibility and stability, while also saving GPU memory.\nactor_train.model_args.dtype: fp16\nactor_infer.model_args.dtype: fp16\nreference.model_args.dtype: fp16\n\n# Switch the large model training framework from DeepSpeed to Megatron-LM. Parameters can be sent in batches, resulting in faster execution.\nstrategy_name: megatron_train\nstrategy_config:\n  tensor_model_parallel_size: 1\n  pipeline_model_parallel_size: 1\n  expert_model_parallel_size: 1\n  use_distributed_optimizer: true\n  recompute_granularity: full\n\n# In megatron training the global train batch size is equivalent to per_device_train_batch_size * gradient_accumulation_steps * world_size\nactor_train.training_args.per_device_train_batch_size: 1\nactor_train.training_args.gradient_accumulation_steps: 16  \n\n# Reduce the maximum number of actions per trajectory, making each Rollout trajectory shorter that reduces the length of LLM-generated content.\nmax_actions_per_traj: 10    \n\n# Reduce the number of parallel training and validation environment groups to accommodate single-GPU resources.\ntrain_env_manager.env_groups: 1\ntrain_env_manager.n_groups: 1\nval_env_manager.env_groups: 2\nval_env_manager.n_groups: [1, 1]\nval_env_manager.tags: [SimpleSokoban, FrozenLake]\n\n# Reduce the total number of training steps for quicker full pipeline runs, useful for rapid debugging.\nmax_steps: 100\n")))}g.isMDXComponent=!0},9243:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_pipeline_start-1b28c489a3d9d8cc9fef5dc2aab7ead7.png"}}]);