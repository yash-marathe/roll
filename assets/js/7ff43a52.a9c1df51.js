"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[8595],{28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var r=t(96540);const i={},o=r.createContext(i);function a(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(o.Provider,{value:n},e.children)}},33889:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Getting Started/Quick Start/multi_nodes_quick_start","title":"Quick Start: Multi-Node Deployment Guide","description":"Environment Preparation","source":"@site/docs/Getting Started/Quick Start/multi_nodes_quick_start.md","sourceDirName":"Getting Started/Quick Start","slug":"/Getting Started/Quick Start/multi_nodes_quick_start","permalink":"/ROLL/docs/Getting Started/Quick Start/multi_nodes_quick_start","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/Getting Started/Quick Start/multi_nodes_quick_start.md","tags":[],"version":"current","lastUpdatedAt":1764225914000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Installation","permalink":"/ROLL/docs/Getting Started/Installation/"},"next":{"title":"Quick Start: Single-Node Deployment Guide","permalink":"/ROLL/docs/Getting Started/Quick Start/single_node_quick_start"}}');var i=t(74848),o=t(28453);const a={},s="Quick Start: Multi-Node Deployment Guide",l={},c=[{value:"Environment Preparation",id:"environment-preparation",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"Pipeline Execution",id:"pipeline-execution",level:2},{value:"Reference: Multi-GPU V100 Memory Configuration Key Points",id:"reference-multi-gpu-v100-memory-configuration-key-points",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"quick-start-multi-node-deployment-guide",children:"Quick Start: Multi-Node Deployment Guide"})}),"\n",(0,i.jsx)(n.h2,{id:"environment-preparation",children:"Environment Preparation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Purchase multiple machines equipped with GPUs and install GPU drivers synchronously, with one machine as the master node and others as worker nodes (the example below uses 2 machines with 2 GPUs each)"}),"\n",(0,i.jsx)(n.li,{children:"Connect to the GPU instances remotely and enter the machine terminal"}),"\n",(0,i.jsx)(n.li,{children:"Run the following command on each machine to install the Docker environment and NVIDIA container toolkit"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"curl -fsSL https://github.com/alibaba/ROLL/blob/main/scripts/install_docker_nvidia_container_toolkit.sh | sudo bash\n"})}),"\n",(0,i.jsx)(n.h2,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["Choose your desired Docker image from the ",(0,i.jsx)(n.a,{href:"https://alibaba.github.io/ROLL/docs/QuickStart/image_address",children:"image addresses"}),". The following example uses ",(0,i.jsx)(n.em,{children:"torch2.6.0 + vLLM0.8.4"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"# 1. Start a Docker container with GPU support, expose container ports, and keep the container running\nsudo docker run -dit \\\n  --gpus all \\\n  -p 9001:22 \\\n  --ipc=host \\\n  --shm-size=10gb \\\n  roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch260-vllm084 \\\n  /bin/bash\n\n# 2. Enter the Docker container\n#    You can use the `sudo docker ps` command to find the running container ID or name.\nsudo docker exec -it <container_id> /bin/bash\n\n# 3. Verify that GPUs are visible\nnvidia-smi\n\n# 4. Clone the project code\ngit clone https://github.com/alibaba/ROLL.git\n\n# 5. Install project dependencies (choose the requirements file corresponding to your image)\ncd ROLL\npip install -r requirements_torch260_vllm.txt -i https://mirrors.aliyun.com/pypi/simple/\n"})}),"\n",(0,i.jsx)(n.h2,{id:"pipeline-execution",children:"Pipeline Execution"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Configure environment variables on the master node:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:'export MASTER_ADDR="ip of master node"\nexport MASTER_PORT="port of master node"  # Default: 6379\nexport WORLD_SIZE=2\nexport RANK=0\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\n'})}),"\n",(0,i.jsx)(n.p,{children:"Notes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"MASTER_ADDR"})," and ",(0,i.jsx)(n.code,{children:"MASTER_PORT"})," define the communication endpoint of the distributed cluster."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"WORLD_SIZE"})," specifies the total number of nodes in the cluster (e.g., 2 nodes)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"RANK"})," identifies the role of the node (0 represents the master node, 1, 2, 3, etc. represent worker nodes)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"NCCL_SOCKET_IFNAME"})," and ",(0,i.jsx)(n.code,{children:"GLOO_SOCKET_IFNAME"})," specify the network interface used for GPU/cluster communication (usually eth0)."]}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Run the pipeline on the master node"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"bash examples/agentic_demo/run_agentic_pipeline_frozen_lake_multi_nodes_demo.sh\n"})}),"\n",(0,i.jsxs)(n.p,{children:["After the Ray cluster starts, you will see log examples like the following:\n",(0,i.jsx)(n.img,{src:"https://img.alicdn.com/imgextra/i3/O1CN01sq2KKz1vBQZViSB0E_!!6000000006134-2-tps-2758-204.png",alt:"log_ray_multi_nodes"})]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Configure environment variables on the worker node"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:'export MASTER_ADDR="ip of master node"\nexport MASTER_PORT="port of master node" # Default: 6379\nexport WORLD_SIZE=2\nexport RANK=1\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\n'})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Connect to the Ray cluster started by the master node on the worker node:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"ray start --address='ip of master node:port of master node' --num-gpus=2\n"})}),"\n",(0,i.jsx)(n.h2,{id:"reference-multi-gpu-v100-memory-configuration-key-points",children:"Reference: Multi-GPU V100 Memory Configuration Key Points"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Reduce the expected number of GPUs from 8 to the 2 V100s you actually have\nnum_gpus_per_node: 2\n# Training processes now map to GPUs 0-3\nactor_train.device_mapping: list(range(0,4))\n# Inference processes now map to GPUs 0-3\nactor_infer.device_mapping: list(range(0,4))\n# Reference model processes now map to GPUs 0-3\nreference.device_mapping: list(range(0,4))\n\n# Significantly reduce the batch size during Rollout/Validation stages to prevent out-of-memory errors when a single GPU processes large batches\nrollout_batch_size: 64\nval_batch_size: 16\n\n# V100 has better native support for FP16 than BF16 (unlike A100/H100). Switching to FP16 can improve compatibility and stability while saving GPU memory.\nactor_train.model_args.dtype: fp16\nactor_infer.model_args.dtype: fp16\nreference.model_args.dtype: fp16\n\n# Switch the large model training framework from DeepSpeed to Megatron-LM, where parameters can be sent in batches for faster execution\nstrategy_name: megatron_train\nstrategy_config:\n  tensor_model_parallel_size: 1\n  pipeline_model_parallel_size: 1\n  expert_model_parallel_size: 1\n  use_distributed_optimizer: true\n  recompute_granularity: full\n\n# In Megatron training, the global training batch size is per_device_train_batch_size * gradient_accumulation_steps * world_size, where world_size = 4\nactor_train.training_args.per_device_train_batch_size: 1\nactor_train.training_args.gradient_accumulation_steps: 16  \n\n# Reduce the maximum number of actions per trajectory to make each Rollout trajectory shorter, reducing the length of LLM-generated content\nmax_actions_per_traj: 10    \n\n# Reduce the number of parallel training environment groups and validation environment groups to accommodate single GPU resources\ntrain_env_manager.env_groups: 1\ntrain_env_manager.n_groups: 1\nval_env_manager.env_groups: 2\nval_env_manager.n_groups: [1, 1]\nval_env_manager.tags: [SimpleSokoban, FrozenLake]\n\n# Reduce the total number of training steps to run a complete training process faster for quick debugging\nmax_steps: 100\n"})})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);