"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[4819],{59029(e,n,i){i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"User Guides/Advanced Features/dynamic_batching","title":"ROLL Dynamic Batching","description":"The ROLL framework supports Dynamic Batching for rollout batches. This feature minimizes invalid token computation and improves overall computational efficiency. This document provides a detailed guide on how to use this feature.","source":"@site/docs/User Guides/Advanced Features/dynamic_batching.md","sourceDirName":"User Guides/Advanced Features","slug":"/User Guides/Advanced Features/dynamic_batching","permalink":"/ROLL/docs/User Guides/Advanced Features/dynamic_batching","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Advanced Features/dynamic_batching.md","tags":[],"version":"current","lastUpdatedAt":1770642330000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Checkpoint Saving and Resuming Guide","permalink":"/ROLL/docs/User Guides/Advanced Features/checkpoint_and_resume"},"next":{"title":"Converting MCoreAdapter Models to Hugging Face Format","permalink":"/ROLL/docs/User Guides/Advanced Features/megatron_convert_2_hf"}}');var a=i(74848),s=i(28453);const r={},c="ROLL Dynamic Batching",o={},l=[{value:"Glossary",id:"glossary",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Example",id:"example",level:2},{value:"Configuration Parameters",id:"configuration-parameters",level:2},{value:"Train",id:"train",level:3},{value:"Infer",id:"infer",level:3},{value:"Full Configuration",id:"full-configuration",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"roll-dynamic-batching",children:"ROLL Dynamic Batching"})}),"\n",(0,a.jsxs)(n.p,{children:["The ROLL framework supports ",(0,a.jsx)(n.strong,{children:"Dynamic Batching"})," for rollout batches. This feature minimizes invalid token computation and improves overall computational efficiency. This document provides a detailed guide on how to use this feature."]}),"\n",(0,a.jsx)(n.h2,{id:"glossary",children:"Glossary"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["attention_mask: data in the rollout batch ,where ",(0,a.jsx)(n.code,{children:"1"})," represents a real token and ",(0,a.jsx)(n.code,{children:"0"})," represents a ",(0,a.jsx)(n.code,{children:"pad_token"})]}),"\n",(0,a.jsx)(n.li,{children:"micro_batch (mbs): The micro-batch during the model forward pass."}),"\n",(0,a.jsx)(n.li,{children:"num_micro_batches: The number of micro_batch in one mini-batch."}),"\n",(0,a.jsx)(n.li,{children:"micro_batch_size: The number of sequences in the micro_batch."}),"\n",(0,a.jsx)(n.li,{children:"micro_batch_seqlen: The sequence length in the micro_batch."}),"\n",(0,a.jsx)(n.li,{children:"dp_size, dp_rank, shard: The size of data parallelism, the specific rank within the data parallel group and the training data in the data parallel group."}),"\n",(0,a.jsx)(n.li,{children:"vpp: Virtual Pipeline Model Parallelism; an efficient pipeline parallel technique supported by the Megatron-LM framework."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:["In Reinforcement Learning (RL) training, the data generated during rollout phase has a ",(0,a.jsx)(n.strong,{children:"long-tail"})," effect, that the sequence lengths vary significantly. This phenomenon is even more pronounced in ",(0,a.jsx)(n.strong,{children:"Agentic Pipelines"}),", where training data is generated through multi-turn interactions with an environment."]}),"\n",(0,a.jsxs)(n.p,{children:["In the train step of RL, all samples in a rollout batch are typically padded to a fixed ",(0,a.jsx)(n.code,{children:"max_len"}),". Consequently, these pad tokens are included in the calculation, leading to a waste of computational resources."]}),"\n",(0,a.jsx)(n.p,{children:"To address this and improve efficiency, the core idea of Dynamic Batching is:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Partition the rollout batch across DP (Data Parallel) Ranks according to actual tokens and ensure a balanced workload."}),"\n",(0,a.jsx)(n.li,{children:"The sequence of samples is rearranged so that samples with similar lengths are grouped together, to remove as many pad tokens as possible."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"example",children:"Example"}),"\n",(0,a.jsx)(n.p,{children:"The following example briefly illustrates the process of Dynamic Batching in ROLL."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Assumptions:"})," ",(0,a.jsx)(n.code,{children:"dp_size=2"}),", ",(0,a.jsx)(n.code,{children:"num_seqs=8"}),", ",(0,a.jsx)(n.code,{children:"max_tokens_microbatch=10"}),", ",(0,a.jsx)(n.code,{children:"sequence_length_round=2"})]}),"\n",(0,a.jsxs)(n.p,{children:["Original input ",(0,a.jsx)(n.code,{children:"attention_mask"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"attention_mask:\n[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"})}),"\n",(0,a.jsxs)(n.p,{children:["The corresponding ",(0,a.jsx)(n.code,{children:"seq_lens"})," are:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"seq_lens:\n[7, 6, 8, 5, 1, 3, 8, 6]\n"})}),"\n",(0,a.jsxs)(n.p,{children:["As shown, the number of actual tokens varies significantly between sequences, causing the waste of GPU resources for processing ",(0,a.jsx)(n.code,{children:"pad_tokens"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["To optimize efficiency, ROLL Dynamic Batching follows these steps to eliminate pad tokens within a ",(0,a.jsx)(n.code,{children:"micro_batch"}),":"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"1. Sort and Shard:"}),"  A shard represents the training data within each dp_rank. By default, the data is sharded in order. In Dynamic Batching, sequences are first sorted by their actual length and then sharded to ensure that the number of tokens is balanced across dp_ranks."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# seq_lens after sorting:\n[1, 3, 5, 6, 6, 7, 8, 8]\n\n# Partition into dp_size shards:\nshard0:\n  [1, 5, 6, 8]\nshard1:\n  [3, 6, 7, 8]\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"2. Micro-batch Partition:"})}),"\n",(0,a.jsx)(n.p,{children:"The partition process consider the following two parameters:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_tokens_per_microbatch"}),": The maximum number of tokens allowed in one micro_batch. ",(0,a.jsx)(n.code,{children:"micro_batch_size * micro_batch_seqlen"})," cannot exceed this value. If it is exceeded, a new micro_batch must be created."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sequence_length_round"}),": The ",(0,a.jsx)(n.code,{children:"micro_batch_seqlen"})," must be a multiple of this value. For example, the sequence lengths in a micro_batch is [200, 240] and ",(0,a.jsx)(n.code,{children:"sequence_length_round"})," is 64, the sequences in this micro-batch must be padded to a length of 256."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The shard partition process for Dynamic Batching aims to find the split that maximizes the number of tokens in a micro-batch, while ensuring the numer of tokens in mirco_batch cannot exceed ",(0,a.jsx)(n.code,{children:"max_tokens_per_microbatch"}),". It also ensures that the sequence length for each micro-batch is padded up to a multiple of ",(0,a.jsx)(n.code,{children:"sequence_length_round"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"The process is detailed as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"shard0:\n  mbs0: # Padding length 6 \n    [1, 0, 0, 0, 0, 0 \n     1, 1, 1, 1, 1, 0]\n  mbs1: # Padding length 8\n    [1, 1, 1, 1, 1, 1, 0, 0]\n  mbs2: # Padding length 8\n    [1, 1, 1, 1, 1, 1, 1, 1]\n\nshard1:\n  mbs0: # Padding length 6\n    [1, 1, 1, 0, 0, 0\n     1, 1, 1, 1, 1, 1]\n  mbs1: # Padding length 8\n    [1, 1, 1, 1, 1, 1, 1, 0]\n  mbs2: # Padding length 8\n    [1, 1, 1, 1, 1, 1, 1, 1]\n"})}),"\n",(0,a.jsxs)(n.p,{children:["In this example, the original total token count was ",(0,a.jsx)(n.code,{children:"80"})," (",(0,a.jsx)(n.code,{children:"8 * 10"}),"). After Dynamic Batching, the total token count is reduced to 56, removing 30% of the ",(0,a.jsx)(n.code,{children:"pad_tokens"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"3. Support Virtual Pipeline Model Parallel :"})," Split micro-batches with more tokens and ",(0,a.jsx)(n.code,{children:"micro_batch_size > 1"}),". This ensures the number of micro-batches is an integer multiple of ",(0,a.jsx)(n.code,{children:"pp_size"})," (compatible with Megatron)."]}),"\n",(0,a.jsxs)(n.p,{children:["Since the ",(0,a.jsx)(n.code,{children:"num_microbatches"})," in the original example is not divisible by pp_size, mbs0 is selected and split into two mbs, as follows:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"shard0:\n  mbs0: # padding length 6 \n    [1, 0, 0, 0, 0, 0]\n  mbs1: # padding length 6 \n    [1, 1, 1, 1, 1, 0]\n  mbs2: # padding length 8\n    [1, 1, 1, 1, 1, 1, 0, 0]\n  mbs3: # padding length 8\n    [1, 1, 1, 1, 1, 1, 1, 1]\nshard1:\n  mbs0: # padding length 6\n    [1, 1, 1, 0, 0, 0]\n  mbs1: # padding length 6\n    [1, 1, 1, 1, 1, 1]\n  mbs2: # padding length 8\n    [1, 1, 1, 1, 1, 1, 1, 0]\n  mbs3: # padding length 8\n    [1, 1, 1, 1, 1, 1, 1, 1]\n\n"})}),"\n",(0,a.jsx)(n.h2,{id:"configuration-parameters",children:"Configuration Parameters"}),"\n",(0,a.jsxs)(n.p,{children:["The Dynamic Batching parameters are divided into ",(0,a.jsx)(n.code,{children:"train"})," and ",(0,a.jsx)(n.code,{children:"infer"}),":"]}),"\n",(0,a.jsx)(n.h3,{id:"train",children:"Train"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"use_dynamic_batching_in_train"}),": Whether to enable this feature during the ",(0,a.jsx)(n.code,{children:"train_step"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_tokens_per_microbatch_in_train"}),": The maximum number of tokens allowed per micro-batch during training."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sequence_length_round_in_train"}),": The sequence length of each micro-batch must be divisible by this value. It should also be divisible by ",(0,a.jsx)(n.code,{children:"tensor_model_parallel_size * context_parallel_size"}),". Common values are 128 or 64."]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"infer",children:"Infer"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"use_dynamic_batching_in_infer"}),": Whether to enable this during phases that do not require gradient update (e.g., ",(0,a.jsx)(n.code,{children:"compute_log_probs"}),")."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_tokens_per_microbatch_in_infer"}),": Same as the train, usually be higher depending on gpu memory."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sequence_length_round_in_infer"}),": Same as train."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"full-configuration",children:"Full Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"actor_train:\n  # Flash Attention is recommended when using both Dynamic Batching and Context Parallel\n  system_envs:\n    NVTE_FLASH_ATTN: '1'\n    NVTE_FUSED_ATTN: '0'\n    NVTE_UNFUSED_ATTN: '0'\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: false\n    dtype: bf16\n    model_type: ~\n  training_args:\n    learning_rate: 1.0e-6\n    weight_decay: 0\n    per_device_train_batch_size: 2\n    gradient_accumulation_steps: 64\n    warmup_steps: 10\n    lr_scheduler_type: cosine\n  data_args:\n    template: qwen2_5\n  strategy_args:\n    strategy_name: megatron_train\n    strategy_config:\n      tensor_model_parallel_size: 1\n      pipeline_model_parallel_size: 1\n      expert_model_parallel_size: 1\n      use_distributed_optimizer: true\n  device_mapping: list(range(0,8))\n  infer_batch_size: 2\n  use_dynamic_batching_in_train: true\n  max_tokens_per_microbatch_in_train: 8192\n  sequence_length_round_in_train: 128\n  use_dynamic_batching_in_infer: true\n  max_tokens_per_microbatch_in_infer: 16384\n  sequence_length_round_in_infer: 128\n\nactor_infer:\n  model_args:\n    disable_gradient_checkpointing: true\n    dtype: bf16\n  generating_args:\n    max_new_tokens: 128 # single-turn response length\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: 1\n  data_args:\n    template: qwen2_5\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      block_size: 16\n      load_format: auto\n  device_mapping: list(range(0,8))\n\nreference:\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: true\n    dtype: bf16\n    model_type: ~\n  data_args:\n    template: qwen2_5\n  strategy_args:\n    strategy_name: megatron_infer\n    strategy_config: ~\n  device_mapping: list(range(0,8))\n  infer_batch_size: 2\n  use_dynamic_batching_in_infer: true\n  max_tokens_per_microbatch_in_infer: 16384\n  sequence_length_round_in_infer: 128\n"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},28453(e,n,i){i.d(n,{R:()=>r,x:()=>c});var t=i(96540);const a={},s=t.createContext(a);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);