"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[3845],{28453:(i,e,n)=>{n.d(e,{R:()=>s,x:()=>t});var r=n(96540);const a={},o=r.createContext(a);function s(i){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof i?i(e):{...e,...i}},[e,i])}function t(i){let e;return e=i.disableParentContext?"function"==typeof i.components?i.components(a):i.components||a:s(i.components),r.createElement(o.Provider,{value:e},i.children)}},47043:(i,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Configuration/offpolicy_setting","title":"Off-Policy Algorithms Configuration Guide","description":"The ROLL framework supports multiple Off-Policy algorithm variants for reinforcement learning training. This document provides detailed configuration methods and usage examples for various algorithms.","source":"@site/docs/User Guides/Configuration/offpolicy_setting.md","sourceDirName":"User Guides/Configuration","slug":"/User Guides/Configuration/offpolicy_setting","permalink":"/ROLL/docs/User Guides/Configuration/offpolicy_setting","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Configuration/offpolicy_setting.md","tags":[],"version":"current","lastUpdatedAt":1764581625000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Megatron Inference and Training Backend Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/megatron"},"next":{"title":"SGLang Inference Backend Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/sglang"}}');var a=n(74848),o=n(28453);const s={},t="Off-Policy Algorithms Configuration Guide",l={},c=[{value:"Supported Algorithm Variants",id:"supported-algorithm-variants",level:2},{value:"Basic Configuration",id:"basic-configuration",level:2},{value:"Core Parameters",id:"core-parameters",level:3},{value:"Worker Configuration",id:"worker-configuration",level:3},{value:"Detailed Algorithm Configuration",id:"detailed-algorithm-configuration",level:2},{value:"1. Vanilla Policy Gradient",id:"1-vanilla-policy-gradient",level:3},{value:"2. PPO (Proximal Policy Optimization)",id:"2-ppo-proximal-policy-optimization",level:3},{value:"3. TIS (Truncated Importance Sampling)",id:"3-tis-truncated-importance-sampling",level:3},{value:"4. TOPR (Tapered off-policy REINFORCE)",id:"4-topr-tapered-off-policy-reinforce",level:3},{value:"5. CISPO (Clipped Importance Sampling Policy Optimization)",id:"5-cispo-clipped-importance-sampling-policy-optimization",level:3},{value:"6. Kimi15",id:"6-kimi15",level:3},{value:"Complete Configuration Example",id:"complete-configuration-example",level:2},{value:"Key Configuration Points",id:"key-configuration-points",level:3},{value:"Usage",id:"usage",level:3},{value:"Algorithm Selection Recommendations",id:"algorithm-selection-recommendations",level:2},{value:"Selection Based on Task Characteristics",id:"selection-based-on-task-characteristics",level:3},{value:"Selection Based on Data Distribution",id:"selection-based-on-data-distribution",level:3},{value:"Monitoring and Debugging",id:"monitoring-and-debugging",level:2},{value:"Key Metrics",id:"key-metrics",level:3},{value:"Debugging Recommendations",id:"debugging-recommendations",level:3},{value:"Frequently Asked Questions",id:"frequently-asked-questions",level:2},{value:"Q: How to choose the appropriate pg_variant?",id:"q-how-to-choose-the-appropriate-pg_variant",level:3},{value:"Q: What is the computational complexity of each algorithm?",id:"q-what-is-the-computational-complexity-of-each-algorithm",level:3},{value:"Q: Can I switch algorithms during training?",id:"q-can-i-switch-algorithms-during-training",level:3},{value:"Q: How to adjust algorithm-specific parameters?",id:"q-how-to-adjust-algorithm-specific-parameters",level:3}];function d(i){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...i.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"off-policy-algorithms-configuration-guide",children:"Off-Policy Algorithms Configuration Guide"})}),"\n",(0,a.jsx)(e.p,{children:"The ROLL framework supports multiple Off-Policy algorithm variants for reinforcement learning training. This document provides detailed configuration methods and usage examples for various algorithms."}),"\n",(0,a.jsx)(e.h2,{id:"supported-algorithm-variants",children:"Supported Algorithm Variants"}),"\n",(0,a.jsx)(e.p,{children:"The ROLL framework currently supports the following Off-Policy algorithms:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"vanilla"})," - Basic Policy Gradient algorithm"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"ppo"})," - Proximal Policy Optimization"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"tis"})," - Truncated Importance Sampling"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"topr"})," - Tapered off-policy REINFORCE"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"cispo"})," - Clipped Importance Sampling Policy Optimization"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"kimi15"})," - Kimi15 algorithm"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"basic-configuration",children:"Basic Configuration"}),"\n",(0,a.jsx)(e.h3,{id:"core-parameters",children:"Core Parameters"}),"\n",(0,a.jsx)(e.p,{children:"Set the basic parameters for Off-Policy algorithms in the configuration file:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:'# Select algorithm variant\npg_variant: topr  # Options: vanilla, tis, topr, cispo, kimi15, ppo\n\n# Training configuration\nmax_steps: 500\nsave_steps: 100\nlogging_steps: 1\neval_steps: 10\n\n# Data configuration\nrollout_batch_size: 128\nprompt_length: 2048\nresponse_length: 8192\nnum_return_sequences_in_group: 8\n\n# Common training parameters\nppo_epochs: 1\nadv_estimator: "reinforce"\nwhiten_advantages: true\n'})}),"\n",(0,a.jsx)(e.h3,{id:"worker-configuration",children:"Worker Configuration"}),"\n",(0,a.jsx)(e.p,{children:"Use the specialized ActorPGWorker to handle Off-Policy algorithms:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"actor_train:\n  worker_cls: roll.pipeline.rlvr.actor_pg_worker.ActorPGWorker\n  pg_variant: topr  # Keep consistent with global configuration\n  model_args:\n    flash_attn: fa2\n    disable_gradient_checkpointing: false\n    dtype: bf16\n  training_args:\n    learning_rate: 1.0e-6\n    weight_decay: 0\n    per_device_train_batch_size: 1\n    gradient_accumulation_steps: 64\n    warmup_steps: 20\n    num_train_epochs: 50\n  strategy_args:\n    strategy_name: megatron_train\n    strategy_config:\n      tensor_model_parallel_size: 1\n      pipeline_model_parallel_size: 1\n      use_distributed_optimizer: true\n      recompute_granularity: full\n  device_mapping: list(range(0,16))\n"})}),"\n",(0,a.jsx)(e.h2,{id:"detailed-algorithm-configuration",children:"Detailed Algorithm Configuration"}),"\n",(0,a.jsx)(e.h3,{id:"1-vanilla-policy-gradient",children:"1. Vanilla Policy Gradient"}),"\n",(0,a.jsx)(e.p,{children:"The most basic policy gradient algorithm, directly using the product of log probability and advantage function as loss."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Configuration Features:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"No additional parameters required"}),"\n",(0,a.jsx)(e.li,{children:"High computational efficiency"}),"\n",(0,a.jsx)(e.li,{children:"Suitable for simple reinforcement learning tasks"}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: vanilla\n\n# No additional configuration parameters needed\n"})}),"\n",(0,a.jsx)(e.h3,{id:"2-ppo-proximal-policy-optimization",children:"2. PPO (Proximal Policy Optimization)"}),"\n",(0,a.jsx)(e.p,{children:"Proximal Policy Optimization algorithm that stabilizes training by clipping importance sampling ratios."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Key Parameters:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: ppo\n\n# PPO specific parameters\npg_clip: 0.2                    # Clipping range\npg_clip_low: 0.2               # Lower bound clipping (optional)\npg_clip_high: 0.2              # Upper bound clipping (optional)\nuse_pg_clip_range: false       # Whether to use asymmetric clipping\ndual_clip_loss: true           # Whether to enable dual clipping\n"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Configuration Example:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: ppo\npg_clip: 0.2\ndual_clip_loss: true\n"})}),"\n",(0,a.jsx)(e.h3,{id:"3-tis-truncated-importance-sampling",children:"3. TIS (Truncated Importance Sampling)"}),"\n",(0,a.jsx)(e.p,{children:"Truncated Importance Sampling algorithm that limits importance sampling ratios to the range [0, 1]."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Key Parameters:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: tis\n\n# TIS specific parameters\ntis_lower_bound: 0.0           # Lower bound\ntis_upper_bound: 1.0           # Upper bound\n"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Configuration Example:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: tis\ntis_lower_bound: 0.0\ntis_upper_bound: 1.0\n"})}),"\n",(0,a.jsx)(e.h3,{id:"4-topr-tapered-off-policy-reinforce",children:"4. TOPR (Tapered off-policy REINFORCE)"}),"\n",(0,a.jsx)(e.p,{children:"Tapered off-policy reinforcement learning algorithm that adopts different update strategies based on positive and negative rewards."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Algorithm Features:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Positive samples: Direct SFT update without importance sampling"}),"\n",(0,a.jsx)(e.li,{children:"Negative samples: TIS update with importance sampling ratio limited to [0, 1]"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Key Parameters:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: topr\n\n# TOPR specific parameters\ntopr_positive_weight: 1.0      # Positive sample weight\ntopr_negative_weight: 1.0      # Negative sample weight\n"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Configuration Example:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: topr\ntopr_positive_weight: 1.0\ntopr_negative_weight: 1.0\n"})}),"\n",(0,a.jsx)(e.h3,{id:"5-cispo-clipped-importance-sampling-policy-optimization",children:"5. CISPO (Clipped Importance Sampling Policy Optimization)"}),"\n",(0,a.jsx)(e.p,{children:"Clipped Importance Sampling Policy Optimization algorithm that uses clipped importance sampling weights and stop-gradient operations."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Key Parameters:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: cispo\n\n# CISPO specific parameters\ncispo_epsilon_low: 0.1         # Lower bound clipping parameter\ncispo_epsilon_high: 0.1        # Upper bound clipping parameter\ncispo_use_unified_mask: false  # Whether to use unified mask\n"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Configuration Example:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: cispo\ncispo_epsilon_low: 0.1\ncispo_epsilon_high: 0.1\ncispo_use_unified_mask: false\n"})}),"\n",(0,a.jsx)(e.h3,{id:"6-kimi15",children:"6. Kimi15"}),"\n",(0,a.jsx)(e.p,{children:"Policy gradient algorithm based on KL regularization, adding KL divergence regularization term to the policy gradient term."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Key Parameters:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: kimi15\n\n# Kimi15 specific parameters\nkimi15_tau: 0.1               # Regularization parameter\n"})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Configuration Example:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: kimi15\nkimi15_tau: 0.1\n"})}),"\n",(0,a.jsx)(e.h2,{id:"complete-configuration-example",children:"Complete Configuration Example"}),"\n",(0,a.jsx)(e.p,{children:"For a complete RLVR Off-Policy configuration example, please refer to:"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Configuration File"}),": ",(0,a.jsx)(e.code,{children:"examples/qwen2.5-7B-rlvr-offpolicy/rlvr_config.yaml"})]}),"\n",(0,a.jsxs)(e.p,{children:["This configuration file contains all necessary parameter settings and supports switching between different algorithm variants by modifying the ",(0,a.jsx)(e.code,{children:"pg_variant"})," parameter:"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"pg_variant: topr  # Options: topr, vanilla, tis, cispo, kimi15, ppo\n"})}),"\n",(0,a.jsx)(e.h3,{id:"key-configuration-points",children:"Key Configuration Points"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Worker Configuration"}),": Use ",(0,a.jsx)(e.code,{children:"ActorPGWorker"})," class to handle Off-Policy algorithms"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Algorithm Selection"}),": Specify algorithm variant through ",(0,a.jsx)(e.code,{children:"pg_variant"})," parameter"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Model Configuration"}),": Support Megatron training and SGLang inference"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reward Configuration"}),": Include mathematical rule reward model configuration"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"usage",children:"Usage"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Copy the configuration file to your working directory"}),"\n",(0,a.jsxs)(e.li,{children:["Modify ",(0,a.jsx)(e.code,{children:"pg_variant"})," and other parameters as needed"]}),"\n",(0,a.jsx)(e.li,{children:"Run the training script:"}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"python examples/start_rlvr_pipeline.py --config-path your_config.yaml\n"})}),"\n",(0,a.jsx)(e.h2,{id:"algorithm-selection-recommendations",children:"Algorithm Selection Recommendations"}),"\n",(0,a.jsx)(e.h3,{id:"selection-based-on-task-characteristics",children:"Selection Based on Task Characteristics"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Simple Tasks"}),": Use ",(0,a.jsx)(e.code,{children:"vanilla"})," or ",(0,a.jsx)(e.code,{children:"ppo"})]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Low computational overhead"}),"\n",(0,a.jsx)(e.li,{children:"Fast convergence"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Complex Reasoning Tasks"}),": Use ",(0,a.jsx)(e.code,{children:"topr"})," or ",(0,a.jsx)(e.code,{children:"cispo"})]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Better stability"}),"\n",(0,a.jsx)(e.li,{children:"Suitable for long sequence generation"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tasks Requiring Exploration"}),": Use ",(0,a.jsx)(e.code,{children:"tis"})," or ",(0,a.jsx)(e.code,{children:"kimi15"})]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Better exploration capability"}),"\n",(0,a.jsx)(e.li,{children:"Suitable for sparse reward environments"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"selection-based-on-data-distribution",children:"Selection Based on Data Distribution"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Balanced Positive/Negative Samples"}),": Use ",(0,a.jsx)(e.code,{children:"ppo"})," or ",(0,a.jsx)(e.code,{children:"vanilla"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"More Negative Samples"}),": Use ",(0,a.jsx)(e.code,{children:"topr"}),", can adjust negative sample weights"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Need Regularization"}),": Use ",(0,a.jsx)(e.code,{children:"kimi15"}),", control regularization intensity through tau parameter"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"monitoring-and-debugging",children:"Monitoring and Debugging"}),"\n",(0,a.jsx)(e.h3,{id:"key-metrics",children:"Key Metrics"}),"\n",(0,a.jsx)(e.p,{children:"Different algorithms output different monitoring metrics:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Common Metrics"}),": ",(0,a.jsx)(e.code,{children:"pg_loss"}),", ",(0,a.jsx)(e.code,{children:"kl_loss"}),", ",(0,a.jsx)(e.code,{children:"entropy_loss"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"PPO Specific"}),": ",(0,a.jsx)(e.code,{children:"ppo_ratio_clipfrac"}),", ",(0,a.jsx)(e.code,{children:"ppo_ratio_low_clipfrac"}),", ",(0,a.jsx)(e.code,{children:"ppo_ratio_high_clipfrac"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"TIS Specific"}),": ",(0,a.jsx)(e.code,{children:"tis_lower_clipfrac"}),", ",(0,a.jsx)(e.code,{children:"tis_upper_clipfrac"}),", ",(0,a.jsx)(e.code,{children:"tis_total_clipfrac"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"TOPR Specific"}),": ",(0,a.jsx)(e.code,{children:"topr_positive_samples"}),", ",(0,a.jsx)(e.code,{children:"topr_negative_samples"}),", ",(0,a.jsx)(e.code,{children:"topr_negative_total_clipfrac"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"CISPO Specific"}),": ",(0,a.jsx)(e.code,{children:"cispo_total_clipfrac"}),", ",(0,a.jsx)(e.code,{children:"cispo_clipped_ratio"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Kimi15 Specific"}),": ",(0,a.jsx)(e.code,{children:"kimi15_policy_grad_magnitude"}),", ",(0,a.jsx)(e.code,{children:"kimi15_kl_reg_magnitude"})]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"debugging-recommendations",children:"Debugging Recommendations"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Monitor Clipping Ratios"}),": High clipping ratios may indicate learning rate is too large"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Observe Sample Distribution"}),": TOPR algorithm focuses on positive/negative sample ratios"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Adjust Hyperparameters"}),": Tune algorithm-specific parameters based on task characteristics"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Use TensorBoard"}),": Visualize metric changes during training"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"frequently-asked-questions",children:"Frequently Asked Questions"}),"\n",(0,a.jsx)(e.h3,{id:"q-how-to-choose-the-appropriate-pg_variant",children:"Q: How to choose the appropriate pg_variant?"}),"\n",(0,a.jsxs)(e.p,{children:["A: It's recommended to start with ",(0,a.jsx)(e.code,{children:"topr"}),", as it performs well on most tasks. Then adjust based on specific task characteristics."]}),"\n",(0,a.jsx)(e.h3,{id:"q-what-is-the-computational-complexity-of-each-algorithm",children:"Q: What is the computational complexity of each algorithm?"}),"\n",(0,a.jsxs)(e.p,{children:["A: ",(0,a.jsx)(e.code,{children:"vanilla"})," < ",(0,a.jsx)(e.code,{children:"ppo"})," < ",(0,a.jsx)(e.code,{children:"tis"})," < ",(0,a.jsx)(e.code,{children:"kimi15"})," < ",(0,a.jsx)(e.code,{children:"cispo"})," < ",(0,a.jsx)(e.code,{children:"topr"})]}),"\n",(0,a.jsx)(e.h3,{id:"q-can-i-switch-algorithms-during-training",children:"Q: Can I switch algorithms during training?"}),"\n",(0,a.jsx)(e.p,{children:"A: It's not recommended to switch algorithms during training, as this can cause training instability."}),"\n",(0,a.jsx)(e.h3,{id:"q-how-to-adjust-algorithm-specific-parameters",children:"Q: How to adjust algorithm-specific parameters?"}),"\n",(0,a.jsx)(e.p,{children:"A: Refer to the configuration examples for each algorithm and tune based on validation set performance. It's recommended to start with small adjustments."})]})}function p(i={}){const{wrapper:e}={...(0,o.R)(),...i.components};return e?(0,a.jsx)(e,{...i,children:(0,a.jsx)(d,{...i})}):d(i)}}}]);