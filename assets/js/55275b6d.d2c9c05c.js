"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[4431],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var l=i(96540);const s={},r=l.createContext(s);function t(e){const n=l.useContext(r);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),l.createElement(r.Provider,{value:n},e.children)}},94715:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>u,frontMatter:()=>t,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"User Guides/Algorithms/LitePPO","title":"Lite PPO","description":"Introduction","source":"@site/docs/User Guides/Algorithms/LitePPO.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/LitePPO","permalink":"/ROLL/docs/User Guides/Algorithms/LitePPO","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/LitePPO.md","tags":[],"version":"current","lastUpdatedAt":1764905933000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Group Sequence Policy Optimization (GSPO)","permalink":"/ROLL/docs/User Guides/Algorithms/GSPO"},"next":{"title":"Proximal Policy Optimization (PPO)","permalink":"/ROLL/docs/User Guides/Algorithms/PPO"}}');var s=i(74848),r=i(28453);const t={},a="Lite PPO",o={},d=[{value:"Introduction",id:"introduction",level:2},{value:"LitePPO Configuration Parameters",id:"liteppo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lite-ppo",children:"Lite PPO"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:'LitePPO is a lightweight proximal policy optimization algorithm designed for efficient training of large language models. LitePPO improves training efficiency and stability through token-level loss computation and "within-group mean + batch standard deviation normalization" only. LitePPO works as follows:'}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Token-level Loss Computation"}),": Computes losses at the token level to improve training granularity and efficiency."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Group-level Reward Normalization"}),': Uses "within-group mean + batch standard deviation normalization" to stabilize the training process.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Redundancy Removal Design"}),": Removes unnecessary components such as overlong filtering, preserving the original PPO objective function."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"liteppo-configuration-parameters",children:"LitePPO Configuration Parameters"}),"\n",(0,s.jsxs)(n.p,{children:["In ROLL, the LitePPO algorithm-specific configuration parameters are as follows (",(0,s.jsx)(n.code,{children:"roll.pipeline.rlvr.rlvr_config.RLVRConfig"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# LitePPO core config\n## normalization\nnorm_mean_type: group\nnorm_std_type: batch\n\n## token-level loss \ntoken_level_loss: true\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 512  # prompt\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "gae"\nnum_return_sequences_in_group: 1\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n\nwhiten_advantages: true\nadvantage_clip: 2.0\nreward_clip: ~\ndual_clip_loss: true\nlambd: 0.95\ngamma: 1\npg_clip: 0.2\nvalue_clip: ~\nkl_penalty: "kl"\ntarget_kl: ~\ninit_kl_coef: 0.2\nkl_horizon: 10000\nadd_token_level_kl: false\n# normalize\nreward_shift: false\nreward_scale: false\n'})}),"\n",(0,s.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"norm_mean_type"}),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"norm_std_type"}),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"token_level_loss"}),": Whether to enable token-level loss computation, default value is true"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,s.jsx)(n.p,{children:"The following parameters are common configuration items for PPO-class algorithms:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rollout_batch_size"}),": Number of prompts per rollout_batch_size, default value is 512"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt_length"}),": Maximum length of prompts, default value is 2048"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"response_length"}),": Maximum length of responses, default value is 4096"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, optional values are "gae", "reinforce", "grpo", default value is "gae"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"num_return_sequences_in_group"}),": Number of responses generated per prompt (group size), default value is 1"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization rounds per batch of samples, default value is 1"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_kl_loss"}),": Whether to use KL divergence loss, default value is true"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"kl_loss_coef"}),": KL-loss coefficient, default value is 0.001"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"loss_agg_mode"}),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values, default value is true"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range, default value is 2.0"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"reward_clip"}),": Reward value clipping range, default value is ~ (means not set)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"dual_clip_loss"}),": Whether to use dual clipping loss, default value is true"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"lambd"}),": Lambda parameter in GAE estimator, used to trade off bias and variance, default value is 0.95"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"gamma"}),": Discount factor, default value is 1"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"pg_clip"}),": PPO clipping range, default value is 0.2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"value_clip"}),": Value function clipping range, default value is ~ (means not set)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"kl_penalty"}),': KL penalty options, optional values are "kl", "abs", "mse", "full", default value is "kl"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"target_kl"}),": Target KL value for adaptive KL control, default value is ~ (means not set)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"init_kl_coef"}),": Initial KL penalty coefficient, default value is 0.2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"kl_horizon"}),": Range for adaptive KL control, default value is 10000"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"add_token_level_kl"}),": Whether to add token-level KL penalty, default value is false"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"reward_shift"}),": Whether to only subtract mean in reward normalization, default value is false"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"reward_scale"}),": Whether to only divide by standard deviation in reward normalization, default value is false"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.p,{children:["[1] Liu, Z.; Liu, J.; He, Y.; Wang, W.; Liu, J.; Pan, L.; Hu, X.; Xiong, S.; Huang, J.; Hu, J.; Huang, S.; Yang, S.; Wang, J.; Su, W.; Zheng, B. Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning. arXiv August 11, 2025. ",(0,s.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2508.08221",children:"https://doi.org/10.48550/arXiv.2508.08221"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);