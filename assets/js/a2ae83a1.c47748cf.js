"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[5053],{28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>d});var r=i(96540);const s={},t=r.createContext(s);function l(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(t.Provider,{value:n},e.children)}},61526:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Algorithms/PPO","title":"Proximal Policy Optimization (PPO)","description":"Introduction","source":"@site/docs/User Guides/Algorithms/PPO.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/PPO","permalink":"/ROLL/docs/User Guides/Algorithms/PPO","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/PPO.md","tags":[],"version":"current","lastUpdatedAt":1764225914000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lite PPO","permalink":"/ROLL/docs/User Guides/Algorithms/LitePPO"},"next":{"title":"RAFT++ (Reward rAnked Fine-Tuning)","permalink":"/ROLL/docs/User Guides/Algorithms/RAFT_Plus_Plus"}}');var s=i(74848),t=i(28453);const l={},d="Proximal Policy Optimization (PPO)",o={},c=[{value:"Introduction",id:"introduction",level:2},{value:"PPO Configuration Parameters",id:"ppo-configuration-parameters",level:2},{value:"PPO Parameter Descriptions",id:"ppo-parameter-descriptions",level:3},{value:"Key Components of PPO",id:"key-components-of-ppo",level:2},{value:"KL Divergence Control",id:"kl-divergence-control",level:2},{value:"Dual-clip PPO",id:"dual-clip-ppo",level:2},{value:"Usage Recommendations",id:"usage-recommendations",level:2},{value:"Reference Example",id:"reference-example",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Proximal Policy Optimization (PPO) is a class of policy gradient methods for reinforcement learning introduced by OpenAI in 2017. PPO strikes a balance between simplicity, stability, and performance, making it one of the most widely used algorithms in modern RL applications, including fine-tuning large-scale language models."}),"\n",(0,s.jsx)(n.p,{children:"Traditional policy gradient methods (such as REINFORCE or Vanilla Policy Gradient) have the following issues:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"High variance and poor sample efficiency"}),"\n",(0,s.jsx)(n.li,{children:"Instability due to large policy updates"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"PPO addresses these issues by using a clipped surrogate objective function that avoids overly large updates without requiring second-order derivatives."}),"\n",(0,s.jsx)(n.h2,{id:"ppo-configuration-parameters",children:"PPO Configuration Parameters"}),"\n",(0,s.jsxs)(n.p,{children:["In ROLL, the configuration parameters for the PPO algorithm are as follows (",(0,s.jsx)(n.code,{children:"roll.pipeline.rlvr.rlvr_config.RLVRConfig"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# ppo related\n\nrollout_batch_size: 512  # prompt\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "gae"\nnum_return_sequences_in_group: 1\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n\nwhiten_advantages: true\nadvantage_clip: 2.0\nreward_clip: ~\ndual_clip_loss: true\nlambd: 0.95\ngamma: 1\npg_clip: 0.2\nvalue_clip: ~\nkl_penalty: "kl"\ntarget_kl: ~\ninit_kl_coef: 0.2\nkl_horizon: 10000\nadd_token_level_kl: false\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n'})}),"\n",(0,s.jsx)(n.h3,{id:"ppo-parameter-descriptions",children:"PPO Parameter Descriptions"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Default Value"}),(0,s.jsx)(n.th,{children:"Options"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"rollout_batch_size"})}),(0,s.jsx)(n.td,{children:"512"}),(0,s.jsx)(n.td,{children:"Positive integer"}),(0,s.jsx)(n.td,{children:"Number of prompts per batch"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"prompt_length"})}),(0,s.jsx)(n.td,{children:"2048"}),(0,s.jsx)(n.td,{children:"Positive integer"}),(0,s.jsx)(n.td,{children:"Maximum length of prompts"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"response_length"})}),(0,s.jsx)(n.td,{children:"4096"}),(0,s.jsx)(n.td,{children:"Positive integer"}),(0,s.jsx)(n.td,{children:"Maximum length of responses"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"adv_estimator"})}),(0,s.jsx)(n.td,{children:'"gae"'}),(0,s.jsx)(n.td,{children:'"gae", "reinforce", "grpo"'}),(0,s.jsx)(n.td,{children:"Advantage estimator type"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"num_return_sequences_in_group"})}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Positive integer"}),(0,s.jsx)(n.td,{children:"Number of responses generated per prompt"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"ppo_epochs"})}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Positive integer"}),(0,s.jsx)(n.td,{children:"Number of optimization rounds per batch of samples"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"use_kl_loss"})}),(0,s.jsx)(n.td,{children:"true"}),(0,s.jsx)(n.td,{children:"true, false"}),(0,s.jsx)(n.td,{children:"Whether to use KL divergence loss"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"kl_loss_coef"})}),(0,s.jsx)(n.td,{children:"0.001"}),(0,s.jsx)(n.td,{children:"Float"}),(0,s.jsx)(n.td,{children:"KL divergence loss coefficient"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"loss_agg_mode"})}),(0,s.jsx)(n.td,{children:'"seq-mean-token-sum"'}),(0,s.jsx)(n.td,{children:'"token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"'}),(0,s.jsx)(n.td,{children:"Loss aggregation mode"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"whiten_advantages"})}),(0,s.jsx)(n.td,{children:"true"}),(0,s.jsx)(n.td,{children:"true, false"}),(0,s.jsx)(n.td,{children:"Whether to whiten advantage values"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"advantage_clip"})}),(0,s.jsx)(n.td,{children:"2.0"}),(0,s.jsx)(n.td,{children:"Float, ~ (means not set)"}),(0,s.jsx)(n.td,{children:"Advantage value clipping range"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"reward_clip"})}),(0,s.jsx)(n.td,{children:"~"}),(0,s.jsx)(n.td,{children:"Float, ~ (means not set)"}),(0,s.jsx)(n.td,{children:"Reward value clipping range"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"dual_clip_loss"})}),(0,s.jsx)(n.td,{children:"true"}),(0,s.jsx)(n.td,{children:"true, false"}),(0,s.jsx)(n.td,{children:"Whether to use dual clipping loss"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"lambd"})}),(0,s.jsx)(n.td,{children:"0.95"}),(0,s.jsx)(n.td,{children:"Float in [0, 1] range"}),(0,s.jsx)(n.td,{children:"Lambda parameter in GAE estimator, used to trade off bias and variance"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"gamma"})}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Float in [0, 1] range"}),(0,s.jsx)(n.td,{children:"Discount factor"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"pg_clip"})}),(0,s.jsx)(n.td,{children:"0.2"}),(0,s.jsx)(n.td,{children:"Float"}),(0,s.jsx)(n.td,{children:"PPO clipping range"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"value_clip"})}),(0,s.jsx)(n.td,{children:"~"}),(0,s.jsx)(n.td,{children:"Float, ~ (means not set)"}),(0,s.jsx)(n.td,{children:"Value function clipping range"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"kl_penalty"})}),(0,s.jsx)(n.td,{children:'"kl"'}),(0,s.jsx)(n.td,{children:'"kl", "abs", "mse", "full"'}),(0,s.jsx)(n.td,{children:"KL penalty options"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"target_kl"})}),(0,s.jsx)(n.td,{children:"~"}),(0,s.jsx)(n.td,{children:"Float, ~ (means not set)"}),(0,s.jsx)(n.td,{children:"Target KL value for adaptive KL control"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"init_kl_coef"})}),(0,s.jsx)(n.td,{children:"0.2"}),(0,s.jsx)(n.td,{children:"Float"}),(0,s.jsx)(n.td,{children:"Initial KL penalty coefficient"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"kl_horizon"})}),(0,s.jsx)(n.td,{children:"10000"}),(0,s.jsx)(n.td,{children:"Positive integer"}),(0,s.jsx)(n.td,{children:"Range for adaptive KL control"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"add_token_level_kl"})}),(0,s.jsx)(n.td,{children:"false"}),(0,s.jsx)(n.td,{children:"true, false"}),(0,s.jsx)(n.td,{children:"Whether to add token-level KL penalty"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"norm_mean_type"})}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:'"batch", "group", "running", None'}),(0,s.jsx)(n.td,{children:"Mean type for reward normalization"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"norm_std_type"})}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:'"batch", "group", "running", None'}),(0,s.jsx)(n.td,{children:"Std type for reward normalization"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"key-components-of-ppo",children:"Key Components of PPO"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Actor-Critic Architecture"}),": PPO requires an actor model (policy) and a critic model (value function). This is different from algorithms like GRPO and RLOO that don't require a critic model."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Generalized Advantage Estimation (GAE)"}),": PPO uses GAE to compute advantage values, which helps reduce variance in policy gradient estimates while maintaining low bias."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Clipped Surrogate Objective Function"}),": The core of PPO is implemented through a clipped surrogate objective function that constrains policy updates."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"kl-divergence-control",children:"KL Divergence Control"}),"\n",(0,s.jsx)(n.p,{children:"PPO provides two mechanisms to prevent the policy from deviating too far from the reference policy:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"KL Loss"})," (GRPO approach, optional):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_kl_loss"}),": Whether to use KL loss in the actor"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"kl_loss_coef"}),": Coefficient for KL loss"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"kl_penalty"}),": KL penalty options"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"KL Penalty in Rewards"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A KL penalty term can be added to the reward function to control policy updates"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"dual-clip-ppo",children:"Dual-clip PPO"}),"\n",(0,s.jsx)(n.p,{children:"Dual-Clip PPO introduces a method that applies a lower bound to the policy ratio when the advantage is less than zero, preventing it from exceeding the specified lower bound when multiplied by a large ratio."}),"\n",(0,s.jsx)(n.h2,{id:"usage-recommendations",children:"Usage Recommendations"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Size"}),": Adjust ",(0,s.jsx)(n.code,{children:"rollout_batch_size"})," and related parameters according to GPU memory"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"KL Control"}),": It is recommended to enable ",(0,s.jsx)(n.code,{children:"use_kl_loss"})," and set an appropriate ",(0,s.jsx)(n.code,{children:"kl_loss_coef"})," value (e.g., 0.001)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clipping Parameters"}),": ",(0,s.jsx)(n.code,{children:"pg_clip"})," is typically set to 0.2 and can be adjusted according to specific tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advantage Estimation"}),": ",(0,s.jsx)(n.code,{children:"whiten_advantages"})," is typically set to true to improve training stability"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loss Aggregation Mode"}),": Different ",(0,s.jsx)(n.code,{children:"loss_agg_mode"})," options can be tried to optimize training effectiveness"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"reference-example",children:"Reference Example"}),"\n",(0,s.jsx)(n.p,{children:"You can refer to the following configuration file to set up PPO training:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"/examples/docs_examples/example_ppo.yaml"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This example shows how to configure and run PPO training."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}}}]);