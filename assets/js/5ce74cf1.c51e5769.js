"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[5671],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var r=i(96540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}},91073:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Configuration/vllm","title":"vLLM Inference Backend Configuration Guide","description":"vLLM is a fast and easy-to-use large language model inference library that efficiently manages attention key-value cache through PagedAttention technology. This document will provide detailed instructions on how to configure and use the vLLM inference backend in the ROLL framework.","source":"@site/docs/User Guides/Configuration/vllm.md","sourceDirName":"User Guides/Configuration","slug":"/User Guides/Configuration/vllm","permalink":"/ROLL/docs/User Guides/Configuration/vllm","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Configuration/vllm.md","tags":[],"version":"current","lastUpdatedAt":1764581625000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"SGLang Inference Backend Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/sglang"},"next":{"title":"Comprehensive Guide: Using the Agentic Part of ROLL","permalink":"/ROLL/docs/User Guides/Pipeline/agent_pipeline_start"}}');var t=i(74848),s=i(28453);const o={},l="vLLM Inference Backend Configuration Guide",a={},c=[{value:"vLLM Introduction",id:"vllm-introduction",level:2},{value:"Configuring vLLM Strategy",id:"configuring-vllm-strategy",level:2},{value:"Configuration Example",id:"configuration-example",level:3},{value:"Configuration Parameter Details",id:"configuration-parameter-details",level:3},{value:"Integration with Other Components",id:"integration-with-other-components",level:2},{value:"Performance Optimization Recommendations",id:"performance-optimization-recommendations",level:2},{value:"Notes",id:"notes",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"vllm-inference-backend-configuration-guide",children:"vLLM Inference Backend Configuration Guide"})}),"\n",(0,t.jsx)(n.p,{children:"vLLM is a fast and easy-to-use large language model inference library that efficiently manages attention key-value cache through PagedAttention technology. This document will provide detailed instructions on how to configure and use the vLLM inference backend in the ROLL framework."}),"\n",(0,t.jsx)(n.h2,{id:"vllm-introduction",children:"vLLM Introduction"}),"\n",(0,t.jsx)(n.p,{children:"vLLM is a high-performance inference engine with the following features:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fast Inference"}),": Efficiently manages attention key-value cache through PagedAttention technology"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Efficient"}),": Reduces memory usage through quantization and optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Easy to Use"}),": Provides simple API interfaces"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalability"}),": Supports distributed inference"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"configuring-vllm-strategy",children:"Configuring vLLM Strategy"}),"\n",(0,t.jsxs)(n.p,{children:["In the ROLL framework, vLLM inference strategy can be configured by setting ",(0,t.jsx)(n.code,{children:"strategy_args"})," in the YAML configuration file."]}),"\n",(0,t.jsx)(n.h3,{id:"configuration-example",children:"Configuration Example"}),"\n",(0,t.jsxs)(n.p,{children:["The following is a typical vLLM configuration example (from ",(0,t.jsx)(n.code,{children:"examples/qwen2.5-7B-rlvr_megatron/rlvr_config.yaml"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"actor_infer:\n  model_args:\n    disable_gradient_checkpointing: true\n    dtype: bf16\n  generating_args:\n    max_new_tokens: ${response_length}\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: ${num_return_sequences_in_group}\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      block_size: 16\n      max_model_len: 8000\n  device_mapping: list(range(0,12))\n  infer_batch_size: 1\n"})}),"\n",(0,t.jsx)(n.h3,{id:"configuration-parameter-details",children:"Configuration Parameter Details"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"strategy_name"}),": Set to ",(0,t.jsx)(n.code,{children:"vllm"})," to use the vLLM inference backend"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"strategy_config"}),": vLLM-specific configuration parameters. For more vLLM optimization configurations, please refer to the ",(0,t.jsx)(n.a,{href:"https://docs.vllm.ai/en/latest/",children:"vLLM official documentation"}),". The strategy_config is passed through directly."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"gpu_memory_utilization"}),": GPU memory utilization ratio for the model executor","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"For example, 0.8 means using 80% of GPU memory"}),"\n",(0,t.jsx)(n.li,{children:"Adjust this value according to model size and hardware configuration"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"block_size"}),": Token block size for contiguous chunks of tokens","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Affects vLLM's internal memory management efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Usually set to 16 or 32"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"max_model_len"}),": Model context length","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"If not specified, it will be automatically derived from the model configuration"}),"\n",(0,t.jsx)(n.li,{children:"Ensure it does not exceed hardware limitations"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"load_format"}),": Format for loading model weights","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:['Since the model will be "updated" at the beginning, this value can be set to ',(0,t.jsx)(n.code,{children:"dummy"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"sleep_level"}),": Sleep level when sleeping the model","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"1 (default): Only destroys KV cache, retains model weights"}),"\n",(0,t.jsx)(n.li,{children:"2: Destroys both model weights and KV cache after generation, thus saving memory"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"device_mapping"}),": Specify the list of GPU device IDs to use"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"infer_batch_size"}),": Batch size during inference"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-other-components",children:"Integration with Other Components"}),"\n",(0,t.jsx)(n.p,{children:"In the configuration example, we can see:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"actor_infer"})," uses vLLM as the inference backend"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"actor_train"})," uses Megatron for training"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"reference"})," uses Megatron for inference"]}),"\n",(0,t.jsxs)(n.li,{children:["Reward models use different inference backends (such as ",(0,t.jsx)(n.code,{children:"hf_infer"}),")"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This design allows different components to choose the most suitable inference engine according to their needs."}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization-recommendations",children:"Performance Optimization Recommendations"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory Management"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Properly set the ",(0,t.jsx)(n.code,{children:"gpu_memory_utilization"})," parameter to balance performance and memory usage"]}),"\n",(0,t.jsx)(n.li,{children:"Monitor GPU memory usage to avoid memory overflow"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Batch Processing Optimization"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Adjust ",(0,t.jsx)(n.code,{children:"infer_batch_size"})," according to model size and hardware capabilities"]}),"\n",(0,t.jsx)(n.li,{children:"Consider the impact of sequence length on batch size"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Context Length"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Properly set ",(0,t.jsx)(n.code,{children:"max_model_len"})," to match task requirements"]}),"\n",(0,t.jsx)(n.li,{children:"Avoid setting excessively large context lengths that could cause memory insufficiency"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"vLLM requires specific versions of dependency libraries, please ensure compatible versions are installed"}),"\n",(0,t.jsx)(n.li,{children:"In resource-constrained environments, carefully balance resource allocation among different components"}),"\n",(0,t.jsx)(n.li,{children:"Integration of vLLM with training frameworks like Megatron may require additional configuration"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By properly configuring the vLLM inference backend, you can fully leverage the performance advantages of the ROLL framework in large-scale language model inference."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);