"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[4222],{5680:(e,n,a)=>{a.d(n,{xA:()=>c,yg:()=>d});var r=a(6540);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function t(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,r)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?t(Object(a),!0).forEach(function(n){i(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):t(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function s(e,n){if(null==e)return{};var a,r,i=function(e,n){if(null==e)return{};var a,r,i={},t=Object.keys(e);for(r=0;r<t.length;r++)a=t[r],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);for(r=0;r<t.length;r++)a=t[r],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=r.createContext({}),g=function(e){var n=r.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},c=function(e){var n=g(e.components);return r.createElement(l.Provider,{value:n},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef(function(e,n){var a=e.components,i=e.mdxType,t=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=g(a),m=i,d=p["".concat(l,".").concat(m)]||p[m]||u[m]||t;return a?r.createElement(d,o(o({ref:n},c),{},{components:a})):r.createElement(d,o({ref:n},c))});function d(e,n){var a=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var t=a.length,o=new Array(t);o[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[p]="string"==typeof e?e:i,o[1]=s;for(var g=2;g<t;g++)o[g]=a[g];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},9564:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>g});var r=a(8168),i=(a(6540),a(5680));const t={},o="ROLL Asynchronous Training User Guide",s={unversionedId:"English/UserGuide/async_training",id:"English/UserGuide/async_training",title:"ROLL Asynchronous Training User Guide",description:"The ROLL framework now supports asynchronous training for both RLVR and Agentic pipelines, significantly improving training efficiency. This document provides detailed instructions on how to use this feature.",source:"@site/docs/English/UserGuide/async_training.md",sourceDirName:"English/UserGuide",slug:"/English/UserGuide/async_training",permalink:"/ROLL/docs/English/UserGuide/async_training",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/async_training.md",tags:[],version:"current",lastUpdatedAt:1761727400,formattedLastUpdatedAt:"Oct 29, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"ROLL x Ascend",permalink:"/ROLL/docs/English/UserGuide/ascend/ascend_usage"},next:{title:"Agentic Asynchronous Training Feature Usage Guide",permalink:"/ROLL/docs/English/UserGuide/async_training_agentic"}},l={},g=[{value:"Asynchronous Training Overview",id:"asynchronous-training-overview",level:2},{value:"Enabling Asynchronous Training",id:"enabling-asynchronous-training",level:2},{value:"Configuration Parameters",id:"configuration-parameters",level:3},{value:"Configuration Examples",id:"configuration-examples",level:3},{value:"Agentic Asynchronous Training Configuration",id:"agentic-asynchronous-training-configuration",level:4},{value:"RLVR Asynchronous Training Configuration",id:"rlvr-asynchronous-training-configuration",level:4},{value:"How Asynchronous Training Works",id:"how-asynchronous-training-works",level:2},{value:"Supported Algorithms",id:"supported-algorithms",level:2},{value:"Agentic Pipeline",id:"agentic-pipeline",level:3},{value:"RLVR Pipeline",id:"rlvr-pipeline",level:3},{value:"Off-Policy Algorithms",id:"off-policy-algorithms",level:3},{value:"Usage Recommendations",id:"usage-recommendations",level:2}],c={toc:g},p="wrapper";function u({components:e,...n}){return(0,i.yg)(p,(0,r.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"roll-asynchronous-training-user-guide"},"ROLL Asynchronous Training User Guide"),(0,i.yg)("p",null,"The ROLL framework now supports asynchronous training for both RLVR and Agentic pipelines, significantly improving training efficiency. This document provides detailed instructions on how to use this feature."),(0,i.yg)("h2",{id:"asynchronous-training-overview"},"Asynchronous Training Overview"),(0,i.yg)("p",null,"In traditional synchronous training, the training and inference processes run serially, meaning that the next batch of inference can only start after the current batch completes and rewards are collected. In asynchronous training, however, training and inference can run in parallel. The inference process can generate multiple batches of data in advance, and the training process can use this pre-generated data for learning."),(0,i.yg)("h2",{id:"enabling-asynchronous-training"},"Enabling Asynchronous Training"),(0,i.yg)("p",null,"To enable asynchronous training, set the ",(0,i.yg)("inlineCode",{parentName:"p"},"async_generation_ratio")," parameter in your configuration file. This parameter has consistent meaning and usage across both RLVR and Agentic pipelines."),(0,i.yg)("h3",{id:"configuration-parameters"},"Configuration Parameters"),(0,i.yg)("p",null,"The ",(0,i.yg)("inlineCode",{parentName:"p"},"async_generation_ratio")," parameter is defined in ",(0,i.yg)("inlineCode",{parentName:"p"},"roll/configs/base_config.py"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'async_generation_ratio: float = field(\n    default=0,\n    metadata={\n        "help": "The ratio of ahead generation requests in pipeline, "\n        "0 means synchronous pipeline. currently only integer is supported."\n    },\n)\n')),(0,i.yg)("h3",{id:"configuration-examples"},"Configuration Examples"),(0,i.yg)("h4",{id:"agentic-asynchronous-training-configuration"},"Agentic Asynchronous Training Configuration"),(0,i.yg)("p",null,"Here is a complete Agentic asynchronous training configuration example (from ",(0,i.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_async.yaml"),"):"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'# Enable asynchronous training\nasync_generation_ratio: 1\n\n# Other related configurations\nrollout_batch_size: 1024\nval_batch_size: 1024\nsequence_length: 8192\n\n# Training parameters\nmax_steps: 1024\nsave_steps: 10000\nlogging_steps: 1\neval_steps: 10\n\n# PPO parameters\nppo_epochs: 1\nadv_estimator: "grpo"\nwhiten_advantages: true\n\n# Model configuration\npretrain: Qwen/Qwen2.5-0.5B-Instruct\nreward_pretrain: Qwen/Qwen2.5-0.5B-Instruct\n\n# Actor configuration\nactor_train:\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: false\n    dtype: bf16\n  training_args:\n    learning_rate: 1.0e-6\n    weight_decay: 0\n    per_device_train_batch_size: 2\n    gradient_accumulation_steps: 128\n    warmup_steps: 10\n  strategy_args:\n    strategy_name: megatron_train\n    strategy_config:\n      tensor_model_parallel_size: 1\n      pipeline_model_parallel_size: 1\n      expert_model_parallel_size: 1\n      use_distributed_optimizer: true\n      recompute_granularity: full\n  device_mapping: list(range(0,4))\n  infer_batch_size: 2\n\nactor_infer:\n  model_args:\n    disable_gradient_checkpointing: true\n    dtype: bf16\n  generating_args:\n    max_new_tokens: 128\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: 1\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      block_size: 16\n      load_format: auto\n  device_mapping: list(range(4,8))\n')),(0,i.yg)("h4",{id:"rlvr-asynchronous-training-configuration"},"RLVR Asynchronous Training Configuration"),(0,i.yg)("p",null,"Here is a complete RLVR asynchronous training configuration example (from ",(0,i.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-7B-rlvr_megatron/rlvr_config_async.yaml"),"):"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'# Enable asynchronous training\nasync_generation_ratio: 1\n\n# Other related configurations\nrollout_batch_size: 64\nprompt_length: 2048\nresponse_length: 8192\n\n# Training parameters\nmax_steps: 1000\nsave_steps: 100\nlogging_steps: 1\n\n# RLVR specific parameters\nis_num_return_sequences_expand: true\nnum_return_sequences_in_group: 8\nppo_epochs: 1\nadv_estimator: "reinforce"\n\n# Model configuration\npretrain: /data/cpfs_0/common/models/Qwen2.5-7B\nreward_pretrain: /data/cpfs_0/common/models/Qwen2.5-7B\n\n# Actor configuration\nactor_train:\n  model_args:\n    dtype: bf16\n  training_args:\n    learning_rate: 1.0e-6\n    weight_decay: 0\n    per_device_train_batch_size: 1\n    gradient_accumulation_steps: 64\n    warmup_steps: 1\n  data_args:\n    template: qwen2_5\n    file_name:\n      - data/math_deepmath_deal.jsonl\n  strategy_args:\n    strategy_name: megatron_train\n    strategy_config:\n      tensor_model_parallel_size: 2\n      pipeline_model_parallel_size: 1\n      sequence_parallel: true\n      use_distributed_optimizer: true\n  device_mapping: list(range(0,16))\n  infer_batch_size: 2\n\nactor_infer:\n  model_args:\n    dtype: fp16\n  generating_args:\n    max_new_tokens: ${response_length}\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: ${num_return_sequences_in_group}\n  strategy_args:\n    strategy_name: sglang\n    strategy_config:\n      mem_fraction_static: 0.85\n      load_format: dummy\n  device_mapping: list(range(16,24))\n  infer_batch_size: 1\n')),(0,i.yg)("h2",{id:"how-asynchronous-training-works"},"How Asynchronous Training Works"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"When ",(0,i.yg)("inlineCode",{parentName:"li"},"async_generation_ratio")," is set to a value greater than 0, the framework starts asynchronous training mode"),(0,i.yg)("li",{parentName:"ol"},"The inference process generates ",(0,i.yg)("inlineCode",{parentName:"li"},"async_generation_ratio")," times more data than needed for training in advance"),(0,i.yg)("li",{parentName:"ol"},"The training process can use this pre-generated data for learning without waiting for the current batch of inference to complete"),(0,i.yg)("li",{parentName:"ol"},"This parallel processing can significantly improve training efficiency, especially when inference is time-consuming")),(0,i.yg)("h2",{id:"supported-algorithms"},"Supported Algorithms"),(0,i.yg)("h3",{id:"agentic-pipeline"},"Agentic Pipeline"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Supports GRPO and other policy gradient algorithms"),(0,i.yg)("li",{parentName:"ul"},"Suitable for environment interaction tasks, such as games, dialogues, etc."),(0,i.yg)("li",{parentName:"ul"},"Configuration example: ",(0,i.yg)("inlineCode",{parentName:"li"},"examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_async.yaml"))),(0,i.yg)("h3",{id:"rlvr-pipeline"},"RLVR Pipeline"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Supports Reinforce and other algorithms"),(0,i.yg)("li",{parentName:"ul"},"Suitable for language modeling tasks, such as mathematical reasoning, code generation, etc."),(0,i.yg)("li",{parentName:"ul"},"Configuration example: ",(0,i.yg)("inlineCode",{parentName:"li"},"examples/qwen2.5-7B-rlvr_megatron/rlvr_config_async.yaml"))),(0,i.yg)("h3",{id:"off-policy-algorithms"},"Off-Policy Algorithms"),(0,i.yg)("p",null,"ROLL also supports various Off-Policy algorithms. For detailed information, please refer to: ",(0,i.yg)("inlineCode",{parentName:"p"},"docs_roll/docs/English/UserGuide/algorithms/offpolicy_setting.md")),(0,i.yg)("p",null,"Configuration example: ",(0,i.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-7B-rlvr-offpolicy/rlvr_config.yaml")),(0,i.yg)("p",null,"Supported algorithm variants include:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"topr")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"vanilla")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"tis")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"cispo")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"kimi15")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"ppo"))),(0,i.yg)("h2",{id:"usage-recommendations"},"Usage Recommendations"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Adjust the value of ",(0,i.yg)("inlineCode",{parentName:"li"},"async_generation_ratio")," according to hardware resources and task characteristics"),(0,i.yg)("li",{parentName:"ol"},"Ensure separate deployment of training and inference roles"),(0,i.yg)("li",{parentName:"ol"},"Monitor resource usage during training to avoid resource bottlenecks"),(0,i.yg)("li",{parentName:"ol"},"Asynchronous generation is paused during validation and resumes after validation is complete"),(0,i.yg)("li",{parentName:"ol"},"For RLVR tasks, you can further optimize performance by combining ",(0,i.yg)("inlineCode",{parentName:"li"},"is_num_return_sequences_expand")," and ",(0,i.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group")," parameters"),(0,i.yg)("li",{parentName:"ol"},"For Off-Policy algorithms, ensure correct configuration of the ",(0,i.yg)("inlineCode",{parentName:"li"},"pg_variant")," parameter and corresponding worker class")))}u.isMDXComponent=!0}}]);