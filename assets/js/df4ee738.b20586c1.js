"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[2405],{28453:(e,o,n)=>{n.d(o,{R:()=>l,x:()=>a});var t=n(96540);const i={},r=t.createContext(i);function l(e){const o=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function a(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(r.Provider,{value:o},e.children)}},33617:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>s,contentTitle:()=>a,default:()=>m,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"User Guides/Advanced Features/offload_reload_control","title":"GPU Time-Division Multiplexing Control Guide","description":"The ROLL framework implements GPU time-division multiplexing functionality, which allows flexible sharing of GPU resources between different roles through offload/reload capabilities. This document will provide detailed instructions on how to use this feature.","source":"@site/docs/User Guides/Advanced Features/offload_reload_control.md","sourceDirName":"User Guides/Advanced Features","slug":"/User Guides/Advanced Features/offload_reload_control","permalink":"/ROLL/docs/User Guides/Advanced Features/offload_reload_control","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Advanced Features/offload_reload_control.md","tags":[],"version":"current","lastUpdatedAt":1764905933000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Converting MCoreAdapter Models to Hugging Face Format","permalink":"/ROLL/docs/User Guides/Advanced Features/megatron_convert_2_hf"},"next":{"title":"Trackers and Metrics","permalink":"/ROLL/docs/User Guides/Tracker & Metrics/trackers_and_metrics"}}');var i=n(74848),r=n(28453);const l={},a="GPU Time-Division Multiplexing Control Guide",s={},d=[{value:"Time-Division Multiplexing Overview",id:"time-division-multiplexing-overview",level:2},{value:"Offload/Reload Control Mechanism",id:"offloadreload-control-mechanism",level:2},{value:"Automatic Control",id:"automatic-control",level:3},{value:"Manual Control",id:"manual-control",level:3},{value:"Usage Example",id:"usage-example",level:2},{value:"Context Manager Support",id:"context-manager-support",level:2},{value:"Memory Monitoring",id:"memory-monitoring",level:2},{value:"Usage Recommendations",id:"usage-recommendations",level:2}];function c(e){const o={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.header,{children:(0,i.jsx)(o.h1,{id:"gpu-time-division-multiplexing-control-guide",children:"GPU Time-Division Multiplexing Control Guide"})}),"\n",(0,i.jsx)(o.p,{children:"The ROLL framework implements GPU time-division multiplexing functionality, which allows flexible sharing of GPU resources between different roles through offload/reload capabilities. This document will provide detailed instructions on how to use this feature."}),"\n",(0,i.jsx)(o.h2,{id:"time-division-multiplexing-overview",children:"Time-Division Multiplexing Overview"}),"\n",(0,i.jsx)(o.p,{children:"In the ROLL framework, different roles (such as actor_train, actor_infer, critic, reference, and rewards) may need to use the same GPU resources. To improve resource utilization, the framework implements GPU time-division multiplexing functionality, which allows model states to be switched between GPU and CPU at different time points."}),"\n",(0,i.jsx)(o.h2,{id:"offloadreload-control-mechanism",children:"Offload/Reload Control Mechanism"}),"\n",(0,i.jsx)(o.h3,{id:"automatic-control",children:"Automatic Control"}),"\n",(0,i.jsx)(o.p,{children:"Taking RLVRPipeline as an example, the framework automatically manages the offload and reload of model states:"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:"# Example in rlvr_pipeline.py\nref_log_probs = self.reference.compute_log_probs(batch, blocking=True)\n"})}),"\n",(0,i.jsx)(o.p,{children:"By default, when executing RPC calls to a worker, the framework will first reload the GPU-related state of the current worker onto the GPU, and after execution is completed, it will offload the state to memory."}),"\n",(0,i.jsx)(o.h3,{id:"manual-control",children:"Manual Control"}),"\n",(0,i.jsxs)(o.p,{children:["You can also manually intervene in model state management by setting ",(0,i.jsx)(o.code,{children:'batch.meta_info["is_offload_states"]'}),":"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:"# Example in rlvr_pipeline.py\nself.actor_train.offload_states(blocking=True)\n"})}),"\n",(0,i.jsxs)(o.p,{children:["When ",(0,i.jsx)(o.code,{children:"is_offload_states"})," is set to ",(0,i.jsx)(o.code,{children:"False"}),", the model state will not be automatically offloaded to CPU after the RPC call is completed, and the model will continue to remain on the GPU."]}),"\n",(0,i.jsxs)(o.p,{children:["You can also directly use ",(0,i.jsx)(o.code,{children:"worker.offload_states()"})," and ",(0,i.jsx)(o.code,{children:"worker.reload_states()"})," for more direct control over offload and reload timing."]}),"\n",(0,i.jsx)(o.h2,{id:"usage-example",children:"Usage Example"}),"\n",(0,i.jsxs)(o.p,{children:["The following is an example of using offload/reload control in ",(0,i.jsx)(o.code,{children:"rlvr_pipeline.py"}),":"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:'# After the inference phase, manually offload reward model states\nif not self.pipeline_config.async_pipeline:\n    for reward_cluster in self.rewards.values():\n        reward_cluster.offload_states()\n\n# When computing reference model log probs, control whether to offload states\nif self.is_lora:\n    batch.meta_info["disable_adapter"] = True\n    batch.meta_info["is_offload_states"] = False\n    ref_log_probs = self.actor_train.compute_log_probs(batch, blocking=True)\nelse:\n    ref_log_probs = self.reference.compute_log_probs(batch, blocking=True)\n'})}),"\n",(0,i.jsx)(o.h2,{id:"context-manager-support",children:"Context Manager Support"}),"\n",(0,i.jsxs)(o.p,{children:["The ROLL framework also provides the ",(0,i.jsx)(o.code,{children:"state_offload_manager"})," context manager to simplify state management:"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:"from roll.utils.context_managers import state_offload_manager\n\nwith state_offload_manager(strategy, metrics, metric_infix, is_offload_states=True):\n    # Execute operations that require GPU state within this context\n    yield\n"})}),"\n",(0,i.jsx)(o.p,{children:"This context manager automatically handles:"}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsx)(o.li,{children:"Loading model states to GPU"}),"\n",(0,i.jsx)(o.li,{children:"Executing operations"}),"\n",(0,i.jsxs)(o.li,{children:["Deciding whether to offload states to CPU based on the ",(0,i.jsx)(o.code,{children:"is_offload_states"})," parameter"]}),"\n"]}),"\n",(0,i.jsx)(o.h2,{id:"memory-monitoring",children:"Memory Monitoring"}),"\n",(0,i.jsx)(o.p,{children:"The framework also provides memory usage monitoring functionality:"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-python",children:'from roll.utils.context_managers import log_gpu_memory_usage\n\n# Record GPU memory usage\nlog_gpu_memory_usage(head="model_loading", logger=logger, rank=None)\n'})}),"\n",(0,i.jsx)(o.h2,{id:"usage-recommendations",children:"Usage Recommendations"}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsx)(o.li,{children:"In resource-constrained situations, properly using the offload/reload feature can significantly improve GPU utilization"}),"\n",(0,i.jsx)(o.li,{children:"In pipeline implementation, arrange the execution order of different roles to maximize resource utilization efficiency, such as parallel computation of ref/reward models"}),"\n",(0,i.jsx)(o.li,{children:"In asynchronous training, properly arrange the execution order of different roles to maximize resource utilization efficiency"}),"\n"]})]})}function m(e={}){const{wrapper:o}={...(0,r.R)(),...e.components};return o?(0,i.jsx)(o,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);