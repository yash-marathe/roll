"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[1600],{28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var r=i(96540);const a={},t=r.createContext(a);function s(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(t.Provider,{value:n},e.children)}},30161:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Agentic/agentic_StarPO","title":"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)","description":"Introduction","source":"@site/docs/User Guides/Agentic/agentic_StarPO.md","sourceDirName":"User Guides/Agentic","slug":"/User Guides/Agentic/agentic_StarPO","permalink":"/ROLL/docs/User Guides/Agentic/agentic_StarPO","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Agentic/agentic_StarPO.md","tags":[],"version":"current","lastUpdatedAt":1764225914000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)","permalink":"/ROLL/docs/User Guides/Agentic/agentic_GiGPO"},"next":{"title":"Agentic Engineering Practice Documentation","permalink":"/ROLL/docs/User Guides/Agentic/agentic_engineer_practice"}}');var a=i(74848),t=i(28453);const s={},o="TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"StarPO Configuration Parameters",id:"starpo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Environment Manager Parameters",id:"environment-manager-parameters",level:3},{value:"Reference Examples",id:"reference-examples",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"trajwiselearningstarpo-state-thinking-actions-reward-policy-optimization",children:"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"StarPO (State-Thinking-Actions-Reward Policy Optimization) is a reinforcement learning algorithm for LLM agent training. It optimizes by treating the entire multi-turn interaction trajectory (including observations, reasoning traces, actions, and feedback) as a coherent unit, rather than independently processing each action as in traditional methods."}),"\n",(0,a.jsx)(n.p,{children:"The core idea of StarPO is trajectory-level optimization, which alternates between two phases:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rollout Phase"}),": Generate reasoning-interaction trajectories"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Phase"}),": Optimize the model based on complete trajectories"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"starpo-configuration-parameters",children:"StarPO Configuration Parameters"}),"\n",(0,a.jsxs)(n.p,{children:["In ROLL, the core implementation of StarPO is located at ",(0,a.jsx)(n.code,{children:"roll/pipeline/agentic/utils.py"}),". The specific configuration parameters for the StarPO algorithm are as follows (",(0,a.jsx)(n.code,{children:"roll.pipeline.agentic.agentic_config.AgenticConfig"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# StarPO core config\n# StarPO related\nadv_estimator: "reinforce"\n\n# rollout_batch_size is the number of trajectories\nrollout_batch_size: 1024\nval_batch_size: 1024\nsequence_length: 1024\n\nadvantage_clip: 0.2\nppo_epochs: 1\n\n# pg_clip: 0.1\n#dual_clip_loss: True\ninit_kl_coef: 0.0\nwhiten_advantages: true\nentropy_loss_coef: 0\nmax_grad_norm: 1.0\n\nreward_normalization:\n  grouping: traj_group_id # Can be tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by calculates reward/adv\n  method: mean # asym_clip / identity / mean_std / mean\n\ntrain_env_manager:\n  max_env_num_per_worker: 16\n  num_env_groups: 128\n  # under the same group, the env config and env seed are ensured to be equal\n  group_size: 8 # grpo\'s grpo\n  tags: [FrozenLake]\n  num_groups_partition: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation\n\nenv_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager\n'})}),"\n",(0,a.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, set to "reinforce", which is the core configuration of the StarPO algorithm']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"env_manager_cls"}),": Environment manager class, StarPO needs to use ",(0,a.jsx)(n.code,{children:"roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,a.jsx)(n.p,{children:"The following parameters are common configuration items for PPO-class algorithms:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"rollout_batch_size"}),": Number of trajectories per rollout batch, default value is 1024"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"val_batch_size"}),": Validation batch size, default value is 1024"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sequence_length"}),": Maximum sequence length, default value is 1024"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range, default value is 0.2"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization epochs per batch of samples, default value is 1"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"init_kl_coef"}),": Initial coefficient for KL penalty, default value is 0.0"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values, default value is true"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"entropy_loss_coef"}),": Entropy loss coefficient, default value is 0"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_grad_norm"}),": Maximum norm for gradient clipping, default value is 1.0"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"environment-manager-parameters",children:"Environment Manager Parameters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.max_env_num_per_worker"}),": Maximum number of environments per worker, default value is 16"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.num_env_groups"}),": Number of training environment groups, default value is 128"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.group_size"}),": Number of environments per group, default value is 8"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.tags"}),": List of environment tags, default value is [FrozenLake]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.num_groups_partition"}),": Group allocation for each environment type, default value is [128]"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"reference-examples",children:"Reference Examples"}),"\n",(0,a.jsx)(n.p,{children:"You can refer to the following configuration files to set up StarPO training:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"./examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsx)(n.p,{children:"[1] Liu, T.; Feng, L.; An, B. StarPO: State-Regularized Policy Optimization for LLM Agent Training. arXiv 2025, 2504.20073."})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);