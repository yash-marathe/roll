"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[6702],{211:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>g,contentTitle:()=>i,default:()=>y,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var a=n(8168),l=(n(6540),n(5680));const r={},i="Proximal Policy Optimization (PPO)",o={unversionedId:"English/UserGuide/algorithms/PPO",id:"English/UserGuide/algorithms/PPO",title:"Proximal Policy Optimization (PPO)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/PPO.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/PPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/PPO",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/PPO.md",tags:[],version:"current",lastUpdatedAt:1761727400,formattedLastUpdatedAt:"Oct 29, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lite PPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/LitePPO"},next:{title:"RAFT++ (Reward rAnked Fine-Tuning)",permalink:"/ROLL/docs/English/UserGuide/algorithms/RAFT_Plus_Plus"}},g={},p=[{value:"Introduction",id:"introduction",level:2},{value:"PPO Configuration Parameters",id:"ppo-configuration-parameters",level:2},{value:"PPO Parameter Descriptions",id:"ppo-parameter-descriptions",level:3},{value:"Key Components of PPO",id:"key-components-of-ppo",level:2},{value:"KL Divergence Control",id:"kl-divergence-control",level:2},{value:"Dual-clip PPO",id:"dual-clip-ppo",level:2},{value:"Usage Recommendations",id:"usage-recommendations",level:2},{value:"Reference Example",id:"reference-example",level:2}],m={toc:p},d="wrapper";function y({components:e,...t}){return(0,l.yg)(d,(0,a.A)({},m,t,{components:e,mdxType:"MDXLayout"}),(0,l.yg)("h1",{id:"proximal-policy-optimization-ppo"},"Proximal Policy Optimization (PPO)"),(0,l.yg)("h2",{id:"introduction"},"Introduction"),(0,l.yg)("p",null,"Proximal Policy Optimization (PPO) is a class of policy gradient methods for reinforcement learning introduced by OpenAI in 2017. PPO strikes a balance between simplicity, stability, and performance, making it one of the most widely used algorithms in modern RL applications, including fine-tuning large-scale language models."),(0,l.yg)("p",null,"Traditional policy gradient methods (such as REINFORCE or Vanilla Policy Gradient) have the following issues:"),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},"High variance and poor sample efficiency"),(0,l.yg)("li",{parentName:"ol"},"Instability due to large policy updates")),(0,l.yg)("p",null,"PPO addresses these issues by using a clipped surrogate objective function that avoids overly large updates without requiring second-order derivatives."),(0,l.yg)("h2",{id:"ppo-configuration-parameters"},"PPO Configuration Parameters"),(0,l.yg)("p",null,"In ROLL, the configuration parameters for the PPO algorithm are as follows (",(0,l.yg)("inlineCode",{parentName:"p"},"roll.pipeline.rlvr.rlvr_config.RLVRConfig"),"):"),(0,l.yg)("pre",null,(0,l.yg)("code",{parentName:"pre",className:"language-yaml"},'# ppo related\n\nrollout_batch_size: 512  # prompt\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "gae"\nnum_return_sequences_in_group: 1\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n\nwhiten_advantages: true\nadvantage_clip: 2.0\nreward_clip: ~\ndual_clip_loss: true\nlambd: 0.95\ngamma: 1\npg_clip: 0.2\nvalue_clip: ~\nkl_penalty: "kl"\ntarget_kl: ~\ninit_kl_coef: 0.2\nkl_horizon: 10000\nadd_token_level_kl: false\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n')),(0,l.yg)("h3",{id:"ppo-parameter-descriptions"},"PPO Parameter Descriptions"),(0,l.yg)("table",null,(0,l.yg)("thead",{parentName:"table"},(0,l.yg)("tr",{parentName:"thead"},(0,l.yg)("th",{parentName:"tr",align:null},"Parameter"),(0,l.yg)("th",{parentName:"tr",align:null},"Default Value"),(0,l.yg)("th",{parentName:"tr",align:null},"Options"),(0,l.yg)("th",{parentName:"tr",align:null},"Description"))),(0,l.yg)("tbody",{parentName:"table"},(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"rollout_batch_size")),(0,l.yg)("td",{parentName:"tr",align:null},"512"),(0,l.yg)("td",{parentName:"tr",align:null},"Positive integer"),(0,l.yg)("td",{parentName:"tr",align:null},"Number of prompts per batch")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"prompt_length")),(0,l.yg)("td",{parentName:"tr",align:null},"2048"),(0,l.yg)("td",{parentName:"tr",align:null},"Positive integer"),(0,l.yg)("td",{parentName:"tr",align:null},"Maximum length of prompts")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"response_length")),(0,l.yg)("td",{parentName:"tr",align:null},"4096"),(0,l.yg)("td",{parentName:"tr",align:null},"Positive integer"),(0,l.yg)("td",{parentName:"tr",align:null},"Maximum length of responses")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"adv_estimator")),(0,l.yg)("td",{parentName:"tr",align:null},'"gae"'),(0,l.yg)("td",{parentName:"tr",align:null},'"gae", "reinforce", "grpo"'),(0,l.yg)("td",{parentName:"tr",align:null},"Advantage estimator type")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"num_return_sequences_in_group")),(0,l.yg)("td",{parentName:"tr",align:null},"1"),(0,l.yg)("td",{parentName:"tr",align:null},"Positive integer"),(0,l.yg)("td",{parentName:"tr",align:null},"Number of responses generated per prompt")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"ppo_epochs")),(0,l.yg)("td",{parentName:"tr",align:null},"1"),(0,l.yg)("td",{parentName:"tr",align:null},"Positive integer"),(0,l.yg)("td",{parentName:"tr",align:null},"Number of optimization rounds per batch of samples")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"use_kl_loss")),(0,l.yg)("td",{parentName:"tr",align:null},"true"),(0,l.yg)("td",{parentName:"tr",align:null},"true, false"),(0,l.yg)("td",{parentName:"tr",align:null},"Whether to use KL divergence loss")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"kl_loss_coef")),(0,l.yg)("td",{parentName:"tr",align:null},"0.001"),(0,l.yg)("td",{parentName:"tr",align:null},"Float"),(0,l.yg)("td",{parentName:"tr",align:null},"KL divergence loss coefficient")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"loss_agg_mode")),(0,l.yg)("td",{parentName:"tr",align:null},'"seq-mean-token-sum"'),(0,l.yg)("td",{parentName:"tr",align:null},'"token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"'),(0,l.yg)("td",{parentName:"tr",align:null},"Loss aggregation mode")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"whiten_advantages")),(0,l.yg)("td",{parentName:"tr",align:null},"true"),(0,l.yg)("td",{parentName:"tr",align:null},"true, false"),(0,l.yg)("td",{parentName:"tr",align:null},"Whether to whiten advantage values")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"advantage_clip")),(0,l.yg)("td",{parentName:"tr",align:null},"2.0"),(0,l.yg)("td",{parentName:"tr",align:null},"Float, ~ (means not set)"),(0,l.yg)("td",{parentName:"tr",align:null},"Advantage value clipping range")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"reward_clip")),(0,l.yg)("td",{parentName:"tr",align:null},"~"),(0,l.yg)("td",{parentName:"tr",align:null},"Float, ~ (means not set)"),(0,l.yg)("td",{parentName:"tr",align:null},"Reward value clipping range")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"dual_clip_loss")),(0,l.yg)("td",{parentName:"tr",align:null},"true"),(0,l.yg)("td",{parentName:"tr",align:null},"true, false"),(0,l.yg)("td",{parentName:"tr",align:null},"Whether to use dual clipping loss")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"lambd")),(0,l.yg)("td",{parentName:"tr",align:null},"0.95"),(0,l.yg)("td",{parentName:"tr",align:null},"Float in ","[0, 1]"," range"),(0,l.yg)("td",{parentName:"tr",align:null},"Lambda parameter in GAE estimator, used to trade off bias and variance")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"gamma")),(0,l.yg)("td",{parentName:"tr",align:null},"1"),(0,l.yg)("td",{parentName:"tr",align:null},"Float in ","[0, 1]"," range"),(0,l.yg)("td",{parentName:"tr",align:null},"Discount factor")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"pg_clip")),(0,l.yg)("td",{parentName:"tr",align:null},"0.2"),(0,l.yg)("td",{parentName:"tr",align:null},"Float"),(0,l.yg)("td",{parentName:"tr",align:null},"PPO clipping range")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"value_clip")),(0,l.yg)("td",{parentName:"tr",align:null},"~"),(0,l.yg)("td",{parentName:"tr",align:null},"Float, ~ (means not set)"),(0,l.yg)("td",{parentName:"tr",align:null},"Value function clipping range")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"kl_penalty")),(0,l.yg)("td",{parentName:"tr",align:null},'"kl"'),(0,l.yg)("td",{parentName:"tr",align:null},'"kl", "abs", "mse", "full"'),(0,l.yg)("td",{parentName:"tr",align:null},"KL penalty options")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"target_kl")),(0,l.yg)("td",{parentName:"tr",align:null},"~"),(0,l.yg)("td",{parentName:"tr",align:null},"Float, ~ (means not set)"),(0,l.yg)("td",{parentName:"tr",align:null},"Target KL value for adaptive KL control")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"init_kl_coef")),(0,l.yg)("td",{parentName:"tr",align:null},"0.2"),(0,l.yg)("td",{parentName:"tr",align:null},"Float"),(0,l.yg)("td",{parentName:"tr",align:null},"Initial KL penalty coefficient")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"kl_horizon")),(0,l.yg)("td",{parentName:"tr",align:null},"10000"),(0,l.yg)("td",{parentName:"tr",align:null},"Positive integer"),(0,l.yg)("td",{parentName:"tr",align:null},"Range for adaptive KL control")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"add_token_level_kl")),(0,l.yg)("td",{parentName:"tr",align:null},"false"),(0,l.yg)("td",{parentName:"tr",align:null},"true, false"),(0,l.yg)("td",{parentName:"tr",align:null},"Whether to add token-level KL penalty")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"norm_mean_type")),(0,l.yg)("td",{parentName:"tr",align:null},"None"),(0,l.yg)("td",{parentName:"tr",align:null},'"batch", "group", "running", None'),(0,l.yg)("td",{parentName:"tr",align:null},"Mean type for reward normalization")),(0,l.yg)("tr",{parentName:"tbody"},(0,l.yg)("td",{parentName:"tr",align:null},(0,l.yg)("inlineCode",{parentName:"td"},"norm_std_type")),(0,l.yg)("td",{parentName:"tr",align:null},"None"),(0,l.yg)("td",{parentName:"tr",align:null},'"batch", "group", "running", None'),(0,l.yg)("td",{parentName:"tr",align:null},"Std type for reward normalization")))),(0,l.yg)("h2",{id:"key-components-of-ppo"},"Key Components of PPO"),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("p",{parentName:"li"},(0,l.yg)("strong",{parentName:"p"},"Actor-Critic Architecture"),": PPO requires an actor model (policy) and a critic model (value function). This is different from algorithms like GRPO and RLOO that don't require a critic model.")),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("p",{parentName:"li"},(0,l.yg)("strong",{parentName:"p"},"Generalized Advantage Estimation (GAE)"),": PPO uses GAE to compute advantage values, which helps reduce variance in policy gradient estimates while maintaining low bias.")),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("p",{parentName:"li"},(0,l.yg)("strong",{parentName:"p"},"Clipped Surrogate Objective Function"),": The core of PPO is implemented through a clipped surrogate objective function that constrains policy updates."))),(0,l.yg)("h2",{id:"kl-divergence-control"},"KL Divergence Control"),(0,l.yg)("p",null,"PPO provides two mechanisms to prevent the policy from deviating too far from the reference policy:"),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("p",{parentName:"li"},(0,l.yg)("strong",{parentName:"p"},"KL Loss")," (GRPO approach, optional):"),(0,l.yg)("ul",{parentName:"li"},(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL loss in the actor"),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": Coefficient for KL loss"),(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"kl_penalty"),": KL penalty options"))),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("p",{parentName:"li"},(0,l.yg)("strong",{parentName:"p"},"KL Penalty in Rewards"),":"),(0,l.yg)("ul",{parentName:"li"},(0,l.yg)("li",{parentName:"ul"},"A KL penalty term can be added to the reward function to control policy updates")))),(0,l.yg)("h2",{id:"dual-clip-ppo"},"Dual-clip PPO"),(0,l.yg)("p",null,"Dual-Clip PPO introduces a method that applies a lower bound to the policy ratio when the advantage is less than zero, preventing it from exceeding the specified lower bound when multiplied by a large ratio."),(0,l.yg)("h2",{id:"usage-recommendations"},"Usage Recommendations"),(0,l.yg)("ol",null,(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Batch Size"),": Adjust ",(0,l.yg)("inlineCode",{parentName:"li"},"rollout_batch_size")," and related parameters according to GPU memory"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"KL Control"),": It is recommended to enable ",(0,l.yg)("inlineCode",{parentName:"li"},"use_kl_loss")," and set an appropriate ",(0,l.yg)("inlineCode",{parentName:"li"},"kl_loss_coef")," value (e.g., 0.001)"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Clipping Parameters"),": ",(0,l.yg)("inlineCode",{parentName:"li"},"pg_clip")," is typically set to 0.2 and can be adjusted according to specific tasks"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Advantage Estimation"),": ",(0,l.yg)("inlineCode",{parentName:"li"},"whiten_advantages")," is typically set to true to improve training stability"),(0,l.yg)("li",{parentName:"ol"},(0,l.yg)("strong",{parentName:"li"},"Loss Aggregation Mode"),": Different ",(0,l.yg)("inlineCode",{parentName:"li"},"loss_agg_mode")," options can be tried to optimize training effectiveness")),(0,l.yg)("h2",{id:"reference-example"},"Reference Example"),(0,l.yg)("p",null,"You can refer to the following configuration file to set up PPO training:"),(0,l.yg)("ul",null,(0,l.yg)("li",{parentName:"ul"},(0,l.yg)("inlineCode",{parentName:"li"},"/examples/docs_examples/example_ppo.yaml"))),(0,l.yg)("p",null,"This example shows how to configure and run PPO training."))}y.isMDXComponent=!0},5680:(e,t,n)=>{n.d(t,{xA:()=>m,yg:()=>u});var a=n(6540);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach(function(t){l(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function o(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var g=a.createContext({}),p=function(e){var t=a.useContext(g),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(g.Provider,{value:t},e.children)},d="mdxType",y={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},s=a.forwardRef(function(e,t){var n=e.components,l=e.mdxType,r=e.originalType,g=e.parentName,m=o(e,["components","mdxType","originalType","parentName"]),d=p(n),s=l,u=d["".concat(g,".").concat(s)]||d[s]||y[s]||r;return n?a.createElement(u,i(i({ref:t},m),{},{components:n})):a.createElement(u,i({ref:t},m))});function u(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var r=n.length,i=new Array(r);i[0]=s;var o={};for(var g in t)hasOwnProperty.call(t,g)&&(o[g]=t[g]);o.originalType=e,o[d]="string"==typeof e?e:l,i[1]=o;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}s.displayName="MDXCreateElement"}}]);