"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[864],{84344(e,n,i){i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"User Guides/Advanced Features/sequence_packing","title":"SEQUENCE PACKING IN ROLL","description":"The ROLL framework now supports Sequence Packing, a feature that eliminates padding tokens by packing variable-length sequences together, thereby improving computational efficiency. This document provides a detailed explanation of the implementation rationale and configuration methods for this feature.","source":"@site/docs/User Guides/Advanced Features/sequence_packing.md","sourceDirName":"User Guides/Advanced Features","slug":"/User Guides/Advanced Features/sequence_packing","permalink":"/ROLL/docs/User Guides/Advanced Features/sequence_packing","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Advanced Features/sequence_packing.md","tags":[],"version":"current","lastUpdatedAt":1770642330000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"GPU Time-Division Multiplexing Control Guide","permalink":"/ROLL/docs/User Guides/Advanced Features/offload_reload_control"},"next":{"title":"Trackers and Metrics","permalink":"/ROLL/docs/User Guides/Tracker & Metrics/trackers_and_metrics"}}');var l=i(74848),c=i(28453);const r={},a="SEQUENCE PACKING IN ROLL",t={},o=[{value:"1. Introduction",id:"1-introduction",level:2},{value:"2. Implementation Principles",id:"2-implementation-principles",level:2},{value:"2.1 Data Partitioning Hierarchy",id:"21-data-partitioning-hierarchy",level:3},{value:"2.2 Core Mechanism of Sequence Packing",id:"22-core-mechanism-of-sequence-packing",level:3},{value:"2.2.1 Alignment Requirement: Multiple of 2 \xd7 CP_SIZE \xd7 TP_SIZE",id:"221-alignment-requirement-multiple-of-2--cp_size--tp_size",level:4},{value:"2.2.2 Why the Factor of 2? Detailed Explanation of CP Load Balancing",id:"222-why-the-factor-of-2-detailed-explanation-of-cp-load-balancing",level:4},{value:"2.2.3 Complete Packing Example",id:"223-complete-packing-example",level:4},{value:"2.3 Loss Computation Workflow",id:"23-loss-computation-workflow",level:3},{value:"2.4 Load Balancing Optimization",id:"24-load-balancing-optimization",level:3},{value:"3. Implementation Workflow",id:"3-implementation-workflow",level:2},{value:"3.1 Core Packing and Unpacking Logic",id:"31-core-packing-and-unpacking-logic",level:3},{value:"3.2 Load Balancing Responsibility Allocation",id:"32-load-balancing-responsibility-allocation",level:3},{value:"4. Configuration Parameters",id:"4-configuration-parameters",level:2},{value:"4.1 How to Enable Sequence Packing",id:"41-how-to-enable-sequence-packing",level:3},{value:"4.2 Parameter Details (Plain Language)",id:"42-parameter-details-plain-language",level:3},{value:"<code>algorithm</code> (Packing Algorithm)",id:"algorithm-packing-algorithm",level:4},{value:"<code>max_packed_sequence_length_train</code> (Max Packed Length for Training)",id:"max_packed_sequence_length_train-max-packed-length-for-training",level:4},{value:"<code>max_packed_sequence_length_forward</code> (Max Packed Length for Inference)",id:"max_packed_sequence_length_forward-max-packed-length-for-inference",level:4},{value:"<code>min_num_micro_batches_train</code> (Minimum Micro-Batches for Training)",id:"min_num_micro_batches_train-minimum-micro-batches-for-training",level:4},{value:"<code>min_num_micro_batches_forward</code> (Minimum Micro-Batches for Inference)",id:"min_num_micro_batches_forward-minimum-micro-batches-for-inference",level:4},{value:"4.3 Full Configuration Example",id:"43-full-configuration-example",level:3},{value:"4.4 Usage Recommendations",id:"44-usage-recommendations",level:3}];function d(e){const n={blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,c.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"sequence-packing-in-roll",children:"SEQUENCE PACKING IN ROLL"})}),"\n",(0,l.jsxs)(n.p,{children:["The ROLL framework now supports ",(0,l.jsx)(n.strong,{children:"Sequence Packing"}),", a feature that eliminates padding tokens by packing variable-length sequences together, thereby improving computational efficiency. This document provides a detailed explanation of the implementation rationale and configuration methods for this feature."]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note"}),": Currently, only ",(0,l.jsx)(n.code,{children:"megatron_strategy"})," supports ",(0,l.jsx)(n.code,{children:"sequence_packing"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"1-introduction",children:"1. Introduction"}),"\n",(0,l.jsx)(n.p,{children:"In reinforcement learning (RL) training scenarios, rollout data typically exhibits a long-tailed distribution. In conventional training pipelines, samples within a micro-batch are padded to a fixed maximum sequence length before being grouped into a batch for training. This approach wastes significant computational resources on processing padding tokens and slows down training."}),"\n",(0,l.jsxs)(n.p,{children:["To address this issue, ROLL introduces ",(0,l.jsx)(n.strong,{children:"Sequence Packing"}),", which:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Packs sequences of varying lengths within each micro-batch to eliminate padding tokens."}),"\n",(0,l.jsx)(n.li,{children:"Employs optimized packing algorithms to improve packing efficiency, reduce the number of micro-batches, and accelerate training."}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"2-implementation-principles",children:"2. Implementation Principles"}),"\n",(0,l.jsx)(n.h3,{id:"21-data-partitioning-hierarchy",children:"2.1 Data Partitioning Hierarchy"}),"\n",(0,l.jsx)(n.p,{children:"In distributed training, data is organized in the following hierarchical structure:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"GLOBAL BATCH (Global Batch)\n\u251c\u2500\u2500 DP RANK 0 \u2192 BATCH 0\n\u2502   \u2514\u2500\u2500 MINI BATCH 0 (used for one gradient update)\n\u2502       \u251c\u2500\u2500 MICRO BATCH 0 (smallest computation unit)\n\u2502       \u251c\u2500\u2500 MICRO BATCH 1\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 DP RANK 1 \u2192 BATCH 1  \n\u2502   \u2514\u2500\u2500 MINI BATCH 0\n\u2502       \u251c\u2500\u2500 MICRO BATCH 0\n\u2502       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"GLOBAL BATCH"}),": The complete rollout results generated by ",(0,l.jsx)(n.code,{children:"actor_infer"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"BATCH"}),": A subset of the Global Batch assigned to a specific Data Parallel (DP) rank."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"MINI BATCH"}),": A portion of a Batch used for a single gradient update (considering gradient accumulation)."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"MICRO BATCH"}),": The smallest computational unit derived from a Mini Batch, used in a single forward/backward pass."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"In standard training, all samples within a micro-batch are padded to a fixed length, leading to substantial computational waste. Sequence Packing solves this by packing sequences at the micro-batch level."}),"\n",(0,l.jsx)(n.h3,{id:"22-core-mechanism-of-sequence-packing",children:"2.2 Core Mechanism of Sequence Packing"}),"\n",(0,l.jsx)(n.p,{children:"The primary goal of Sequence Packing is to eliminate padding tokens while ensuring correct and efficient execution under complex distributed training configurations\u2014particularly when Context Parallelism (CP) and Tensor Parallelism (TP) are enabled. To achieve this, the packing process must satisfy specific alignment constraints critical for both correctness and performance."}),"\n",(0,l.jsx)(n.h4,{id:"221-alignment-requirement-multiple-of-2--cp_size--tp_size",children:"2.2.1 Alignment Requirement: Multiple of 2 \xd7 CP_SIZE \xd7 TP_SIZE"}),"\n",(0,l.jsxs)(n.p,{children:["When Context Parallelism (CP) and Tensor Parallelism (TP) are enabled, the packed sequence length ",(0,l.jsxs)(n.strong,{children:["must be a multiple of ",(0,l.jsx)(n.code,{children:"2 \xd7 CP_SIZE \xd7 TP_SIZE"})]}),"."]}),"\n",(0,l.jsx)(n.p,{children:"This requirement stems from the needs of both parallelism strategies:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"TENSOR PARALLELISM (TP)"}),": When Sequence Parallelism is enabled, sequences are split across TP ranks during the forward pass. Thus, the sequence length must be divisible by ",(0,l.jsx)(n.code,{children:"TP_SIZE"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"CONTEXT PARALLELISM (CP)"}),": To achieve load balancing in CP, sequences must be logically divided into ",(0,l.jsx)(n.code,{children:"2 \xd7 CP_SIZE"})," chunks. Hence, the sequence length must also be divisible by ",(0,l.jsx)(n.code,{children:"2 \xd7 CP_SIZE"}),"."]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["Combining these two requirements, the sequence length must be a multiple of ",(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"2 \xd7 CP_SIZE \xd7 TP_SIZE"})})," to ensure compatibility with both TP and CP."]}),"\n",(0,l.jsx)(n.h4,{id:"222-why-the-factor-of-2-detailed-explanation-of-cp-load-balancing",children:"2.2.2 Why the Factor of 2? Detailed Explanation of CP Load Balancing"}),"\n",(0,l.jsx)(n.p,{children:"In Context Parallel (CP) training, the asymmetric nature of causal attention leads to severe load imbalance."}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Root Cause \u2013 Asymmetry in Causal Attention"})}),"\n",(0,l.jsxs)(n.p,{children:["Consider a sequence of length 6: ",(0,l.jsx)(n.code,{children:"[0, 1, 2, 3, 4, 5]"}),", with ",(0,l.jsx)(n.code,{children:"CP=2"}),":"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Full causal attention mask:\n     0  1  2  3  4  5\n0  [ 1  0  0  0  0  0 ]\n1  [ 1  1  0  0  0  0 ]  \n2  [ 1  1  1  0  0  0 ]\n3  [ 1  1  1  1  0  0 ]\n4  [ 1  1  1  1  1  0 ]\n5  [ 1  1  1  1  1  1 ]\n"})}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Problem with Naive Partitioning"}),":"]}),"\n",(0,l.jsx)(n.p,{children:"If the sequence is simply split evenly:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["CP0 handles: ",(0,l.jsx)(n.code,{children:"[0, 1, 2]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP1 handles: ",(0,l.jsx)(n.code,{children:"[3, 4, 5]"})]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"The actual computational loads become:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"CP0"}),": Only computes attention weights for its own positions (6 weight computations)."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"CP1"}),": Must compute attention weights from its positions to all preceding positions (15 weight computations)."]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Load ratio: 6:15 = 2:5"})," \u2014 CP1 bears 2.5\xd7 more computation than CP0!"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Solution \u2013 2\xd7CP Interleaved Chunking"})}),"\n",(0,l.jsxs)(n.p,{children:["Megatron-Core resolves this by splitting the sequence into ",(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"2 \xd7 CP"})})," chunks and applying an interleaved assignment strategy:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Original sequence: [0, 1, 2, 3, 4, 5]\nSplit into 4 chunks: |[0,1]|[2,3]|[4,5]|[p,p]|  (padded to multiple of 4)\n\nInterleaved assignment:\n- Chunk 0 [0,1] \u2192 CP0\n- Chunk 1 [2,3] \u2192 CP1  \n- Chunk 2 [4,5] \u2192 CP1\n- Chunk 3 [p,p] \u2192 CP0\n\nFinal assignment:\n- CP0: [0,1] + [p,p]\n- CP1: [2,3] + [4,5]\n"})}),"\n",(0,l.jsx)(n.p,{children:"This carefully designed assignment balances the computational load between CP ranks, avoiding performance bottlenecks."}),"\n",(0,l.jsxs)(n.p,{children:["Thus, ",(0,l.jsx)(n.strong,{children:"the factor of 2 is essential for CP load balancing"}),", ensuring roughly equal workloads across CP ranks under causal attention."]}),"\n",(0,l.jsx)(n.h4,{id:"223-complete-packing-example",children:"2.2.3 Complete Packing Example"}),"\n",(0,l.jsx)(n.p,{children:"Assume a micro-batch contains the following samples (original max sequence length = 8):"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Sample ID"}),(0,l.jsx)(n.th,{children:"Original Sequence"}),(0,l.jsx)(n.th,{children:"Valid Length"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"0"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"[0, 0, p, p, p, p, p, p]"})}),(0,l.jsx)(n.td,{children:"2"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"1"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"[1, 1, 1, 1, p, p, p, p]"})}),(0,l.jsx)(n.td,{children:"4"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"2"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"[2, 2, 2, 2, 2, 2, p, p]"})}),(0,l.jsx)(n.td,{children:"6"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:"3"}),(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"[3, p, p, p, p, p, p, p]"})}),(0,l.jsx)(n.td,{children:"1"})]})]})]}),"\n",(0,l.jsxs)(n.p,{children:["Configuration: ",(0,l.jsx)(n.code,{children:"CP_SIZE=2"}),", ",(0,l.jsx)(n.code,{children:"TP_SIZE=1"})]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Step 1: Remove original padding"})}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Sample 0: [0, 0]\nSample 1: [1, 1, 1, 1]  \nSample 2: [2, 2, 2, 2, 2, 2]\nSample 3: [3]\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Step 2: Re-pad to alignment boundary"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Alignment factor = 2 \xd7 CP_SIZE \xd7 TP_SIZE = 2 \xd7 2 \xd7 1 = 4"}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Re-padded sequences:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"Sample 0: [0, 0, p, p] \u2192 length 4\nSample 1: [1, 1, 1, 1] \u2192 length 4  \nSample 2: [2, 2, 2, 2, 2, 2, p, p] \u2192 length 8\nSample 3: [3, p, p, p] \u2192 length 4\n"})}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Step 3: Detailed CP Chunking Process"})}),"\n",(0,l.jsxs)(n.p,{children:["With ",(0,l.jsx)(n.code,{children:"CP_SIZE=2"}),", each sequence is logically split into ",(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"2 \xd7 CP_SIZE = 4"})})," segments and assigned via interleaving:"]}),"\n",(0,l.jsxs)(n.p,{children:["For any sequence of length L under ",(0,l.jsx)(n.code,{children:"CP_SIZE=2"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Split into 4 consecutive segments: seg0, seg1, seg2, seg3"}),"\n",(0,l.jsx)(n.li,{children:"Each segment has length L/4"}),"\n",(0,l.jsxs)(n.li,{children:["Assignment rule:","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"CP0"}),": seg0 + seg3"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"CP1"}),": seg1 + seg2"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Applied to our example:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Sample 0"})," ",(0,l.jsx)(n.code,{children:"[0, 0, p, p]"})," (length 4):"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["seg0: ",(0,l.jsx)(n.code,{children:"[0]"}),", seg1: ",(0,l.jsx)(n.code,{children:"[0]"}),", seg2: ",(0,l.jsx)(n.code,{children:"[p]"}),", seg3: ",(0,l.jsx)(n.code,{children:"[p]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP0 gets: seg0 + seg3 = ",(0,l.jsx)(n.code,{children:"[0] + [p]"})," \u2192 processes ",(0,l.jsx)(n.code,{children:"[0, p]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP1 gets: seg1 + seg2 = ",(0,l.jsx)(n.code,{children:"[0] + [p]"})," \u2192 processes ",(0,l.jsx)(n.code,{children:"[0, p]"})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Sample 1"})," ",(0,l.jsx)(n.code,{children:"[1, 1, 1, 1]"})," (length 4):"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["seg0: ",(0,l.jsx)(n.code,{children:"[1]"}),", seg1: ",(0,l.jsx)(n.code,{children:"[1]"}),", seg2: ",(0,l.jsx)(n.code,{children:"[1]"}),", seg3: ",(0,l.jsx)(n.code,{children:"[1]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP0: ",(0,l.jsx)(n.code,{children:"[1] + [1]"})," \u2192 ",(0,l.jsx)(n.code,{children:"[1, 1]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP1: ",(0,l.jsx)(n.code,{children:"[1] + [1]"})," \u2192 ",(0,l.jsx)(n.code,{children:"[1, 1]"})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Sample 2"})," ",(0,l.jsx)(n.code,{children:"[2, 2, 2, 2, 2, 2, p, p]"})," (length 8):"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["seg0: ",(0,l.jsx)(n.code,{children:"[2, 2]"}),", seg1: ",(0,l.jsx)(n.code,{children:"[2, 2]"}),", seg2: ",(0,l.jsx)(n.code,{children:"[2, 2]"}),", seg3: ",(0,l.jsx)(n.code,{children:"[p, p]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP0: ",(0,l.jsx)(n.code,{children:"[2, 2] + [p, p]"})," \u2192 ",(0,l.jsx)(n.code,{children:"[2, 2, p, p]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP1: ",(0,l.jsx)(n.code,{children:"[2, 2] + [2, 2]"})," \u2192 ",(0,l.jsx)(n.code,{children:"[2, 2, 2, 2]"})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Sample 3"})," ",(0,l.jsx)(n.code,{children:"[3, p, p, p]"})," (length 4):"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["seg0: ",(0,l.jsx)(n.code,{children:"[3]"}),", seg1: ",(0,l.jsx)(n.code,{children:"[p]"}),", seg2: ",(0,l.jsx)(n.code,{children:"[p]"}),", seg3: ",(0,l.jsx)(n.code,{children:"[p]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP0: ",(0,l.jsx)(n.code,{children:"[3] + [p]"})," \u2192 ",(0,l.jsx)(n.code,{children:"[3, p]"})]}),"\n",(0,l.jsxs)(n.li,{children:["CP1: ",(0,l.jsx)(n.code,{children:"[p] + [p]"})," \u2192 ",(0,l.jsx)(n.code,{children:"[p, p]"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Step 4: Final Packed Input per CP Rank"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"CP0\u2019s full input"}),": ",(0,l.jsx)(n.code,{children:"[0, p, 1, 1, 2, 2, p, p, 3, p]"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"CP1\u2019s full input"}),": ",(0,l.jsx)(n.code,{children:"[0, p, 1, 1, 2, 2, 2, 2, p, p]"})]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Step 5: Cumulative Sequence Lengths"})}),"\n",(0,l.jsxs)(n.p,{children:["Padded cumulative lengths: ",(0,l.jsx)(n.code,{children:"[0, 4, 8, 16, 20]"})]}),"\n",(0,l.jsx)(n.h3,{id:"23-loss-computation-workflow",children:"2.3 Loss Computation Workflow"}),"\n",(0,l.jsx)(n.p,{children:"Under Sequence Packing, loss calculation requires special handling:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Unpack Model Outputs"}),": Use ",(0,l.jsx)(n.code,{children:"_unpack_sequences"})," to restore individual sequences from the packed output."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Compute start/end positions of each sequence on the current CP rank using ",(0,l.jsx)(n.code,{children:"cu_seqlens_padded"}),"."]}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"seq_starts = cu_seqlens_padded[:-1] // cp_size"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"seq_ends = cu_seqlens_padded[1:] // cp_size"})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Per-Sequence Loss Calculation"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Apply the loss function to each unpacked sequence individually."}),"\n",(0,l.jsxs)(n.li,{children:["Adjust original data to match the actual sequence length using ",(0,l.jsx)(n.code,{children:"adjust_sequence_length"}),"."]}),"\n",(0,l.jsx)(n.li,{children:"Accumulate losses from all sequences."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Result Aggregation"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Sum all per-sequence losses to obtain the total loss."}),"\n",(0,l.jsx)(n.li,{children:"Aggregate metrics across sequences."}),"\n",(0,l.jsx)(n.li,{children:"Apply loss scaling if enabled."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"This per-sequence approach ensures correct loss computation even under complex combinations of CP, TP, and packing."}),"\n",(0,l.jsx)(n.h3,{id:"24-load-balancing-optimization",children:"2.4 Load Balancing Optimization"}),"\n",(0,l.jsxs)(n.p,{children:["To maximize the effectiveness of Sequence Packing, ROLL applies the ",(0,l.jsx)(n.strong,{children:"Karmarkar-Karp algorithm"})," at multiple levels for load balancing."]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Karmarkar-Karp Algorithm Overview"}),":\nA classical multi-way partitioning algorithm that divides a set of numbers into ",(0,l.jsx)(n.em,{children:"k"})," subsets with sums as balanced as possible. In Sequence Packing, it ensures computational loads across processing units remain balanced, preventing bottlenecks."]}),"\n",(0,l.jsx)(n.p,{children:"Key optimizations include:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"GLOBAL BATCH \u2192 DP RANK Load Balancing"}),": Ensures each DP rank receives a similar total number of tokens."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"MINI BATCH \u2192 MICRO BATCH Load Balancing"}),": Balances computational load across micro-batches."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"Implementation details and responsibility allocation are described in Section 3.2."}),"\n",(0,l.jsx)(n.h2,{id:"3-implementation-workflow",children:"3. Implementation Workflow"}),"\n",(0,l.jsx)(n.h3,{id:"31-core-packing-and-unpacking-logic",children:"3.1 Core Packing and Unpacking Logic"}),"\n",(0,l.jsxs)(n.p,{children:["Packing logic resides primarily in the strategy layer. When ",(0,l.jsx)(n.code,{children:"use_sequence_packing"})," is enabled, the strategy automatically packs micro-batches and unpacks logits for loss computation."]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsxs)(n.strong,{children:["Core packing function ",(0,l.jsx)(n.code,{children:"_pack_sequences"})," performs"]}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Removes original padding and extracts valid tokens."}),"\n",(0,l.jsx)(n.li,{children:"Computes cumulative sequence lengths (both original and padded)."}),"\n",(0,l.jsxs)(n.li,{children:["Re-pads sequences to a multiple of ",(0,l.jsx)(n.code,{children:"2 * cp_size * tp_size"}),"."]}),"\n",(0,l.jsx)(n.li,{children:"Handles CP chunking and assignment."}),"\n",(0,l.jsxs)(n.li,{children:["Concatenates sequences and creates ",(0,l.jsx)(n.code,{children:"PackedSeqParams"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Loss computation"})," is handled by ",(0,l.jsx)(n.code,{children:"loss_wrapper"}),", which unpacks outputs and computes per-sequence losses."]}),"\n",(0,l.jsx)(n.h3,{id:"32-load-balancing-responsibility-allocation",children:"3.2 Load Balancing Responsibility Allocation"}),"\n",(0,l.jsx)(n.p,{children:"Load balancing in ROLL follows a clear division of responsibilities:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"GLOBAL BATCH \u2192 DP RANK Load Balancing"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Responsible Module"}),": Pipeline layer (",(0,l.jsx)(n.code,{children:"batch_balance"})," function)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Objective"}),": Equalize total token count per DP rank"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Method"}),": Apply Karmarkar-Karp algorithm before data distribution"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"MINI BATCH \u2192 MICRO BATCH Load Balancing"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Responsible Module"}),": Strategy layer (",(0,l.jsx)(n.code,{children:"make_micro_batch_iter_for_sequence_packing"}),")"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Objective"}),": Balance computational load across micro-batches"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Method"}),": Apply Karmarkar-Karp during micro-batch generation"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Preservation of Randomness"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["The division from Batch \u2192 Mini Batch retains randomness (for shuffling) and thus does ",(0,l.jsx)(n.strong,{children:"not"})," apply load balancing."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"This layered optimization ensures balanced workloads from global to local levels, maximizing hardware utilization."}),"\n",(0,l.jsx)(n.h2,{id:"4-configuration-parameters",children:"4. Configuration Parameters"}),"\n",(0,l.jsx)(n.h3,{id:"41-how-to-enable-sequence-packing",children:"4.1 How to Enable Sequence Packing"}),"\n",(0,l.jsxs)(n.p,{children:["To use Sequence Packing, simply set ",(0,l.jsx)(n.code,{children:"use_sequence_packing: true"})," in your configuration file."]}),"\n",(0,l.jsx)(n.h3,{id:"42-parameter-details-plain-language",children:"4.2 Parameter Details (Plain Language)"}),"\n",(0,l.jsxs)(n.h4,{id:"algorithm-packing-algorithm",children:[(0,l.jsx)(n.code,{children:"algorithm"})," (Packing Algorithm)"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"none"})}),": Default simple packing\u2014sequences are packed in their original order."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"load_balance"})}),": Intelligent load-balanced packing\u2014reorders data to balance computational load across micro-batches. ",(0,l.jsx)(n.strong,{children:"Recommended"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"max_packed_sequence_length_train-max-packed-length-for-training",children:[(0,l.jsx)(n.code,{children:"max_packed_sequence_length_train"})," (Max Packed Length for Training)"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Controls the maximum allowed length of a packed sequence during training."}),"\n",(0,l.jsx)(n.li,{children:"E.g., setting to 8192 means no packed sequence will exceed 8192 tokens."}),"\n",(0,l.jsx)(n.li,{children:"Choose a reasonable value to avoid out-of-memory errors while maintaining packing efficiency."}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"max_packed_sequence_length_forward-max-packed-length-for-inference",children:[(0,l.jsx)(n.code,{children:"max_packed_sequence_length_forward"})," (Max Packed Length for Inference)"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Same as above, but applied during inference."}),"\n",(0,l.jsx)(n.li,{children:"Typically set to the same value as the training parameter."}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"min_num_micro_batches_train-minimum-micro-batches-for-training",children:[(0,l.jsx)(n.code,{children:"min_num_micro_batches_train"})," (Minimum Micro-Batches for Training)"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Specifies the minimum number of micro-batches per mini-batch during training."}),"\n",(0,l.jsx)(n.li,{children:"Setting to 1 means no constraint\u2014the system auto-determines optimal splitting."}),"\n",(0,l.jsx)(n.li,{children:"Increase this value if facing GPU memory issues to reduce micro-batch size."}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"min_num_micro_batches_forward-minimum-micro-batches-for-inference",children:[(0,l.jsx)(n.code,{children:"min_num_micro_batches_forward"})," (Minimum Micro-Batches for Inference)"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Same as above, but for inference."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"43-full-configuration-example",children:"4.3 Full Configuration Example"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-yaml",children:"actor_train:\n  # Enable sequence packing\n  use_sequence_packing: True\n  \n  # Sequence packing configuration\n  sequence_packing_args:\n    # Use load-balancing algorithm for better performance\n    algorithm: load_balance\n    \n    # Max packed sequence length during training\n    max_packed_sequence_length_train: 8192\n    \n    # Max packed sequence length during inference\n    max_packed_sequence_length_forward: 8192\n    \n    # Minimum 1 micro-batch during training (no constraint)\n    min_num_micro_batches_train: 1\n    \n    # Minimum 1 micro-batch during inference\n    min_num_micro_batches_forward: 1\n  \n  # Sequence packing requires megatron strategy\n  strategy_args:\n    strategy_name: megatron_train\n"})}),"\n",(0,l.jsx)(n.h3,{id:"44-usage-recommendations",children:"4.4 Usage Recommendations"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Mandatory Condition"}),": Only supported under ",(0,l.jsx)(n.code,{children:"megatron_train"})," or ",(0,l.jsx)(n.code,{children:"megatron_infer"})," strategies."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Recommended Setting"}),": Use ",(0,l.jsx)(n.code,{children:"algorithm: load_balance"})," for optimal performance."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Length Tuning"}),": Set ",(0,l.jsx)(n.code,{children:"max_packed_sequence_length"})," based on your GPU memory capacity\u2014typically equal to the model\u2019s maximum supported sequence length."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Custom Loss Functions"}),": If using a custom loss function with sequence packing, refer to the custom loss documentation and ensure ",(0,l.jsx)(n.code,{children:"apply_loss_scale"})," is correctly configured."]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"With proper configuration, Sequence Packing significantly boosts training efficiency\u2014especially in RL scenarios with highly variable sequence lengths\u2014while maintaining model performance."})]})}function h(e={}){const{wrapper:n}={...(0,c.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},28453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var s=i(96540);const l={},c=s.createContext(l);function r(e){const n=s.useContext(c);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),s.createElement(c.Provider,{value:n},e.children)}}}]);