"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[5704],{28453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var i=r(96540);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}},96318:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Getting Started/FAQ/qa_issues","title":"Frequently Asked Questions (Q&A)","description":"This document compiles common issues that may be encountered when using the ROLL framework and their solutions.","source":"@site/docs/Getting Started/FAQ/qa_issues.md","sourceDirName":"Getting Started/FAQ","slug":"/Getting Started/FAQ/qa_issues","permalink":"/ROLL/docs/Getting Started/FAQ/qa_issues","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/Getting Started/FAQ/qa_issues.md","tags":[],"version":"current","lastUpdatedAt":1764225914000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ROLL Debugging Guide","permalink":"/ROLL/docs/Getting Started/Debugging Guide/debug_guide"},"next":{"title":"Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/config_guide"}}');var o=r(74848),t=r(28453);const s={},a="Frequently Asked Questions (Q&A)",d={},c=[{value:"Model Conversion Related",id:"model-conversion-related",level:2},{value:"How to convert Megatron models to HF format?",id:"how-to-convert-megatron-models-to-hf-format",level:3},{value:"Resource Configuration Related",id:"resource-configuration-related",level:2},{value:"What is colocate mode?",id:"what-is-colocate-mode",level:3},{value:"What is separate mode?",id:"what-is-separate-mode",level:3},{value:"Training Parameters Related",id:"training-parameters-related",level:2},{value:"What do <code>rollout_batch_size</code> and <code>num_return_sequences_in_group</code> mean?",id:"what-do-rollout_batch_size-and-num_return_sequences_in_group-mean",level:3},{value:"How to set <code>gradient_accumulation_steps</code> and <code>per_device_train_batch_size</code>?",id:"how-to-set-gradient_accumulation_steps-and-per_device_train_batch_size",level:3},{value:"For DeepSpeed Backend:",id:"for-deepspeed-backend",level:4},{value:"For Megatron Backend:",id:"for-megatron-backend",level:4},{value:"Debugging and Performance Analysis Related",id:"debugging-and-performance-analysis-related",level:2},{value:"How to get the training timeline?",id:"how-to-get-the-training-timeline",level:3},{value:"How to debug code?",id:"how-to-debug-code",level:3},{value:"Common Errors and Solutions",id:"common-errors-and-solutions",level:2},{value:"Error: <code>self.node2pg[node_rank] KeyError: 1</code>",id:"error-selfnode2pgnode_rank-keyerror-1",level:3},{value:"Error: <code>assert self.lr_decay_steps &gt; 0</code>",id:"error-assert-selflr_decay_steps--0",level:3},{value:"Error: <code>AssertionError: batch_size 32 &lt; chunks 64</code>",id:"error-assertionerror-batch_size-32--chunks-64",level:3},{value:"Error: <code>TypeError: BackendCompilerFailed.__init__() missing 1 required positional argument</code>",id:"error-typeerror-backendcompilerfailed__init__-missing-1-required-positional-argument",level:3}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"frequently-asked-questions-qa",children:"Frequently Asked Questions (Q&A)"})}),"\n",(0,o.jsx)(n.p,{children:"This document compiles common issues that may be encountered when using the ROLL framework and their solutions."}),"\n",(0,o.jsx)(n.h2,{id:"model-conversion-related",children:"Model Conversion Related"}),"\n",(0,o.jsx)(n.h3,{id:"how-to-convert-megatron-models-to-hf-format",children:"How to convert Megatron models to HF format?"}),"\n",(0,o.jsx)(n.p,{children:"Use the following command for format conversion:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"python mcore_adapter/tools/convert.py --checkpoint_path path_to_megatron_model --output_path path_to_output_hf_model\n"})}),"\n",(0,o.jsx)(n.h2,{id:"resource-configuration-related",children:"Resource Configuration Related"}),"\n",(0,o.jsx)(n.h3,{id:"what-is-colocate-mode",children:"What is colocate mode?"}),"\n",(0,o.jsxs)(n.p,{children:["In colocate mode, multiple roles (such as ",(0,o.jsx)(n.code,{children:"actor_train"}),", ",(0,o.jsx)(n.code,{children:"actor_infer"}),", ",(0,o.jsx)(n.code,{children:"reference"}),") can reuse the same GPU devices in their ",(0,o.jsx)(n.code,{children:"device_mapping"}),". For example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"actor_train:\n  device_mapping: list(range(0,8))\nactor_infer:\n  device_mapping: list(range(0,8))\nreference:\n  device_mapping: list(range(0,8))\n"})}),"\n",(0,o.jsx)(n.p,{children:"The framework's underlying resource management mechanism ensures GPU reuse between multiple roles, improving resource utilization."}),"\n",(0,o.jsx)(n.h3,{id:"what-is-separate-mode",children:"What is separate mode?"}),"\n",(0,o.jsxs)(n.p,{children:["In separate mode, there is no intersection between different roles' ",(0,o.jsx)(n.code,{children:"device_mapping"}),", and each role holds a set of independent GPU device resources. For example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"actor_train:\n  device_mapping: list(range(0,8))\nactor_infer:\n  device_mapping: list(range(8,16))\nreference:\n  device_mapping: list(range(16,24))\n"})}),"\n",(0,o.jsx)(n.p,{children:"This approach can avoid resource competition between roles and improve system stability."}),"\n",(0,o.jsx)(n.h2,{id:"training-parameters-related",children:"Training Parameters Related"}),"\n",(0,o.jsxs)(n.h3,{id:"what-do-rollout_batch_size-and-num_return_sequences_in_group-mean",children:["What do ",(0,o.jsx)(n.code,{children:"rollout_batch_size"})," and ",(0,o.jsx)(n.code,{children:"num_return_sequences_in_group"})," mean?"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"rollout_batch_size"}),": The number of prompts in a batch"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"num_return_sequences_in_group"}),": The sampling count for each prompt, i.e., the n parameter in vLLM/SGLang inference"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Actual number of samples in a batch = ",(0,o.jsx)(n.code,{children:"rollout_batch_size"})," * ",(0,o.jsx)(n.code,{children:"num_return_sequences_in_group"})]}),"\n",(0,o.jsx)(n.p,{children:"For Megatron Backend, note:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"rollout_batch_size * num_return_sequences_in_group must be an integer multiple of:\ngradient_accumulation_steps * per_device_train_batch_size * (world_size/tensor_model_parallel_size/pipeline_model_parallel_size/context_parallel_size)\n"})}),"\n",(0,o.jsxs)(n.h3,{id:"how-to-set-gradient_accumulation_steps-and-per_device_train_batch_size",children:["How to set ",(0,o.jsx)(n.code,{children:"gradient_accumulation_steps"})," and ",(0,o.jsx)(n.code,{children:"per_device_train_batch_size"}),"?"]}),"\n",(0,o.jsx)(n.h4,{id:"for-deepspeed-backend",children:"For DeepSpeed Backend:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"global_batch_size = per_device_train_batch_size * gradient_accumulation_steps * world_size\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Where ",(0,o.jsx)(n.code,{children:"world_size"})," is the length of ",(0,o.jsx)(n.code,{children:"device_mapping"})," for ",(0,o.jsx)(n.code,{children:"actor_train"}),"/",(0,o.jsx)(n.code,{children:"critic"})]}),"\n",(0,o.jsx)(n.h4,{id:"for-megatron-backend",children:"For Megatron Backend:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"global_batch_size = per_device_train_batch_size * gradient_accumulation_steps * world_size / \n                    tensor_model_parallel_size / pipeline_model_parallel_size / context_parallel_size\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Where ",(0,o.jsx)(n.code,{children:"world_size"})," is the length of ",(0,o.jsx)(n.code,{children:"device_mapping"})," for ",(0,o.jsx)(n.code,{children:"actor_train"}),"/",(0,o.jsx)(n.code,{children:"critic"})]}),"\n",(0,o.jsxs)(n.p,{children:["Note: No need to divide by ",(0,o.jsx)(n.code,{children:"expert_model_parallel_size"})]}),"\n",(0,o.jsx)(n.h2,{id:"debugging-and-performance-analysis-related",children:"Debugging and Performance Analysis Related"}),"\n",(0,o.jsx)(n.h3,{id:"how-to-get-the-training-timeline",children:"How to get the training timeline?"}),"\n",(0,o.jsx)(n.p,{children:"You can try enabling profiling in YAML:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'system_envs:\n  RAY_PROFILING: "1"\nprofiler_output_dir: /data/oss_bucket_0/yali/llm/profile/${exp_name}\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Then use the ",(0,o.jsx)(n.a,{href:"https://ui.perfetto.dev/",children:"Perfetto UI"})," tool for analysis."]}),"\n",(0,o.jsx)(n.h3,{id:"how-to-debug-code",children:"How to debug code?"}),"\n",(0,o.jsxs)(n.p,{children:["Set ",(0,o.jsx)(n.code,{children:'"RAY_DEBUG": "legacy"'})," in Platform' env, and then you can use pdb for step-by-step debugging."]}),"\n",(0,o.jsx)(n.h2,{id:"common-errors-and-solutions",children:"Common Errors and Solutions"}),"\n",(0,o.jsxs)(n.h3,{id:"error-selfnode2pgnode_rank-keyerror-1",children:["Error: ",(0,o.jsx)(n.code,{children:"self.node2pg[node_rank] KeyError: 1"})]}),"\n",(0,o.jsxs)(n.p,{children:["Check the total number of GPUs requested and the ",(0,o.jsx)(n.code,{children:"device_mapping"})," configuration. This error generally occurs because ",(0,o.jsx)(n.code,{children:"max(device_mapping)"})," is less than or greater than ",(0,o.jsx)(n.code,{children:"total_gpu_nums"}),"."]}),"\n",(0,o.jsxs)(n.h3,{id:"error-assert-selflr_decay_steps--0",children:["Error: ",(0,o.jsx)(n.code,{children:"assert self.lr_decay_steps > 0"})]}),"\n",(0,o.jsxs)(n.p,{children:["When ROLL distributes data, it will distribute ",(0,o.jsx)(n.code,{children:"rollout_batch_size"})," samples to each ",(0,o.jsx)(n.code,{children:"actor_train"})," worker according to DP size, and then calculate the samples for each gradient update according to ",(0,o.jsx)(n.code,{children:"gradient_accumulation_steps"}),". The configuration results in 0 when divided."]}),"\n",(0,o.jsxs)(n.p,{children:["For detailed configuration logic, refer to the manual: ",(0,o.jsx)(n.a,{href:"https://alibaba.github.io/ROLL/docs/QuickStart/config_guide#training-arguments-training_args",children:"Training Arguments"})]}),"\n",(0,o.jsxs)(n.h3,{id:"error-assertionerror-batch_size-32--chunks-64",children:["Error: ",(0,o.jsx)(n.code,{children:"AssertionError: batch_size 32 < chunks 64"})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"batch_size"})," is less than the DP size of ",(0,o.jsx)(n.code,{children:"reference"}),"/",(0,o.jsx)(n.code,{children:"actor_train"}),", causing insufficient data for splitting during dispatch. This can be resolved by adjusting ",(0,o.jsx)(n.code,{children:"rollout_batch_size"}),"."]}),"\n",(0,o.jsxs)(n.h3,{id:"error-typeerror-backendcompilerfailed__init__-missing-1-required-positional-argument",children:["Error: ",(0,o.jsx)(n.code,{children:"TypeError: BackendCompilerFailed.__init__() missing 1 required positional argument"})]}),"\n",(0,o.jsx)(n.p,{children:"You can try adding a configuration item in YAML to resolve this:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"system_envs:\n  NVTE_TORCH_COMPILE: '0'\n"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}}}]);