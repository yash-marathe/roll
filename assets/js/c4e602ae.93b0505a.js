"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[1955],{5359:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var t=n(8168),r=(n(6540),n(5680));const l={},i="TOPR (Tapered Off-Policy REINFORCE)",o={unversionedId:"English/UserGuide/algorithms/TOPR",id:"English/UserGuide/algorithms/TOPR",title:"TOPR (Tapered Off-Policy REINFORCE)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/TOPR.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/TOPR",permalink:"/ROLL/docs/English/UserGuide/algorithms/TOPR",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/TOPR.md",tags:[],version:"current",lastUpdatedAt:1761727400,formattedLastUpdatedAt:"Oct 29, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Reward Feedback Learning (Reward FL)",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reward_FL"},next:{title:"Off-Policy Algorithms Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/algorithms/offpolicy_setting"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"TOPR Configuration Parameters",id:"topr-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}],u={toc:p},g="wrapper";function m({components:e,...a}){return(0,r.yg)(g,(0,t.A)({},u,a,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"topr-tapered-off-policy-reinforce"},"TOPR (Tapered Off-Policy REINFORCE)"),(0,r.yg)("h2",{id:"introduction"},"Introduction"),(0,r.yg)("p",null,"TOPR (Tapered Off-Policy REINFORCE) is a stable and efficient reinforcement learning algorithm designed for large language models. TOPR improves training stability and efficiency by combining off-policy mechanisms with tapering techniques. TOPR works as follows:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Off-policy Mechanism"),": Utilizes historical data for training to improve sample efficiency."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Tapering Technique"),": Stabilizes the training process by gradually reducing dependence on old policies."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Policy Update"),": Updates policy parameters using a loss function that combines positive and negative samples.")),(0,r.yg)("h2",{id:"topr-configuration-parameters"},"TOPR Configuration Parameters"),(0,r.yg)("p",null,"In ROLL, the TOPR algorithm-specific configuration parameters are as follows (",(0,r.yg)("inlineCode",{parentName:"p"},"roll.pipeline.rlvr.rlvr_config.RLVRConfig"),"):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'# TOPR core config\n# TOPR\nrl_loss_coef: 0.0\npositive_loss_coef: x_1 # x_1 > 0.0\nuse_topr_neg_loss_coef: x_2 # x_2 > 0.0\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 512  # prompt\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "gae"\nnum_return_sequences_in_group: 1\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n\nwhiten_advantages: true\nadvantage_clip: 2.0\nreward_clip: ~\ndual_clip_loss: true\nlambd: 0.95\ngamma: 1\npg_clip: 0.2\nvalue_clip: ~\nkl_penalty: "kl"\ntarget_kl: ~\ninit_kl_coef: 0.2\nkl_horizon: 10000\nadd_token_level_kl: false\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n')),(0,r.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"rl_loss_coef"),": Reinforcement learning loss term coefficient, default value is 0.0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"positive_loss_coef"),": Positive sample loss term coefficient, needs to be set to a value greater than 0.0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"use_topr_neg_loss_coef"),": Negative sample loss term coefficient, needs to be set to a value greater than 0.0")),(0,r.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,r.yg)("p",null,"The following parameters are common configuration items for PPO-class algorithms:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of prompts per rollout_batch_size, default value is 512"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"prompt_length"),": Maximum length of prompts, default value is 2048"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"response_length"),": Maximum length of responses, default value is 4096"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, optional values are "gae", "reinforce", "grpo", default value is "gae"'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group"),": Number of responses generated per prompt (group size), default value is 1"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization rounds per batch of samples, default value is 1"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL divergence loss, default value is true"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": KL-loss coefficient, default value is 0.001"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"loss_agg_mode"),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values, default value is true"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range, default value is 2.0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"reward_clip"),": Reward value clipping range, default value is ~ (means not set)"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"dual_clip_loss"),": Whether to use dual clipping loss, default value is true"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"lambd"),": Lambda parameter in GAE estimator, used to trade off bias and variance, default value is 0.95"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"gamma"),": Discount factor, default value is 1"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"pg_clip"),": PPO clipping range, default value is 0.2"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"value_clip"),": Value function clipping range, default value is ~ (means not set)"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"kl_penalty"),': KL penalty options, optional values are "kl", "abs", "mse", "full", default value is "kl"'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"target_kl"),": Target KL value for adaptive KL control, default value is ~ (means not set)"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"init_kl_coef"),": Initial KL penalty coefficient, default value is 0.2"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"kl_horizon"),": Range for adaptive KL control, default value is 10000"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"add_token_level_kl"),": Whether to add token-level KL penalty, default value is false"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"norm_mean_type"),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"norm_std_type"),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None')),(0,r.yg)("h2",{id:"reference-example"},"Reference Example"),(0,r.yg)("p",null,"You can refer to the following configuration file to set up TOPR training:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_topr.yaml"))),(0,r.yg)("h2",{id:"references"},"References"),(0,r.yg)("p",null,"[1]"," Roux, N. L.; Bellemare, M. G.; Lebensold, J.; Bergeron, A.; Greaves, J.; Fr\xe9chette, A.; Pelletier, C.; Thibodeau-Laufer, E.; Toth, S.; Work, S. Tapered Off-Policy REINFORCE: Stable and Efficient Reinforcement Learning for LLMs. arXiv March 19, 2025. ",(0,r.yg)("a",{parentName:"p",href:"https://doi.org/10.48550/arXiv.2503.14286"},"https://doi.org/10.48550/arXiv.2503.14286"),"."))}m.isMDXComponent=!0},5680:(e,a,n)=>{n.d(a,{xA:()=>u,yg:()=>d});var t=n(6540);function r(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function l(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter(function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable})),n.push.apply(n,t)}return n}function i(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?l(Object(n),!0).forEach(function(a){r(e,a,n[a])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach(function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))})}return e}function o(e,a){if(null==e)return{};var n,t,r=function(e,a){if(null==e)return{};var n,t,r={},l=Object.keys(e);for(t=0;t<l.length;t++)n=l[t],a.indexOf(n)>=0||(r[n]=e[n]);return r}(e,a);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(t=0;t<l.length;t++)n=l[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=t.createContext({}),p=function(e){var a=t.useContext(s),n=a;return e&&(n="function"==typeof e?e(a):i(i({},a),e)),n},u=function(e){var a=p(e.components);return t.createElement(s.Provider,{value:a},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},c=t.forwardRef(function(e,a){var n=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),g=p(n),c=r,d=g["".concat(s,".").concat(c)]||g[c]||m[c]||l;return n?t.createElement(d,i(i({ref:a},u),{},{components:n})):t.createElement(d,i({ref:a},u))});function d(e,a){var n=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var l=n.length,i=new Array(l);i[0]=c;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[g]="string"==typeof e?e:r,i[1]=o;for(var p=2;p<l;p++)i[p]=n[p];return t.createElement.apply(null,i)}return t.createElement.apply(null,n)}c.displayName="MDXCreateElement"}}]);