"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[3851],{6331:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"DesignImplementation","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/DesignImplementation/AgenticPipeline","label":"AgenticPipeline","docId":"DesignImplementation/AgenticPipeline","unlisted":false},{"type":"link","href":"/ROLL/docs/DesignImplementation/RLVRPipeline","label":"RLVR Pipeline","docId":"DesignImplementation/RLVRPipeline","unlisted":false}]},{"type":"category","label":"DevelopmentGuide","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/DevelopmentGuide/support_new_models","label":"How to Add Support for a New Model","docId":"DevelopmentGuide/support_new_models","unlisted":false},{"type":"link","href":"/ROLL/docs/DevelopmentGuide/customer_env","label":"Customer Env","docId":"DevelopmentGuide/customer_env","unlisted":false},{"type":"link","href":"/ROLL/docs/DevelopmentGuide/prompt_intro","label":"Prompt Generation Guide","docId":"DevelopmentGuide/prompt_intro","unlisted":false}]},{"type":"category","label":"QuickStart","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/QuickStart/config_guide","label":"Configuration Guide","docId":"QuickStart/config_guide","unlisted":false},{"type":"link","href":"/ROLL/docs/QuickStart/config_system","label":"ROLL Configuration System Detailed Explanation","docId":"QuickStart/config_system","unlisted":false},{"type":"link","href":"/ROLL/docs/QuickStart/debug_guide","label":"ROLL Debugging Guide","docId":"QuickStart/debug_guide","unlisted":false},{"type":"link","href":"/ROLL/docs/QuickStart/image_address","label":"Image Provided","docId":"QuickStart/image_address","unlisted":false},{"type":"link","href":"/ROLL/docs/QuickStart/installation","label":"Installation","docId":"QuickStart/installation","unlisted":false},{"type":"link","href":"/ROLL/docs/QuickStart/multi_nodes_quick_start","label":"Quick Start: Multi-Node Deployment Guide","docId":"QuickStart/multi_nodes_quick_start","unlisted":false},{"type":"link","href":"/ROLL/docs/QuickStart/qa_issues","label":"Frequently Asked Questions (Q&A)","docId":"QuickStart/qa_issues","unlisted":false},{"type":"link","href":"/ROLL/docs/QuickStart/single_node_quick_start","label":"Quick Start: Single-Node Deployment Guide","docId":"QuickStart/single_node_quick_start","unlisted":false}]},{"type":"category","label":"UserGuide","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"agentic","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/UserGuide/agentic/Tool_Use","label":"Tool Use Guide","docId":"UserGuide/agentic/Tool_Use","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/agentic/agentic_GiGPO","label":"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)","docId":"UserGuide/agentic/agentic_GiGPO","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/agentic/agentic_StarPO","label":"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)","docId":"UserGuide/agentic/agentic_StarPO","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/agentic/agentic_engineer_practice","label":"Agentic Engineering Practice Documentation","docId":"UserGuide/agentic/agentic_engineer_practice","unlisted":false}]},{"type":"category","label":"algorithms","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/GRPO","label":"Group Relative Policy Optimization (GRPO)","docId":"UserGuide/algorithms/GRPO","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/GSPO","label":"Group Sequence Policy Optimization (GSPO)","docId":"UserGuide/algorithms/GSPO","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/LitePPO","label":"Lite PPO","docId":"UserGuide/algorithms/LitePPO","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/PPO","label":"Proximal Policy Optimization (PPO)","docId":"UserGuide/algorithms/PPO","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/RAFT_Plus_Plus","label":"RAFT++ (Reward rAnked Fine-Tuning)","docId":"UserGuide/algorithms/RAFT_Plus_Plus","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/Reinforce_Plus_Plus","label":"Reinforce++","docId":"UserGuide/algorithms/Reinforce_Plus_Plus","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/Reward_FL","label":"Reward Feedback Learning (Reward FL)","docId":"UserGuide/algorithms/Reward_FL","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/TOPR","label":"TOPR (Tapered Off-Policy REINFORCE)","docId":"UserGuide/algorithms/TOPR","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/algorithms/offpolicy_setting","label":"Off-Policy Algorithms Configuration Guide","docId":"UserGuide/algorithms/offpolicy_setting","unlisted":false}]},{"type":"category","label":"ascend","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/UserGuide/ascend/ascend_usage","label":"ROLL x Ascend","docId":"UserGuide/ascend/ascend_usage","unlisted":false}]},{"type":"link","href":"/ROLL/docs/UserGuide/async_parallel_rollout","label":"Agentic Asynchronous Parallel Rollout","docId":"UserGuide/async_parallel_rollout","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/async_training","label":"ROLL Asynchronous Training User Guide","docId":"UserGuide/async_training","unlisted":false},{"type":"category","label":"backend","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/UserGuide/backend/deepspeed","label":"DeepSpeed Training Backend Configuration Guide","docId":"UserGuide/backend/deepspeed","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/backend/fp8_rollout","label":"FP8 Quantization Configuration Guide","docId":"UserGuide/backend/fp8_rollout","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/backend/lora","label":"LoRA Fine-tuning Configuration Guide","docId":"UserGuide/backend/lora","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/backend/megatron","label":"Megatron Inference and Training Backend Configuration Guide","docId":"UserGuide/backend/megatron","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/backend/sglang","label":"SGLang Inference Backend Configuration Guide","docId":"UserGuide/backend/sglang","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/backend/vllm","label":"vLLM Inference Backend Configuration Guide","docId":"UserGuide/backend/vllm","unlisted":false}]},{"type":"link","href":"/ROLL/docs/UserGuide/checkpoint_and_resume","label":"Checkpoint Saving and Resuming Guide","docId":"UserGuide/checkpoint_and_resume","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/device_mapping","label":"ROLL Resource Configuration","docId":"UserGuide/device_mapping","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/megatron_convert_2_hf","label":"Converting MCoreAdapter Models to Hugging Face Format","docId":"UserGuide/megatron_convert_2_hf","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/offload_reload_control","label":"GPU Time-Division Multiplexing Control Guide","docId":"UserGuide/offload_reload_control","unlisted":false},{"type":"category","label":"pipeline","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ROLL/docs/UserGuide/pipeline/agent_pipeline_start","label":"Comprehensive Guide: Using the Agentic Part of ROLL","docId":"UserGuide/pipeline/agent_pipeline_start","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/pipeline/agentic_pipeline_start","label":"Agentic Pipeline","docId":"UserGuide/pipeline/agentic_pipeline_start","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/pipeline/distill_pipeline_start","label":"Distill Pipeline","docId":"UserGuide/pipeline/distill_pipeline_start","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/pipeline/dpo_pipeline_start","label":"DPO Pipeline","docId":"UserGuide/pipeline/dpo_pipeline_start","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/pipeline/rlvr_pipeline_start","label":"RLVR Pipeline","docId":"UserGuide/pipeline/rlvr_pipeline_start","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/pipeline/vl_rlvr_pipeline_start","label":"RLVR Pipeline for VLM","docId":"UserGuide/pipeline/vl_rlvr_pipeline_start","unlisted":false}]},{"type":"link","href":"/ROLL/docs/UserGuide/start","label":"start","docId":"UserGuide/start","unlisted":false},{"type":"link","href":"/ROLL/docs/UserGuide/trackers_and_metrics","label":"Trackers and Metrics","docId":"UserGuide/trackers_and_metrics","unlisted":false}]}]},"docs":{"DesignImplementation/AgenticPipeline":{"id":"DesignImplementation/AgenticPipeline","title":"AgenticPipeline","description":"Agentic Pipeline Architecture Diagram","sidebar":"tutorialSidebar"},"DesignImplementation/RLVRPipeline":{"id":"DesignImplementation/RLVRPipeline","title":"RLVR Pipeline","description":"RLVR Pipeline (Reinforcement Learning with Verifiable Rewards Pipeline) is a core component in the ROLL framework, specifically designed as an efficient distributed training pipeline for large language model reinforcement learning. Through virtual reward mechanisms, this pipeline can significantly improve LLM performance on key tasks such as complex reasoning, code generation, and mathematical calculations.","sidebar":"tutorialSidebar"},"DevelopmentGuide/customer_env":{"id":"DevelopmentGuide/customer_env","title":"Customer Env","description":"Reinforcement Learning Environment","sidebar":"tutorialSidebar"},"DevelopmentGuide/prompt_intro":{"id":"DevelopmentGuide/prompt_intro","title":"Prompt Generation Guide","description":"In the architecture of Large Language Model (LLM)-based Reinforcement Learning Agents, the Prompt serves as the sole medium for LLMs to interact with the environment. Unlike traditional agents that directly receive numerical states or output discrete action IDs, LLMs \\"perceive\\" the environment (observations) and \\"express\\" their decisions (actions) through prompts in text format.","sidebar":"tutorialSidebar"},"DevelopmentGuide/support_new_models":{"id":"DevelopmentGuide/support_new_models","title":"How to Add Support for a New Model","description":"To integrate a new model into ROLL, you must supply:","sidebar":"tutorialSidebar"},"QuickStart/config_guide":{"id":"QuickStart/config_guide","title":"Configuration Guide","description":"Pipeline Config","sidebar":"tutorialSidebar"},"QuickStart/config_system":{"id":"QuickStart/config_system","title":"ROLL Configuration System Detailed Explanation","description":"The ROLL framework adopts a structured configuration system that defines experimental parameters through YAML files. This document will provide a detailed introduction to ROLL\'s configuration design, helping new users understand the framework\'s configuration structure and extension methods.","sidebar":"tutorialSidebar"},"QuickStart/debug_guide":{"id":"QuickStart/debug_guide","title":"ROLL Debugging Guide","description":"When developing and using the ROLL framework, debugging is an essential step. This document will introduce several effective debugging methods to help you quickly locate and resolve issues.","sidebar":"tutorialSidebar"},"QuickStart/image_address":{"id":"QuickStart/image_address","title":"Image Provided","description":"We provide pre-built Docker images for a quick start (Links will be updated):","sidebar":"tutorialSidebar"},"QuickStart/installation":{"id":"QuickStart/installation","title":"Installation","description":"\ud83d\udc33 Install from Docker","sidebar":"tutorialSidebar"},"QuickStart/multi_nodes_quick_start":{"id":"QuickStart/multi_nodes_quick_start","title":"Quick Start: Multi-Node Deployment Guide","description":"Environment Preparation","sidebar":"tutorialSidebar"},"QuickStart/qa_issues":{"id":"QuickStart/qa_issues","title":"Frequently Asked Questions (Q&A)","description":"This document compiles common issues that may be encountered when using the ROLL framework and their solutions.","sidebar":"tutorialSidebar"},"QuickStart/single_node_quick_start":{"id":"QuickStart/single_node_quick_start","title":"Quick Start: Single-Node Deployment Guide","description":"Environment Preparation","sidebar":"tutorialSidebar"},"UserGuide/agentic/agentic_engineer_practice":{"id":"UserGuide/agentic/agentic_engineer_practice","title":"Agentic Engineering Practice Documentation","description":"This document introduces the development practices of the Agentic component in the ROLL framework, including environment manager development protocols, GlobalDataset usage, validation mode configuration, and trajectory synthesis functionality.","sidebar":"tutorialSidebar"},"UserGuide/agentic/agentic_GiGPO":{"id":"UserGuide/agentic/agentic_GiGPO","title":"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/agentic/agentic_StarPO":{"id":"UserGuide/agentic/agentic_StarPO","title":"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/agentic/Tool_Use":{"id":"UserGuide/agentic/Tool_Use","title":"Tool Use Guide","description":"Overview","sidebar":"tutorialSidebar"},"UserGuide/algorithms/GRPO":{"id":"UserGuide/algorithms/GRPO","title":"Group Relative Policy Optimization (GRPO)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/algorithms/GSPO":{"id":"UserGuide/algorithms/GSPO","title":"Group Sequence Policy Optimization (GSPO)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/algorithms/LitePPO":{"id":"UserGuide/algorithms/LitePPO","title":"Lite PPO","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/algorithms/offpolicy_setting":{"id":"UserGuide/algorithms/offpolicy_setting","title":"Off-Policy Algorithms Configuration Guide","description":"The ROLL framework supports multiple Off-Policy algorithm variants for reinforcement learning training. This document provides detailed configuration methods and usage examples for various algorithms.","sidebar":"tutorialSidebar"},"UserGuide/algorithms/PPO":{"id":"UserGuide/algorithms/PPO","title":"Proximal Policy Optimization (PPO)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/algorithms/RAFT_Plus_Plus":{"id":"UserGuide/algorithms/RAFT_Plus_Plus","title":"RAFT++ (Reward rAnked Fine-Tuning)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/algorithms/Reinforce_Plus_Plus":{"id":"UserGuide/algorithms/Reinforce_Plus_Plus","title":"Reinforce++","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/algorithms/Reward_FL":{"id":"UserGuide/algorithms/Reward_FL","title":"Reward Feedback Learning (Reward FL)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/algorithms/TOPR":{"id":"UserGuide/algorithms/TOPR","title":"TOPR (Tapered Off-Policy REINFORCE)","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/ascend/ascend_usage":{"id":"UserGuide/ascend/ascend_usage","title":"ROLL x Ascend","description":"Last updated: 09/28/2025.","sidebar":"tutorialSidebar"},"UserGuide/async_parallel_rollout":{"id":"UserGuide/async_parallel_rollout","title":"Agentic Asynchronous Parallel Rollout","description":"Introduction","sidebar":"tutorialSidebar"},"UserGuide/async_training":{"id":"UserGuide/async_training","title":"ROLL Asynchronous Training User Guide","description":"The ROLL framework now supports asynchronous training for both RLVR and Agentic pipelines, significantly improving training efficiency. This document provides detailed instructions on how to use this feature.","sidebar":"tutorialSidebar"},"UserGuide/backend/deepspeed":{"id":"UserGuide/backend/deepspeed","title":"DeepSpeed Training Backend Configuration Guide","description":"DeepSpeed is Microsoft\'s efficient deep learning optimization library that provides memory optimization, distributed training, and performance optimization features. This document will provide detailed instructions on how to configure and use the DeepSpeed training backend in the ROLL framework.","sidebar":"tutorialSidebar"},"UserGuide/backend/fp8_rollout":{"id":"UserGuide/backend/fp8_rollout","title":"FP8 Quantization Configuration Guide","description":"This document describes how to use FP8 quantization in ROLL to optimize inference performance and VRAM usage.","sidebar":"tutorialSidebar"},"UserGuide/backend/lora":{"id":"UserGuide/backend/lora","title":"LoRA Fine-tuning Configuration Guide","description":"LoRA (Low-Rank Adaptation) is an efficient parameter-efficient fine-tuning method that achieves parameter-efficient fine-tuning by adding low-rank matrices to pre-trained models. This document will provide detailed instructions on how to configure and use LoRA fine-tuning in the ROLL framework.","sidebar":"tutorialSidebar"},"UserGuide/backend/megatron":{"id":"UserGuide/backend/megatron","title":"Megatron Inference and Training Backend Configuration Guide","description":"Megatron is NVIDIA\'s large-scale language model training and inference framework that supports efficient distributed training and inference. This document will provide detailed instructions on how to configure and use the Megatron backend in the ROLL framework.","sidebar":"tutorialSidebar"},"UserGuide/backend/sglang":{"id":"UserGuide/backend/sglang","title":"SGLang Inference Backend Configuration Guide","description":"SGLang is a fast and easy-to-use inference engine, particularly suitable for inference tasks of large-scale language models. This document will provide detailed instructions on how to configure and use the SGLang inference backend in the ROLL framework.","sidebar":"tutorialSidebar"},"UserGuide/backend/vllm":{"id":"UserGuide/backend/vllm","title":"vLLM Inference Backend Configuration Guide","description":"vLLM is a fast and easy-to-use large language model inference library that efficiently manages attention key-value cache through PagedAttention technology. This document will provide detailed instructions on how to configure and use the vLLM inference backend in the ROLL framework.","sidebar":"tutorialSidebar"},"UserGuide/checkpoint_and_resume":{"id":"UserGuide/checkpoint_and_resume","title":"Checkpoint Saving and Resuming Guide","description":"In the ROLL framework, the checkpoint mechanism allows you to save the model state during training so that you can resume training when needed. This document will provide detailed instructions on how to configure and use the checkpoint saving and resuming functionality.","sidebar":"tutorialSidebar"},"UserGuide/device_mapping":{"id":"UserGuide/device_mapping","title":"ROLL Resource Configuration","description":"In the ROLL framework, resource settings are specified through the device_mapping parameter in YAML configuration files to determine which GPU devices each worker uses. This document will provide detailed instructions on how to configure resources, including colocated and disaggregated modes, multi-role resource configuration, and how worker counts are calculated.","sidebar":"tutorialSidebar"},"UserGuide/megatron_convert_2_hf":{"id":"UserGuide/megatron_convert_2_hf","title":"Converting MCoreAdapter Models to Hugging Face Format","description":"MCoreAdapter provides tools for converting between Megatron(McoreAdapter) and Hugging Face model formats. This document will guide you on how to convert a trained Megatron model to Hugging Face format for use in other projects.","sidebar":"tutorialSidebar"},"UserGuide/offload_reload_control":{"id":"UserGuide/offload_reload_control","title":"GPU Time-Division Multiplexing Control Guide","description":"The ROLL framework implements GPU time-division multiplexing functionality, which allows flexible sharing of GPU resources between different roles through offload/reload capabilities. This document will provide detailed instructions on how to use this feature.","sidebar":"tutorialSidebar"},"UserGuide/pipeline/agent_pipeline_start":{"id":"UserGuide/pipeline/agent_pipeline_start","title":"Comprehensive Guide: Using the Agentic Part of ROLL","description":"Table of Contents","sidebar":"tutorialSidebar"},"UserGuide/pipeline/agentic_pipeline_start":{"id":"UserGuide/pipeline/agentic_pipeline_start","title":"Agentic Pipeline","description":"Table of Contents","sidebar":"tutorialSidebar"},"UserGuide/pipeline/distill_pipeline_start":{"id":"UserGuide/pipeline/distill_pipeline_start","title":"Distill Pipeline","description":"Table of Contents","sidebar":"tutorialSidebar"},"UserGuide/pipeline/dpo_pipeline_start":{"id":"UserGuide/pipeline/dpo_pipeline_start","title":"DPO Pipeline","description":"Table of Contents","sidebar":"tutorialSidebar"},"UserGuide/pipeline/rlvr_pipeline_start":{"id":"UserGuide/pipeline/rlvr_pipeline_start","title":"RLVR Pipeline","description":"Table of Contents","sidebar":"tutorialSidebar"},"UserGuide/pipeline/vl_rlvr_pipeline_start":{"id":"UserGuide/pipeline/vl_rlvr_pipeline_start","title":"RLVR Pipeline for VLM","description":"Table of Contents","sidebar":"tutorialSidebar"},"UserGuide/start":{"id":"UserGuide/start","title":"start","description":"\ud83d\ude80 An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models \ud83d\ude80","sidebar":"tutorialSidebar"},"UserGuide/trackers_and_metrics":{"id":"UserGuide/trackers_and_metrics","title":"Trackers and Metrics","description":"The ROLL framework supports multiple experiment tracking tools to help you monitor and analyze the training process. This document will provide detailed instructions on how to configure and use these trackers.","sidebar":"tutorialSidebar"}}}}')}}]);