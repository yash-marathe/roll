"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[7073],{5170:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=a(8168),t=(a(6540),a(5680));const i={},o="Agentic Asynchronous Training Feature Usage Guide",s={unversionedId:"English/UserGuide/async_training_agentic",id:"English/UserGuide/async_training_agentic",title:"Agentic Asynchronous Training Feature Usage Guide",description:"The ROLL framework supports Agentic asynchronous training functionality, which can significantly improve training efficiency. This document will provide detailed instructions on how to use this feature.",source:"@site/docs/English/UserGuide/async_training_agentic.md",sourceDirName:"English/UserGuide",slug:"/English/UserGuide/async_training_agentic",permalink:"/ROLL/docs/English/UserGuide/async_training_agentic",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/async_training_agentic.md",tags:[],version:"current",lastUpdatedAt:1758773047,formattedLastUpdatedAt:"Sep 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"TOPR (Tapered Off-Policy REINFORCE)",permalink:"/ROLL/docs/English/UserGuide/algorithms/TOPR"},next:{title:"DeepSpeed Training Backend Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/deepspeed"}},l={},c=[{value:"Asynchronous Training Overview",id:"asynchronous-training-overview",level:2},{value:"Enabling Asynchronous Training",id:"enabling-asynchronous-training",level:2},{value:"Configuration Parameters",id:"configuration-parameters",level:3},{value:"Example Configuration",id:"example-configuration",level:3},{value:"How Asynchronous Training Works",id:"how-asynchronous-training-works",level:2},{value:"Usage Recommendations",id:"usage-recommendations",level:2}],g={toc:c},u="wrapper";function p({components:e,...n}){return(0,t.yg)(u,(0,r.A)({},g,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"agentic-asynchronous-training-feature-usage-guide"},"Agentic Asynchronous Training Feature Usage Guide"),(0,t.yg)("p",null,"The ROLL framework supports Agentic asynchronous training functionality, which can significantly improve training efficiency. This document will provide detailed instructions on how to use this feature."),(0,t.yg)("h2",{id:"asynchronous-training-overview"},"Asynchronous Training Overview"),(0,t.yg)("p",null,"In traditional synchronous training, the training and inference processes are executed sequentially, meaning that the next batch of inference cannot begin until a batch of inference is completed and rewards are collected. In asynchronous training, training and inference can be performed in parallel. The inference process can generate multiple batches of data in advance, and the training process can use these pre-generated data for learning."),(0,t.yg)("h2",{id:"enabling-asynchronous-training"},"Enabling Asynchronous Training"),(0,t.yg)("p",null,"To enable Agentic asynchronous training functionality, you need to set the ",(0,t.yg)("inlineCode",{parentName:"p"},"async_generation_ratio")," parameter in the configuration file."),(0,t.yg)("h3",{id:"configuration-parameters"},"Configuration Parameters"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"async_generation_ratio")," parameter is defined in ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/configs/base_config.py"),":"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-python"},'async_generation_ratio: float = field(\n    default=0,\n    metadata={\n        "help": "The ratio of ahead generation requests in pipeline, "\n        "0 means synchronous pipeline. currently only integer is supported."\n    },\n)\n')),(0,t.yg)("h3",{id:"example-configuration"},"Example Configuration"),(0,t.yg)("p",null,"The following is a complete asynchronous training configuration example (from ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-7B-agentic_megatron/agentic_val_webshop_async.yaml"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'# Enable asynchronous training\nasync_generation_ratio: 1\n\n# Other related configurations\nrollout_batch_size: 64\nval_batch_size: 64\nsequence_length: 8192\n\n# Training parameters\nmax_steps: 1024\nsave_steps: 10000\nlogging_steps: 1\neval_steps: 10\n\n# PPO parameters\nppo_epochs: 1\nadv_estimator: "grpo"\nwhiten_advantages: true\n\n# Model configuration\npretrain: Qwen/Qwen2.5-7B-Instruct\nreward_pretrain: Qwen/Qwen2.5-7B-Instruct\n\n# Role configurations\nactor_train:\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: false\n    dtype: bf16\n  training_args:\n    learning_rate: 1.0e-6\n    weight_decay: 0\n    per_device_train_batch_size: 1\n    gradient_accumulation_steps: 16\n    warmup_steps: 10\n  strategy_args:\n    strategy_name: megatron_train\n    strategy_config:\n      tensor_model_parallel_size: 1\n      context_parallel_size: 1\n      pipeline_model_parallel_size: 1\n      expert_model_parallel_size: 1\n      use_distributed_optimizer: true\n      recompute_granularity: full\n  device_mapping: list(range(0,4))\n  infer_batch_size: 1\n\nactor_infer:\n  model_args:\n    disable_gradient_checkpointing: true\n    dtype: bf16\n  generating_args:\n    max_new_tokens: 1024\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: 1\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      block_size: 16\n      load_format: auto\n  device_mapping: list(range(4,8))\n  infer_batch_size: 1\n')),(0,t.yg)("h2",{id:"how-asynchronous-training-works"},"How Asynchronous Training Works"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"When ",(0,t.yg)("inlineCode",{parentName:"li"},"async_generation_ratio")," is set to a value greater than 0, the framework will start asynchronous training mode"),(0,t.yg)("li",{parentName:"ol"},"The inference process will generate data ahead by ",(0,t.yg)("inlineCode",{parentName:"li"},"async_generation_ratio")," times the amount needed for training"),(0,t.yg)("li",{parentName:"ol"},"The training process can use these pre-generated data for learning without waiting for the current batch of inference to complete"),(0,t.yg)("li",{parentName:"ol"},"This parallelized processing can significantly improve training efficiency, especially when inference takes a long time")),(0,t.yg)("h2",{id:"usage-recommendations"},"Usage Recommendations"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Adjust the value of ",(0,t.yg)("inlineCode",{parentName:"li"},"async_generation_ratio")," based on hardware resources and task characteristics"),(0,t.yg)("li",{parentName:"ol"},"Ensure that training and inference roles are deployed separately"),(0,t.yg)("li",{parentName:"ol"},"Monitor resource usage during the training process to avoid resource bottlenecks"),(0,t.yg)("li",{parentName:"ol"},"Asynchronous generation will be paused during the validation phase and resumed after validation is complete")))}p.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>g,yg:()=>y});var r=a(6540);function t(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,r)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach(function(n){t(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function s(e,n){if(null==e)return{};var a,r,t=function(e,n){if(null==e)return{};var a,r,t={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||(t[a]=e[a]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var l=r.createContext({}),c=function(e){var n=r.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},g=function(e){var n=c(e.components);return r.createElement(l.Provider,{value:n},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef(function(e,n){var a=e.components,t=e.mdxType,i=e.originalType,l=e.parentName,g=s(e,["components","mdxType","originalType","parentName"]),u=c(a),d=t,y=u["".concat(l,".").concat(d)]||u[d]||p[d]||i;return a?r.createElement(y,o(o({ref:n},g),{},{components:a})):r.createElement(y,o({ref:n},g))});function y(e,n){var a=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=a.length,o=new Array(i);o[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:t,o[1]=s;for(var c=2;c<i;c++)o[c]=a[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}d.displayName="MDXCreateElement"}}]);