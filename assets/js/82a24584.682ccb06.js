"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[933],{3733:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var r=t(8168),a=(t(6540),t(5680));const i={},o="Quickstart: Multi Nodes Deployment Guide",l={unversionedId:"English/QuickStart/multi_nodes_quick_start_en",id:"English/QuickStart/multi_nodes_quick_start_en",title:"Quickstart: Multi Nodes Deployment Guide",description:"Environment Preparation",source:"@site/docs/English/QuickStart/multi_nodes_quick_start_en.md",sourceDirName:"English/QuickStart",slug:"/English/QuickStart/multi_nodes_quick_start_en",permalink:"/ROLL/docs/English/QuickStart/multi_nodes_quick_start_en",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/QuickStart/multi_nodes_quick_start_en.md",tags:[],version:"current",lastUpdatedAt:1755156639,formattedLastUpdatedAt:"Aug 14, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Experiment Data Analysis and Visualization",permalink:"/ROLL/docs/English/QuickStart/metrics_info_en"},next:{title:"Quickstart: Singel Node Deployment Guide",permalink:"/ROLL/docs/English/QuickStart/single_node_quick_start_en"}},s={},c=[{value:"Environment Preparation",id:"environment-preparation",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"pipeline\u8fd0\u884c",id:"pipeline\u8fd0\u884c",level:2},{value:"Reference: V100 Multi-GPU Memory Configuration Optimization",id:"reference-v100-multi-gpu-memory-configuration-optimization",level:2}],p={toc:c},u="wrapper";function m({components:e,...n}){return(0,a.yg)(u,(0,r.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"quickstart-multi-nodes-deployment-guide"},"Quickstart: Multi Nodes Deployment Guide"),(0,a.yg)("h2",{id:"environment-preparation"},"Environment Preparation"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Purchase multiple machines equipped with GPU and install GPU drivers simultaneously. One machine serves as the master node, and the others serve as worker nodes. (e.g., 2 machines with 2 GPUs each)."),(0,a.yg)("li",{parentName:"ol"},"Connect remotely to the GPU instance and access the machine terminal"),(0,a.yg)("li",{parentName:"ol"},"Install Docker environment and NVIDIA Container Toolkit on each machine")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"curl -fsSL https://github.com/alibaba/ROLL/blob/main/scripts/install_docker_nvidia_container_toolkit.sh  | sudo bash   \n")),(0,a.yg)("h2",{id:"environment-configuration"},"Environment Configuration"),(0,a.yg)("p",null,"Choose your desired image from the ",(0,a.yg)("a",{parentName:"p",href:"https://alibaba.github.io/ROLL/docs/English/QuickStart/image_address"},"image addresses"),". The following example will use ",(0,a.yg)("em",{parentName:"p"},"torch2.6.0 + vLLM0.8.4"),"."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"# 1. Start a Docker container with GPU support, expose the port, and keep the container running.\nsudo docker run -dit \\\n  --gpus all \\\n  -p 9001:22 \\\n  --ipc=host \\\n  --shm-size=10gb \\\n  roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch260-vllm084 \\\n  /bin/bash\n\n# 2. Enter the Docker container\n#    You can find your running container's ID or name using `sudo docker ps`.\nsudo docker exec -it <container_id> /bin/bash\n\n# 3. Verify GPU visibility\nnvidia-smi\n\n# 4. Clone the project repo\ngit clone https://github.com/alibaba/ROLL.git\n\n# 5. Install dependencies (select the requirements file corresponding to your chosen image)\ncd ROLL\npip install -r requirements_torch260_vllm.txt -i https://mirrors.aliyun.com/pypi/simple/\n")),(0,a.yg)("h2",{id:"pipeline\u8fd0\u884c"},"pipeline\u8fd0\u884c"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Set environment variables on the master node:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},'export MASTER_ADDR="ip of master node"\nexport MASTER_PORT="port of master node"  # Default: 6379\nexport WORLD_SIZE=2\nexport RANK=0\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\n')),(0,a.yg)("p",null,"Notes:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"MASTER_ADDR")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"MASTER_PORT")," define the communication endpoint for the distributed cluster."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"WORLD_SIZE")," specifies the total number of nodes in the cluster (e.g., 2 nodes)."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"RANK")," identifies the node's role (0 for master, 1\u30012\u30013 etc. for worker nodes)."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"NCCL_SOCKET_IFNAME")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"GLOO_SOCKET_IFNAME")," specify the network interface for GPU/cluster communication (typically eth0).")),(0,a.yg)("ol",{start:2},(0,a.yg)("li",{parentName:"ol"},"Run the pipeline on the master node:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"bash examples/agentic_demo/run_agentic_pipeline_frozen_lake_multi_nodes_demo.sh\n")),(0,a.yg)("p",null,"After the Ray cluster starts, you will see log examples like:\n",(0,a.yg)("img",{alt:"log_ray_multi_nodes",src:t(3746).A,width:"2758",height:"204"})),(0,a.yg)("ol",{start:3},(0,a.yg)("li",{parentName:"ol"},"Set environment variables on the worker node:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},'export MASTER_ADDR="ip of master node"\nexport MASTER_PORT="port of master node" # Default: 6379\nexport WORLD_SIZE=2\nexport RANK=1\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\n')),(0,a.yg)("ol",{start:4},(0,a.yg)("li",{parentName:"ol"},"Connect the worker node to the Ray cluster on the master node:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"ray start --address='ip of master node:port of master node' --num-gpus=2\n")),(0,a.yg)("h2",{id:"reference-v100-multi-gpu-memory-configuration-optimization"},"Reference: V100 Multi-GPU Memory Configuration Optimization"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"# Reduce the system's expected number of GPUs from 8 to your actual 2 V100\nnum_gpus_per_node: 2\n# Training processes are now mapped to GPU 0-3\nactor_train.device_mapping: list(range(0,4))\n# Inference processes are now mapped only to GPU 0-3\nactor_infer.device_mapping: list(range(0,4))\n# Reference model processes are now mapped to GPU 0-3\nreference.device_mapping: list(range(0,4))\n\n# Significantly reduce the batch sizes for Rollout and Validation stages to prevent out-of-memory errors on a single GPU\nrollout_batch_size: 64\nval_batch_size: 16\n\n# V100 has better native support for FP16 than BF16 (unlike A100/H100). Switching to FP16 improves compatibility and stability, while also saving GPU memory.\nactor_train.model_args.dtype: fp16\nactor_infer.model_args.dtype: fp16\nreference.model_args.dtype: fp16\n\n# Switch the large model training framework from DeepSpeed to Megatron-LM. Parameters can be sent in batches, resulting in faster execution.\nstrategy_name: megatron_train\nstrategy_config:\n  tensor_model_parallel_size: 1\n  pipeline_model_parallel_size: 1\n  expert_model_parallel_size: 1\n  use_distributed_optimizer: true\n  recompute_granularity: full\n\n# In megatron training the global train batch size is equivalent to per_device_train_batch_size * gradient_accumulation_steps * world_size, and here world_size is 4\nactor_train.training_args.per_device_train_batch_size: 1\nactor_train.training_args.gradient_accumulation_steps: 16  \n\n# Reduce the maximum number of actions per trajectory, making each Rollout trajectory shorter that reduces the length of LLM-generated content.\nmax_actions_per_traj: 10    \n\n# Reduce the number of parallel training and validation environment groups to accommodate single-GPU resources.\ntrain_env_manager.env_groups: 1\ntrain_env_manager.n_groups: 1\nval_env_manager.env_groups: 2\nval_env_manager.n_groups: [1, 1]\nval_env_manager.tags: [SimpleSokoban, FrozenLake]\n\n# Reduce the total number of training steps for quicker full pipeline runs, useful for rapid debugging.\nmax_steps: 100\n")))}m.isMDXComponent=!0},3746:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_ray_multi_nodes-1caa4dc6e157ccf6d8f9542480af8b93.png"},5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>g});var r=t(6540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach(function(n){a(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef(function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),d=a,g=u["".concat(s,".").concat(d)]||u[d]||m[d]||i;return t?r.createElement(g,o(o({ref:n},p),{},{components:t})):r.createElement(g,o({ref:n},p))});function g(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"}}]);