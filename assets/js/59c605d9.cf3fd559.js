"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[2182],{28453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>l});var r=i(96540);const t={},a=r.createContext(t);function o(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),r.createElement(a.Provider,{value:e},n.children)}},30111:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Configuration/lora","title":"LoRA Fine-tuning Configuration Guide","description":"LoRA (Low-Rank Adaptation) is an efficient parameter-efficient fine-tuning method that achieves parameter-efficient fine-tuning by adding low-rank matrices to pre-trained models. This document will provide detailed instructions on how to configure and use LoRA fine-tuning in the ROLL framework.","source":"@site/docs/User Guides/Configuration/lora.md","sourceDirName":"User Guides/Configuration","slug":"/User Guides/Configuration/lora","permalink":"/ROLL/docs/User Guides/Configuration/lora","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Configuration/lora.md","tags":[],"version":"current","lastUpdatedAt":1764639784000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"FP8 Quantization Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/fp8_rollout"},"next":{"title":"Megatron Inference and Training Backend Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/megatron"}}');var t=i(74848),a=i(28453);const o={},l="LoRA Fine-tuning Configuration Guide",s={},c=[{value:"LoRA Introduction",id:"lora-introduction",level:2},{value:"Configuring LoRA Fine-tuning",id:"configuring-lora-fine-tuning",level:2},{value:"Configuration Example",id:"configuration-example",level:3},{value:"Configuration Parameter Details",id:"configuration-parameter-details",level:3},{value:"LoRA Compatibility with Training Backends",id:"lora-compatibility-with-training-backends",level:2},{value:"Performance Optimization Recommendations",id:"performance-optimization-recommendations",level:2},{value:"Notes",id:"notes",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"lora-fine-tuning-configuration-guide",children:"LoRA Fine-tuning Configuration Guide"})}),"\n",(0,t.jsx)(e.p,{children:"LoRA (Low-Rank Adaptation) is an efficient parameter-efficient fine-tuning method that achieves parameter-efficient fine-tuning by adding low-rank matrices to pre-trained models. This document will provide detailed instructions on how to configure and use LoRA fine-tuning in the ROLL framework."}),"\n",(0,t.jsx)(e.h2,{id:"lora-introduction",children:"LoRA Introduction"}),"\n",(0,t.jsx)(e.p,{children:"LoRA achieves parameter-efficient fine-tuning through the following approaches:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Low-Rank Matrix Decomposition"}),": Decompose weight update matrices into the product of two low-rank matrices"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Parameter Efficiency"}),": Train only a small number of additional parameters instead of all model parameters"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Easy Deployment"}),": Fine-tuned models can be easily merged into the original model"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"configuring-lora-fine-tuning",children:"Configuring LoRA Fine-tuning"}),"\n",(0,t.jsx)(e.p,{children:"In the ROLL framework, LoRA fine-tuning can be configured by setting relevant parameters in the YAML configuration file."}),"\n",(0,t.jsx)(e.h3,{id:"configuration-example",children:"Configuration Example"}),"\n",(0,t.jsxs)(e.p,{children:["The following is a typical LoRA configuration example (from ",(0,t.jsx)(e.code,{children:"examples/qwen2.5-7B-rlvr_megatron/rlvl_lora_zero3.yaml"}),"):"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:"# LoRA global configuration\nlora_target: o_proj,q_proj,k_proj,v_proj\nlora_rank: 32\nlora_alpha: 32\n\nactor_train:\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: true\n    dtype: bf16\n    lora_target: ${lora_target}\n    lora_rank: ${lora_rank}\n    lora_alpha: ${lora_alpha}\n    model_type: ~\n  training_args:\n    learning_rate: 1.0e-5\n    weight_decay: 0\n    per_device_train_batch_size: 1\n    gradient_accumulation_steps: 32\n    warmup_steps: 20\n    num_train_epochs: 50\n  strategy_args:\n    strategy_name: deepspeed_train\n    strategy_config: ${deepspeed_zero3}\n  device_mapping: list(range(0,16))\n  infer_batch_size: 4\n\nactor_infer:\n  model_args:\n    attn_implementation: fa2\n    disable_gradient_checkpointing: true\n    dtype: bf16\n    lora_target: ${lora_target}\n    lora_rank: ${lora_rank}\n    lora_alpha: ${lora_alpha}\n  generating_args:\n    max_new_tokens: ${response_length}\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: ${num_return_sequences_in_group}\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      gpu_memory_utilization: 0.6\n      enforce_eager: false\n      block_size: 16\n      max_model_len: 8000\n  device_mapping: list(range(0,12))\n  infer_batch_size: 1\n"})}),"\n",(0,t.jsx)(e.h3,{id:"configuration-parameter-details",children:"Configuration Parameter Details"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"lora_target"}),": Specify the model layers to apply LoRA"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["For example: ",(0,t.jsx)(e.code,{children:"o_proj,q_proj,k_proj,v_proj"})," means applying LoRA to the output projection and query, key, value projection layers in the attention mechanism"]}),"\n",(0,t.jsx)(e.li,{children:"Can be adjusted according to the specific model structure"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"lora_rank"}),": Rank of the LoRA matrix"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Controls the size of the LoRA matrix"}),"\n",(0,t.jsx)(e.li,{children:"Smaller ranks can reduce the number of parameters but may affect performance"}),"\n",(0,t.jsx)(e.li,{children:"Usually set to 8, 16, 32, 64, etc."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"lora_alpha"}),": LoRA scaling factor"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Controls the magnitude of LoRA updates"}),"\n",(0,t.jsxs)(e.li,{children:["Usually set to the same as ",(0,t.jsx)(e.code,{children:"lora_rank"})," or its multiple"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"LoRA Parameters in model_args"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"lora_target"}),": Specify the layers to apply LoRA"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"lora_rank"}),": Rank of the LoRA matrix"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"lora_alpha"}),": LoRA scaling factor"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"lora-compatibility-with-training-backends",children:"LoRA Compatibility with Training Backends"}),"\n",(0,t.jsx)(e.p,{children:"Currently, LoRA fine-tuning only supports the DeepSpeed training backend:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-yaml",children:"actor_train:\n  strategy_args:\n    strategy_name: deepspeed_train  # LoRA only supports deepspeed_train\n"})}),"\n",(0,t.jsx)(e.p,{children:"This is because DeepSpeed provides optimization features that integrate well with LoRA."}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization-recommendations",children:"Performance Optimization Recommendations"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Selecting Appropriate LoRA Layers"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Applying LoRA to attention mechanism-related layers usually works well"}),"\n",(0,t.jsx)(e.li,{children:"The best LoRA layer combination can be determined through experimentation"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Adjusting LoRA Parameters"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"lora_rank"}),": Adjust according to model size and task complexity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"lora_alpha"}),": Usually set to ",(0,t.jsx)(e.code,{children:"lora_rank"})," or its multiple"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Learning Rate Setting"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"LoRA fine-tuning usually requires a higher learning rate"}),"\n",(0,t.jsxs)(e.li,{children:["Set to ",(0,t.jsx)(e.code,{children:"1.0e-5"})," in the example"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"notes",children:"Notes"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"LoRA fine-tuning currently only supports the DeepSpeed training backend"}),"\n",(0,t.jsx)(e.li,{children:"Ensure the model supports LoRA fine-tuning"}),"\n",(0,t.jsx)(e.li,{children:"Pay attention to compatibility with LoRA when using gradient checkpointing"}),"\n",(0,t.jsx)(e.li,{children:"LoRA fine-tuning performance may differ from full parameter fine-tuning and needs to be evaluated according to specific tasks"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"By properly configuring LoRA fine-tuning, you can significantly reduce the number of training parameters and computational resource consumption while maintaining model performance."})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);