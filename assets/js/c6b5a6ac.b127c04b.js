"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[7766],{4097:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>g,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Configuration/sglang","title":"SGLang Inference Backend Configuration Guide","description":"SGLang is a fast and easy-to-use inference engine, particularly suitable for inference tasks of large-scale language models. This document will provide detailed instructions on how to configure and use the SGLang inference backend in the ROLL framework.","source":"@site/docs/User Guides/Configuration/sglang.md","sourceDirName":"User Guides/Configuration","slug":"/User Guides/Configuration/sglang","permalink":"/ROLL/docs/User Guides/Configuration/sglang","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Configuration/sglang.md","tags":[],"version":"current","lastUpdatedAt":1764639784000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Off-Policy Algorithms Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/offpolicy_setting"},"next":{"title":"vLLM Inference Backend Configuration Guide","permalink":"/ROLL/docs/User Guides/Configuration/vllm"}}');var s=i(74848),a=i(28453);const o={},t="SGLang Inference Backend Configuration Guide",l={},c=[{value:"SGLang Introduction",id:"sglang-introduction",level:2},{value:"Configuring SGLang Strategy",id:"configuring-sglang-strategy",level:2},{value:"Basic Configuration Example",id:"basic-configuration-example",level:3},{value:"Configuration Parameter Details",id:"configuration-parameter-details",level:3},{value:"Integration with Other Components",id:"integration-with-other-components",level:2},{value:"Performance Optimization Recommendations",id:"performance-optimization-recommendations",level:2},{value:"Notes",id:"notes",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sglang-inference-backend-configuration-guide",children:"SGLang Inference Backend Configuration Guide"})}),"\n",(0,s.jsx)(n.p,{children:"SGLang is a fast and easy-to-use inference engine, particularly suitable for inference tasks of large-scale language models. This document will provide detailed instructions on how to configure and use the SGLang inference backend in the ROLL framework."}),"\n",(0,s.jsx)(n.h2,{id:"sglang-introduction",children:"SGLang Introduction"}),"\n",(0,s.jsx)(n.p,{children:"SGLang is a structured generation language specifically designed for inference of large language models. It provides efficient inference performance and flexible programming interfaces."}),"\n",(0,s.jsx)(n.h2,{id:"configuring-sglang-strategy",children:"Configuring SGLang Strategy"}),"\n",(0,s.jsxs)(n.p,{children:["In the ROLL framework, SGLang inference strategy can be configured by setting ",(0,s.jsx)(n.code,{children:"strategy_args"})," in the YAML configuration file."]}),"\n",(0,s.jsx)(n.h3,{id:"basic-configuration-example",children:"Basic Configuration Example"}),"\n",(0,s.jsxs)(n.p,{children:["The following is a typical SGLang configuration example (from ",(0,s.jsx)(n.code,{children:"examples/qwen3-30BA3B-rlvr_megatron/rlvr_config_sglang.yaml"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"actor_infer:\n  model_args:\n    disable_gradient_checkpointing: true\n    dtype: bf16\n  generating_args:\n    max_new_tokens: ${response_length}\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: ${num_return_sequences_in_group}\n  strategy_args:\n    strategy_name: sglang\n    strategy_config:\n      mem_fraction_static: 0.7\n      load_format: dummy\n  num_gpus_per_worker: 2\n  device_mapping: list(range(0,24))\n"})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-parameter-details",children:"Configuration Parameter Details"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"strategy_name"}),": Set to ",(0,s.jsx)(n.code,{children:"sglang"})," to use the SGLang inference backend"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"strategy_config"}),": SGLang-specific configuration parameters. For more SGLang configuration parameters, see the ",(0,s.jsx)(n.a,{href:"https://docs.sglang.ai/",children:"official documentation"}),". The strategy_config is passed through directly to SGLang."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"mem_fraction_static"}),": GPU memory utilization ratio for static memory such as model weights and KV cache","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Increase this value if KV cache building fails"}),"\n",(0,s.jsx)(n.li,{children:"Decrease this value if CUDA memory is insufficient"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"load_format"}),": Format for loading model weights","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:['Since the model will be "updated" at the beginning, this value can be set to ',(0,s.jsx)(n.code,{children:"dummy"})]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"num_gpus_per_worker"}),": Number of GPUs allocated per worker"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"SGLang can utilize multiple GPUs for parallel inference"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"device_mapping"}),": Specify the list of GPU device IDs to use"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"infer_batch_size"}),": Batch size during inference"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-other-components",children:"Integration with Other Components"}),"\n",(0,s.jsx)(n.p,{children:"In the above example, we can see:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"actor_infer"})," uses SGLang as the inference backend"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"actor_train"})," uses Megatron for training"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"reference"})," uses Megatron for inference"]}),"\n",(0,s.jsxs)(n.li,{children:["Reward models use different inference backends (such as ",(0,s.jsx)(n.code,{children:"hf_infer"}),")"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This design allows different components to choose the most suitable inference engine according to their needs."}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization-recommendations",children:"Performance Optimization Recommendations"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Memory Management"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Properly set the ",(0,s.jsx)(n.code,{children:"mem_fraction_static"})," parameter to balance performance and memory usage"]}),"\n",(0,s.jsx)(n.li,{children:"Monitor GPU memory usage to avoid memory overflow"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parallel Processing"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Appropriately increase ",(0,s.jsx)(n.code,{children:"num_gpus_per_worker"})," to utilize multiple GPUs for model loading and parallel inference"]}),"\n",(0,s.jsxs)(n.li,{children:["Adjust ",(0,s.jsx)(n.code,{children:"device_mapping"})," according to hardware configuration. The number of SGLang engines is ",(0,s.jsx)(n.code,{children:"len(device_mapping) // num_gpus_per_worker"})]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Batch Processing Optimization"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"infer_batch_size"})," is not effective, as continuous batching is automatically performed"]}),"\n",(0,s.jsx)(n.li,{children:"Consider the impact of sequence length on batch size"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"SGLang requires specific versions of dependency libraries, please ensure compatible versions are installed"}),"\n",(0,s.jsx)(n.li,{children:"In resource-constrained environments, carefully balance resource allocation among different components"}),"\n",(0,s.jsx)(n.li,{children:"Integration of SGLang with training frameworks like Megatron may require additional configuration"}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var r=i(96540);const s={},a=r.createContext(s);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);