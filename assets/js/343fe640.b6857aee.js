"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[3540],{94672(e,n,r){r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Development/Developer Guide/llm_as_judge_optimization","title":"LLM as Judge Optimization in Agentic Environments","description":"This document describes the optimized implementation of LLM as Judge in Agentic environments within the ROLL framework, including system architecture, call chains, configuration methods, and best practices.","source":"@site/docs/Development/Developer Guide/llm_as_judge_optimization.md","sourceDirName":"Development/Developer Guide","slug":"/Development/Developer Guide/llm_as_judge_optimization","permalink":"/ROLL/docs/Development/Developer Guide/llm_as_judge_optimization","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/Development/Developer Guide/llm_as_judge_optimization.md","tags":[],"version":"current","lastUpdatedAt":1770642330000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Customer Env","permalink":"/ROLL/docs/Development/Developer Guide/customer_env"},"next":{"title":"Prompt Generation Guide","permalink":"/ROLL/docs/Development/Developer Guide/prompt_intro"}}');var t=r(74848),l=r(28453);const s={},a="LLM as Judge Optimization in Agentic Environments",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Key Advantages",id:"key-advantages",level:3},{value:"Application Scenarios",id:"application-scenarios",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"Overall Architecture",id:"overall-architecture",level:3},{value:"Key Components",id:"key-components",level:3},{value:"1. Reward Cluster",id:"1-reward-cluster",level:4},{value:"2. Reward Scheduler (Ray Named Actor)",id:"2-reward-scheduler-ray-named-actor",level:4},{value:"3. Reward Proxy",id:"3-reward-proxy",level:4},{value:"4. Unified Utility Function <code>generate_by_proxy</code>",id:"4-unified-utility-function-generate_by_proxy",level:4},{value:"Call Chain",id:"call-chain",level:2},{value:"Complete Call Flow",id:"complete-call-flow",level:3},{value:"Configuration Guide",id:"configuration-guide",level:2},{value:"Complete Configuration Example",id:"complete-configuration-example",level:3},{value:"Configuration Key Points",id:"configuration-key-points",level:3},{value:"1. device_mapping (Required)",id:"1-device_mapping-required",level:4},{value:"2. strategy_name (Inference Backend Selection)",id:"2-strategy_name-inference-backend-selection",level:4},{value:"3. generating_args (Generation Parameters)",id:"3-generating_args-generation-parameters",level:4},{value:"Summary",id:"summary",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"llm-as-judge-optimization-in-agentic-environments",children:"LLM as Judge Optimization in Agentic Environments"})}),"\n",(0,t.jsx)(n.p,{children:"This document describes the optimized implementation of LLM as Judge in Agentic environments within the ROLL framework, including system architecture, call chains, configuration methods, and best practices."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"LLM as Judge is a method that uses large language models as evaluators to assess agent response quality. In Agentic training scenarios, when large-scale environment instances perform concurrent rollouts, using LLM as Judge to compute rewards generates massive concurrent LLM requests, which poses significant challenges to the stability and throughput of external LLM services."}),"\n",(0,t.jsxs)(n.p,{children:["To address this challenge, the ROLL framework implements a scalable ",(0,t.jsx)(n.strong,{children:"localized parallel evaluation system"})," through an ",(0,t.jsx)(n.strong,{children:"independent Reward Cluster"})," and ",(0,t.jsx)(n.strong,{children:"efficient scheduling mechanisms"}),", avoiding dependency on external services and ensuring the stability and controllability of the training process."]}),"\n",(0,t.jsx)(n.admonition,{title:"Documentation Scope",type:"info",children:(0,t.jsxs)(n.p,{children:["This document uses the ",(0,t.jsx)(n.strong,{children:"DeepEyes environment's"})," LLM as Judge implementation as an example. For other environments that need LLM as Judge, you can refer to the calling patterns in ",(0,t.jsx)(n.code,{children:"env_manager"})," and ",(0,t.jsx)(n.code,{children:"env"})," to implement your own custom solutions."]})}),"\n",(0,t.jsx)(n.h3,{id:"key-advantages",children:"Key Advantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Independent Resource Management"}),": Reward model is separated from Policy model, allowing independent GPU resource allocation and avoiding resource contention"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Localized Deployment"}),": Avoid external API dependencies through local Reward Cluster, ensuring service stability and data security"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High Concurrency Support"}),": Efficient parallel reward evaluation through RequestScheduler, supporting scalable environment concurrency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Interface Design"}),": Provides ",(0,t.jsx)(n.code,{children:"generate_by_proxy"})," unified utility function, simplifying LLM calls and supporting both text and multimodal inputs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible Configuration"}),": Supports multiple inference backends (vLLM, SGLang) and custom generation parameters"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"application-scenarios",children:"Application Scenarios"}),"\n",(0,t.jsx)(n.p,{children:"Typical Agentic training scenarios:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Environment Scale"}),": 256 environment groups with 4 environments each, totaling 1024 concurrent environment instances"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rollout Frequency"}),": Each environment calls LLM Judge after completing an episode"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Concurrency Pressure"}),": During rollout peaks, 500+ environments may simultaneously request reward evaluation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Stability Requirements"}),": Training process cannot be interrupted by external API rate limiting or timeouts"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The optimized implementation described in this document effectively addresses these challenges."}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"overall-architecture",children:"Overall Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"AgenticPipeline\n    \u251c\u2500\u2500 Reward Cluster (optional, independent GPU resources)\n    \u2502   \u251c\u2500\u2500 InferWorker (default)\n    \u2502   \u2514\u2500\u2500 Supports vLLM/SGLang backends\n    \u2502\n    \u251c\u2500\u2500 Reward Scheduler (Ray Named Actor)\n    \u2502   \u251c\u2500\u2500 Request routing and load balancing\n    \u2502   \u251c\u2500\u2500 Concurrency control\n    \u2502   \u2514\u2500\u2500 Request tracking and cleanup\n    \u2502\n    \u2514\u2500\u2500 Environment Manager\n        \u251c\u2500\u2500 llm_proxy: for policy inference\n        \u251c\u2500\u2500 reward_proxy: for LLM as Judge\n        \u2514\u2500\u2500 env instances\n            \u2514\u2500\u2500 Call reward_proxy in obtain_outcome_reward\n"})}),"\n",(0,t.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,t.jsx)(n.h4,{id:"1-reward-cluster",children:"1. Reward Cluster"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Location"}),": ",(0,t.jsx)(n.code,{children:"roll/pipeline/agentic/agentic_pipeline.py:88-98"})]}),"\n",(0,t.jsxs)(n.p,{children:["Reward Cluster is an optional component, created only when ",(0,t.jsx)(n.code,{children:"device_mapping"})," is configured:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"self.reward = None\nif (self.pipeline_config.reward is not None and\n    len(self.pipeline_config.reward.device_mapping) > 0):\n    self.reward = Cluster(\n        name=self.pipeline_config.reward.name,\n        worker_cls=self.pipeline_config.reward.worker_cls,  # Default: InferWorker\n        resource_manager=self.resource_manager,\n        worker_config=self.pipeline_config.reward,\n    )\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Worker Class Default Configuration"}),": ",(0,t.jsx)(n.code,{children:"roll/pipeline/agentic/agentic_config.py:287"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Defaults to ",(0,t.jsx)(n.code,{children:"InferWorker"})," as inference engine, reusing ActorInfer Worker implementation"]}),"\n",(0,t.jsx)(n.li,{children:"Supports multiple backends including vLLM and SGLang"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"2-reward-scheduler-ray-named-actor",children:"2. Reward Scheduler (Ray Named Actor)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Location"}),": ",(0,t.jsx)(n.code,{children:"roll/pipeline/agentic/agentic_pipeline.py:112-125"})]}),"\n",(0,t.jsx)(n.p,{children:"Reward Scheduler is created as a Ray Named Actor for shared access by all environment managers:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'self.reward_scheduler = RequestScheduler.options(\n    name=f"RewardScheduler-{self.pipeline_config.reward.name}",\n    get_if_exists=True,\n    namespace=RAY_NAMESPACE,\n    scheduling_strategy=NodeAffinitySchedulingStrategy(...)\n).remote(\n    infer_cluster=self.reward,\n    pipeline_config=self.pipeline_config,\n    resource_manager=self.resource_manager,\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Core Functionality"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Smart Routing"}),": Uses least-loaded routing algorithm to distribute requests to different DP ranks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sticky Routing"}),": Requests from the same environment are routed to the same worker (beneficial for KV cache)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Request Tracking"}),": Maintains mapping from ",(0,t.jsx)(n.code,{children:"request_id"})," to workers"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"3-reward-proxy",children:"3. Reward Proxy"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Location"}),": ",(0,t.jsx)(n.code,{children:"roll/pipeline/agentic/env_manager/vl_traj_env_manager.py:85-109"})]}),"\n",(0,t.jsx)(n.p,{children:"Environment manager retrieves Reward Scheduler via Ray and creates Reward Proxy:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Get reward scheduler from Ray (Named Actor)\nif self.pipeline_config.reward:\n    self.reward_scheduler = ray.get_actor(\n        name=f"RewardScheduler-{pipeline_config.reward.name}",\n        namespace=RAY_NAMESPACE\n    )\n\n    # Create reward proxy\n    self.reward_proxy = create_llm_proxy(\n        generate_scheduler=self.reward_scheduler,\n        llm_proxy_config=pipeline_config.reward.llm_proxy,\n        tokenizer=self.reward_tokenizer,\n        env=None,\n    )\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Proxy Factory Function"}),": ",(0,t.jsx)(n.code,{children:"roll/pipeline/agentic/llm_proxy/__init__.py:11"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Supports multiple proxy types: ",(0,t.jsx)(n.code,{children:"policy"}),", ",(0,t.jsx)(n.code,{children:"openai"}),", ",(0,t.jsx)(n.code,{children:"random"})]}),"\n",(0,t.jsx)(n.li,{children:"Extensible through registration mechanism"}),"\n",(0,t.jsx)(n.li,{children:"Policy proxy has been validated in training; for externally deployed LLM services, use openai proxy (note concurrency challenges)"}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"4-unified-utility-function-generate_by_proxy",children:["4. Unified Utility Function ",(0,t.jsx)(n.code,{children:"generate_by_proxy"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Location"}),": ",(0,t.jsx)(n.code,{children:"roll/pipeline/agentic/llm_proxy/proxy_utils.py:18-170"})]}),"\n",(0,t.jsx)(n.p,{children:"This is the core component called by environments, providing a unified LLM calling interface:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def generate_by_proxy(\n    messages: List[Dict[str, Any]],\n    tokenizer: PreTrainedTokenizer,\n    proxy: BaseLLMProxy,\n    enable_thinking: bool = False,\n    generation_config: Optional[Dict[str, Any]] = None,\n    collator: Optional[Any] = None,\n    mm_data: Optional[Dict[str, Any]] = None,\n    src_rank: Optional[int] = None,\n) -> Optional[str]\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Core Features"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Interface"}),": Same calling pattern for both text and multimodal inputs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automatic Formatting"}),": Uses ",(0,t.jsx)(n.code,{children:"tokenizer.apply_chat_template"})," to format messages"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal Support"}),": Supports image/video inputs through ",(0,t.jsx)(n.code,{children:"collator"})," parameter"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Thinking Mechanism"}),": Supports chain-of-thought for models like DeepSeek and Qwen"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Routing Control"}),": Implements sticky routing through ",(0,t.jsx)(n.code,{children:"src_rank"})," parameter"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"}),": Returns ",(0,t.jsx)(n.code,{children:"None"})," to indicate inference failure, handled by caller"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"call-chain",children:"Call Chain"}),"\n",(0,t.jsx)(n.h3,{id:"complete-call-flow",children:"Complete Call Flow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"1. DeepEyesEnv.step() (env/deepeyes/env.py:182-197)\n   Triggers obtain_outcome_reward when done=True\n   \u2193\n2. DeepEyesEnv.obtain_outcome_reward() (env/deepeyes/env.py:199-254)\n   Builds judge prompt, calls reward model\n   \u2193\n3. generate_by_proxy() (llm_proxy/proxy_utils.py:18)\n   Unified LLM calling utility function\n   \u2193\n4. reward_proxy.generate() (llm_proxy/policy_proxy.py:15)\n   Calls scheduler via Ray\n   \u2193\n5. reward_scheduler.generate_one_request() (scheduler/generate_scheduler.py:1296)\n   Request routing and load balancing\n   \u2193\n6. infer_cluster.workers[dp_rank].generate_request()\n   Actual model inference\n   \u2193\n7. Returns LLM judgment result\n"})}),"\n",(0,t.jsx)(n.h2,{id:"configuration-guide",children:"Configuration Guide"}),"\n",(0,t.jsx)(n.h3,{id:"complete-configuration-example",children:"Complete Configuration Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Reward Configuration (LLM as Judge for AgenticPipeline)\nreward:\n  name: "reward"\n  worker_cls: "roll.pipeline.base_worker.InferWorker"  # Default value, can be omitted\n  model_args:\n    model_name_or_path: Qwen/Qwen2.5-72B-Instruct\n    dtype: bf16\n  generating_args:\n    max_new_tokens: 2048\n    temperature: 0.2      # Lower temperature for stable judgments\n    top_p: 0.95\n    top_k: 20\n  strategy_args:\n    strategy_name: vllm   # or sglang\n    strategy_config:\n      gpu_memory_utilization: 0.8\n      tensor_parallel_size: 4\n      load_format: auto\n  # Critical: Must be non-empty to create reward cluster\n  device_mapping: list(range(8, 16))  # GPUs 8-15\n  llm_proxy:\n    proxy_type: policy  # Use policy proxy\n'})}),"\n",(0,t.jsx)(n.h3,{id:"configuration-key-points",children:"Configuration Key Points"}),"\n",(0,t.jsx)(n.h4,{id:"1-device_mapping-required",children:"1. device_mapping (Required)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Recommended: Policy and Reward use independent GPUs\nactor_infer:\n  device_mapping: list(range(0, 8))   # GPUs 0-7\n\nreward:\n  device_mapping: list(range(8, 16))  # GPUs 8-15, independent resources\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Empty or None"}),": Reward cluster not created, environments cannot use LLM as Judge"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Non-empty"}),": Creates independent reward cluster, enables LLM as Judge"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Independent Deployment"}),": Use different GPU resources from actor_infer. Policy inference and Reward evaluation run in parallel. actor_infer and reward must be deployed independently"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"2-strategy_name-inference-backend-selection",children:"2. strategy_name (Inference Backend Selection)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"strategy_args:\n  strategy_name: vllm   # or sglang\n  strategy_config:\n    gpu_memory_utilization: 0.8\n    tensor_parallel_size: 4\n    load_format: auto  # Must configure auto; vllm/sglang strategies default to dummy load which randomly initializes parameters\n"})}),"\n",(0,t.jsx)(n.h4,{id:"3-generating_args-generation-parameters",children:"3. generating_args (Generation Parameters)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"generating_args:\n  max_new_tokens: 2048    # Adjust based on judge output length\n  temperature: 0.2        # Lower temperature for stability\n  top_p: 0.95\n  top_k: 20\n"})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"The optimized LLM as Judge implementation in Agentic environments achieves efficient scalability through the following key designs:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Independent Reward Cluster"}),": Resource isolation, avoiding competition with Policy inference"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ray Named Actor"}),": Reward Scheduler as a shared service, accessible by all environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unified Utility Function"}),": ",(0,t.jsx)(n.code,{children:"generate_by_proxy"})," simplifies calls, supports text and multimodal"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Smart Routing"}),": Sticky routing and load balancing, improving cache utilization"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By properly configuring and using these components, you can build an efficient and reliable LLM as Judge evaluation system."})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453(e,n,r){r.d(n,{R:()=>s,x:()=>a});var i=r(96540);const t={},l=i.createContext(t);function s(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(l.Provider,{value:n},e.children)}}}]);