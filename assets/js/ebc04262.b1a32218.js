"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[8045],{574:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=t(8168),i=(t(6540),t(5680));const r={},o="Custom Environment",s={unversionedId:"English/DevelopmentGuide/custom_env",id:"English/DevelopmentGuide/custom_env",title:"Custom Environment",description:"Reinforcement Learning Environment",source:"@site/docs/English/DevelopmentGuide/custom_env.md",sourceDirName:"English/DevelopmentGuide",slug:"/English/DevelopmentGuide/custom_env",permalink:"/ROLL/docs/English/DevelopmentGuide/custom_env",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/DevelopmentGuide/custom_env.md",tags:[],version:"current",lastUpdatedAt:1755444305,formattedLastUpdatedAt:"Aug 17, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How to Add Support for a New Model",permalink:"/ROLL/docs/English/DevelopmentGuide/support_new_models_en"},next:{title:"Customer Env",permalink:"/ROLL/docs/English/DevelopmentGuide/customer_env_en"}},l={},c=[{value:"Reinforcement Learning Environment",id:"reinforcement-learning-environment",level:2},{value:"Core Functions",id:"core-functions",level:2},{value:"Code Examples",id:"code-examples",level:2},{value:"Sokoban Environment: Classic Puzzle Task with Discrete Actions",id:"sokoban-environment-classic-puzzle-task-with-discrete-actions",level:3},{value:"WebShop Environment: Complex Interaction Task Driven by Natural Language",id:"webshop-environment-complex-interaction-task-driven-by-natural-language",level:3},{value:"Creating Custom Environments",id:"creating-custom-environments",level:2},{value:"Step Overview",id:"step-overview",level:3},{value:"Design Recommendations",id:"design-recommendations",level:3}],m={toc:c},d="wrapper";function p({components:e,...n}){return(0,i.yg)(d,(0,a.A)({},m,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"custom-environment"},"Custom Environment"),(0,i.yg)("h2",{id:"reinforcement-learning-environment"},"Reinforcement Learning Environment"),(0,i.yg)("p",null,"In reinforcement learning, the environment is the world where the agent interacts. It defines the states (State) that the agent can perceive, the actions (Action) that the agent can perform, and the rewards (Reward) that the agent receives after each interaction. The environment is responsible for simulating real-world dynamics, updating states based on agent actions, and providing feedback."),(0,i.yg)("p",null,"To help you quickly get started and understand the adaptability and performance of our ROLL framework's Agentic Pipeline in different task scenarios, we have specially provided two core example environments:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Traditional RL environments based on discrete actions (inheriting from BaseDiscreteActionEnv): Such as Sokoban (push box) and FrozenLake (ice lake). They represent classic RL challenges such as discrete action control and uncertain state transitions."),(0,i.yg)("li",{parentName:"ul"},"Complex environments based on natural language interaction (inheriting from BaseLanguageBasedEnv): Such as WebShop (simulated online shopping) and Countdown (number game). They represent advanced LLM Agent challenges such as complex natural language understanding and generation, multi-step planning, and reasoning.")),(0,i.yg)("h2",{id:"core-functions"},"Core Functions"),(0,i.yg)("p",null,"A standard Env typically needs to implement the following functions:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Observation Space",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Defines the format, range, and type of information that the agent can obtain from the environment."),(0,i.yg)("li",{parentName:"ul"},"Example: Box(low=0, high=255, shape=(84, 84, 3)) for image input, or Text(max_length=8192) for long text input."))),(0,i.yg)("li",{parentName:"ul"},"Action Space",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Defines the type and range of actions that the agent can perform."),(0,i.yg)("li",{parentName:"ul"},"Example: Discrete(n=4) for discrete actions (such as up, down, left, right), or Text(max_length=256) for text generation actions."))),(0,i.yg)("li",{parentName:"ul"},"reset() method",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Called at the beginning of each training episode."),(0,i.yg)("li",{parentName:"ul"},"Resets the environment to its initial state and returns the initial observation."),(0,i.yg)("li",{parentName:"ul"},"Standard return: initial_observation, info (where info is an optional auxiliary information dictionary)."))),(0,i.yg)("li",{parentName:"ul"},"step(action) method",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Called after the agent performs an action."),(0,i.yg)("li",{parentName:"ul"},"Updates the environment state based on the agent's action, calculates the reward, and determines whether the episode is over."),(0,i.yg)("li",{parentName:"ul"},"Standard return:",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"next_observation: New observation after performing the action."),(0,i.yg)("li",{parentName:"ul"},"reward: Reward (floating point number) that the agent receives for performing the action."),(0,i.yg)("li",{parentName:"ul"},"terminated: Boolean value indicating whether the episode ends due to reaching a termination condition (such as game failure, reaching the goal)."),(0,i.yg)("li",{parentName:"ul"},"truncated: Boolean value indicating whether the episode ends due to non-natural termination conditions such as time limits."),(0,i.yg)("li",{parentName:"ul"},"info: Dictionary containing diagnostic information (such as debugging data, which should not be used as input to the agent)."))))),(0,i.yg)("li",{parentName:"ul"},"render() method (optional)",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Used to visualize the environment state, such as displaying a graphical interface on the screen."),(0,i.yg)("li",{parentName:"ul"},"For headless training scenarios, this method usually does not need to be implemented."))),(0,i.yg)("li",{parentName:"ul"},"close() method (optional)",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Used to clean up environment resources, such as closing rendering windows or releasing file handles.")))),(0,i.yg)("h2",{id:"code-examples"},"Code Examples"),(0,i.yg)("h3",{id:"sokoban-environment-classic-puzzle-task-with-discrete-actions"},"Sokoban Environment: Classic Puzzle Task with Discrete Actions"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Environment Configuration SokobanEnvConfig")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'class SokobanEnvConfig:\n    # Room dimensions (rows, columns)\n    dim_room: Tuple[int, int] = (6, 6) \n    # Maximum steps per episode\n    max_steps: int = 100 \n    # Number of boxes in the room\n    num_boxes: int = 3 \n    # Search depth used when generating solvable rooms\n    search_depth: int = 300 \n    # Mapping from integer IDs of grid elements to character representations for text rendering\n    grid_lookup: Optional[Dict[int, str]] = field(\n        default_factory=lambda: {0: "#", 1: "_", 2: "O", 3: "\u221a", 4: "X", 5: "P", 6: "S"}\n    )\n    # Mapping from grid element characters to readable names\n    grid_vocab: Optional[Dict[str, str]] = field(\n        default_factory=lambda: {\n            "#": "wall",\n            "_": "empty",\n            "O": "target",\n            "\u221a": "box on target",\n            "X": "box",\n            "P": "player",\n            "S": "player on target",\n        }\n    )\n    # Mapping from action IDs to action names (1:Up, 2:Down, 3:Left, 4:Right)\n    action_lookup: Optional[Dict[int, str]] = field(\n        default_factory=lambda: {1: "Up", 2: "Down", 3: "Left", 4: "Right"}\n    )\n    # Compatibility fields for setting dim_room via dim_x, dim_y\n    dim_x: Optional[int] = None\n    dim_y: Optional[int] = None\n    render_mode: str = "text"\n')),(0,i.yg)("ol",{start:2},(0,i.yg)("li",{parentName:"ol"},"Environment Implementation SokobanEnv\nThis is a standard reinforcement learning environment implementation that inherits from BaseDiscreteActionEnv (a generic interface for discrete action environments) and GymSokobanEnv (the core logic of the Sokoban game) in the framework.")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Define action space: 4 discrete actions with IDs starting from 1 (1, 2, 3, 4)")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"self.ACTION_SPACE = gym.spaces.discrete.Discrete(4, start=1)\n")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"reset method: Generate a new Sokoban room layout and reset the game's internal state")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"def reset(self, seed=None):\n    try:\n        # Use all_seed to ensure reproducibility of room generation\n        with all_seed(seed):\n            # Call generate_room to generate a new room layout\n            self.room_fixed, self.room_state, self.box_mapping, action_sequence = generate_room(\n                dim=self.dim_room,\n                num_steps=self.num_gen_steps, # Steps required for room generation\n                num_boxes=self.num_boxes,\n                search_depth=self.search_depth,\n            )\n        # Reset episode-related counters and states\n        self.num_env_steps, self.reward_last, self.boxes_on_target = 0, 0, 0\n        self.player_position = np.argwhere(self.room_state == 5)[0] # Find player position\n        \n        # Return initial observation (obtained via render method)\n        return self.render()\n    except (RuntimeError, RuntimeWarning) as e:\n        # If room generation fails, try again with a new seed\n        next_seed = abs(hash(str(seed))) % (2**32) if seed is not None else None\n        return self.reset(next_seed)\n")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"step method: Update the environment state based on the action performed by the agent.")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'def step(self, action: int):\n    # Record the player\'s old position to determine if the action was effective\n    previous_pos = self.player_position\n    \n    # Call the parent class GymSokobanEnv\'s step method to execute the action\n    _, reward, done, _ = GymSokobanEnv.step(self, action)\n    \n    # Get the new observation after executing the action\n    next_obs = self.render()\n    \n    # Determine if the action actually changed the player\'s position\n    action_effective = not np.array_equal(previous_pos, self.player_position)\n    \n    # Construct and return the additional information dictionary\n    info = {\n        "action_is_effective": action_effective, # Whether the action actually moved the player or boxes\n        "action_is_valid": True, # Whether the passed action ID is valid (even if it hits a wall)\n        "success": self.boxes_on_target == self.num_boxes, # Whether all boxes are on targets (game won)\n    }\n\n    # Return the standard reinforcement learning environment step result (next_observation, reward, terminated, info)\n    return next_obs, reward, done, info\n')),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"render method: Render the current environment state as text or image.")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'def render(self, mode=None):\n    # Use the specified mode or default mode\n    render_mode = mode if mode is not None else self.render_mode \n    \n    if render_mode == "text":\n        # Text rendering: Convert the internal numeric representation of room state to ASCII character grid\n        room = np.where((self.room_state == 5) & (self.room_fixed == 2), 6, self.room_state)\n        return "\\n".join("".join(self.GRID_LOOKUP.get(cell, "?") for cell in row) for row in room.tolist())\n    elif render_mode == "rgb_array":\n        # Image rendering: Delegate to the parent class GymSokobanEnv\'s get_image method\n        return self.get_image(mode="rgb_array", scale=1)\n    else:\n        raise ValueError(f"Invalid mode: {render_mode}")\n')),(0,i.yg)("ol",{start:3},(0,i.yg)("li",{parentName:"ol"},"Module Testing")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import matplotlib.pyplot as plt\n# Create a Sokoban environment configuration\nconfig = SokobanEnvConfig(dim_room=(6, 6), num_boxes=1, max_steps=100, search_depth=10)\n# Create a Sokoban environment instance using this configuration\nenv = SokobanEnv(config)\n# Loop 10 times, each time resetting the environment with a different seed and printing the initial state to observe different room layouts.\nfor i in range(10):\n    # Reset the environment and pass in the seed\n    print(env.reset(seed=1010 + i))\n    print()\n# Enter an interactive loop that allows the user to control the agent through keyboard input.  \nwhile True:\n    keyboard = input("Enter action: ")\n    if keyboard == "q":\n        break\n    # Convert input to integer action ID  \n    action = int(keyboard)\n    assert action in env.ACTION_LOOKUP, f"Invalid action: {action}"\n    # Execute the action and get the new observation, reward, termination state, and information\n    obs, reward, done, info = env.step(action)\n    print(obs, reward, done, info)\n# If the environment supports RGB array rendering, get the final game screen image  \nnp_img = env.get_image("rgb_array")\n# Save the image\nplt.imsave("sokoban1.png", np_img)\n')),(0,i.yg)("h3",{id:"webshop-environment-complex-interaction-task-driven-by-natural-language"},"WebShop Environment: Complex Interaction Task Driven by Natural Language"),(0,i.yg)("p",null,"WebShop is a simulated online shopping task environment that requires the agent to complete operations such as searching, selecting products, viewing details, and placing orders according to natural language instructions. Each trajectory can contain up to 50 steps, which places high demands on the model's context understanding ability and task execution efficiency."),(0,i.yg)("p",null,"The following focuses on the differences from Sokoban:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"WebShop parses available actions in the environment and converts them into a list of text strings that the agent can generate.")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'def get_available_actions(self):\n    # Get raw available action information from the underlying WebShop simulator\n    # Unlike Sokoban\'s fixed action set, WebShop\'s action space is dynamic.\n    orig_available_actions = WebAgentTextEnv.get_available_actions(self) \n    available_actions = []\n    # Define the text format for search actions\n    if orig_available_actions["has_search_bar"]:\n        available_actions.append("search[<content>]") \n    # Define the text format for click actions\n    for clickable in orig_available_actions["clickables"]:\n        if clickable != "search":\n            available_actions.append(f"click[{clickable}]") \n    # Return a string list to guide the Agent on which string to generate      \n    return available_actions\n')),(0,i.yg)("ol",{start:2},(0,i.yg)("li",{parentName:"ol"},"WebShop's reset can specify a session ID and initial instruction text.")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'def reset(\n    self, seed=None, session: Optional[Union[str, int]] = None, instruction_text: Optional[str] = None\n) -> any:\n  \n    # Session ID management: If not provided, generate a random one\n    if session is None:\n        with all_seed(seed):\n            session = "".join(random.choices(string.ascii_lowercase, k=10))\n    \n    # Call the parent class WebAgentTextEnv\'s reset, which returns text observation\n    obs, _ = WebAgentTextEnv.reset(self, session=session, instruction_text=instruction_text)\n    \n    # Prepare render cache: Add the initial instruction to the cache for the render method\n    self.prepare_render_cache(WebAgentTextEnv.get_instruction_text(self))\n    return obs\n')),(0,i.yg)("ol",{start:3},(0,i.yg)("li",{parentName:"ol"},"WebShop's action is a natural language text string.")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'def step(self, action):\n    # Call the parent class WebAgentTextEnv\'s step, which parses and executes the text action\n    state, reward, done, info = WebAgentTextEnv.step(self, action)\n    \n    # Prepare render cache: Update the cached observation\n    self.prepare_render_cache(self.observation)\n    \n    # Construct additional information dictionary\n    info = {\n        "action_is_effective": tuple(self.get_available_actions()) \n        == ("click[back to search]", "click[< prev]", "click[next >]"), \n        "action_is_valid": True,\n        "success": done, \n    }\n    return self.observation, reward, done, info\n')),(0,i.yg)("h2",{id:"creating-custom-environments"},"Creating Custom Environments"),(0,i.yg)("h3",{id:"step-overview"},"Step Overview"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Choose a base class: Select to inherit from BaseDiscreteActionEnv or BaseLanguageBasedEnv based on your task type (discrete actions or language interaction)")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Define init: Initialize environment parameters, define observation_space and action_space")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Implement reset(): Define the initial state of the environment")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Implement step(action): Define how the environment updates state, calculates rewards, and determines episode termination based on actions")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Implement render(): Define the environment's rendering logic")),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("p",{parentName:"li"},"Implement close(): Define resource cleanup logic"))),(0,i.yg)("h3",{id:"design-recommendations"},"Design Recommendations"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"State Representation",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Discrete action environments: Structured grid states, position information, etc."),(0,i.yg)("li",{parentName:"ul"},"Language environments: Text observations should contain all relevant context (such as complete web content, instructions) and consider context window limitations. Too much redundant information will reduce LLM efficiency or make it unable to process."))),(0,i.yg)("li",{parentName:"ol"},"Action Space Design",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Discrete action environments: Actions are predefined integers or enumeration values."),(0,i.yg)("li",{parentName:"ul"},"Language environments: Actions are natural language text. This requires the agent to have natural language generation capabilities, and the environment needs to be able to parse and validate these text actions."))),(0,i.yg)("li",{parentName:"ol"},"Reward Function Design",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Clear objectives: Rewards should clearly guide the agent to achieve the behavior you expect."),(0,i.yg)("li",{parentName:"ul"},"Sparse rewards vs. dense rewards:",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Discrete action environments: Rewards are usually given when completing sub-goals or final goals."),(0,i.yg)("li",{parentName:"ul"},"Language environments:",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"WebShop may have sparse rewards but can also design intermediate rewards."),(0,i.yg)("li",{parentName:"ul"},"Countdown uses hierarchical rewards (0, format score, full score) to guide learning."))))),(0,i.yg)("li",{parentName:"ul"},"Avoid reward hacking: Ensure the agent cannot obtain high rewards through unintended means."),(0,i.yg)("li",{parentName:"ul"},"Format penalty terms: In language environments, it is crucial to impose penalties on text actions that do not conform to expected formats, as it can effectively guide LLM to generate structured and parseable outputs."))),(0,i.yg)("li",{parentName:"ol"},"Episode Termination Conditions",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Clearly define conditions for success, failure, or timeout to end a training episode. Use terminated and truncated to represent natural termination and non-natural termination respectively."),(0,i.yg)("li",{parentName:"ul"},"WebShop also has a maximum step limit"))),(0,i.yg)("li",{parentName:"ol"},"Uncertainty/Randomness: If the environment contains uncertainty (such as FrozenLake), ensure its behavior is a predictable probability distribution, and control randomness through seed in reset."),(0,i.yg)("li",{parentName:"ol"},"Reproducibility: Use the seed parameter to initialize the random number generator to ensure that the environment's behavior is reproducible each time it runs.")))}p.isMDXComponent=!0},5680:(e,n,t)=>{t.d(n,{xA:()=>m,yg:()=>u});var a=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},m=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},d="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef(function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),d=c(t),g=i,u=d["".concat(l,".").concat(g)]||d[g]||p[g]||r;return t?a.createElement(u,o(o({ref:n},m),{},{components:t})):a.createElement(u,o({ref:n},m))});function u(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,o=new Array(r);o[0]=g;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[d]="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=t[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"}}]);