"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[9536],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var r=i(96540);const s={},a=r.createContext(s);function t(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),r.createElement(a.Provider,{value:n},e.children)}},92530:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"User Guides/Algorithms/Reward_FL","title":"Reward Feedback Learning (Reward FL)","description":"Introduction","source":"@site/docs/User Guides/Algorithms/Reward_FL.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/Reward_FL","permalink":"/ROLL/docs/User Guides/Algorithms/Reward_FL","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/Reward_FL.md","tags":[],"version":"current","lastUpdatedAt":1764905933000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Reinforce++","permalink":"/ROLL/docs/User Guides/Algorithms/Reinforce_Plus_Plus"},"next":{"title":"TOPR (Tapered Off-Policy REINFORCE)","permalink":"/ROLL/docs/User Guides/Algorithms/TOPR"}}');var s=i(74848),a=i(28453);const t={},l="Reward Feedback Learning (Reward FL)",d={},o=[{value:"Introduction",id:"introduction",level:2},{value:"Reward FL Configuration Parameters",id:"reward-fl-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"Wan2_2 Related Parameters",id:"wan2_2-related-parameters",level:3},{value:"Note",id:"note",level:2},{value:"Refernece Model",id:"refernece-model",level:2},{value:"Preprocess checkpoints",id:"preprocess-checkpoints",level:2},{value:"Setup environments",id:"setup-environments",level:2},{value:"Reference Example",id:"reference-example",level:2},{value:"Reference",id:"reference",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"reward-feedback-learning-reward-fl",children:"Reward Feedback Learning (Reward FL)"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Reward Feedback Learning (Reward FL) is a reinforcement learning algorithm that optimize diffusion models against a scorer. Reward Fl works as follows:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sampling"}),": For a given prompt and first frame latent, the model generates a corresponding video."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward Assignment"}),": Each video is evaluated and assigned a reward based on its face informations."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Update"}),": The model updates its parameters based on reward signals from the generated videos, reinforcing strategies that obtain higher rewards."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"reward-fl-configuration-parameters",children:"Reward FL Configuration Parameters"}),"\n",(0,s.jsxs)(n.p,{children:["In ROLL, the Reward FL algorithm-specific configuration parameters are as follows (",(0,s.jsx)(n.code,{children:"roll.pipeline.diffusion.reward_fl.reward_fl_config.RewardFLConfig"}),"):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# reward fl\nlearning_rate: 2e-6\nlr_scheduler_type: constant\nper_device_train_batch_size: 1\ngradient_accumulation_steps: 1\nwarmup_steps: 10\nnum_train_epochs: 1\n\nmodel_name: "wan2_2"\n\n# wan2_2 related\nmodel_paths: ./examples/wan2.2-14B-reward_fl_ds/wan22_paths.json\nreward_model_path: /data/models/antelopev2/\ntokenizer_path: /data/models/Wan-AI/Wan2.1-T2V-1.3B/google/umt5-xxl/\nmodel_id_with_origin_paths: null\ntrainable_models: dit2\nuse_gradient_checkpointing_offload: true\nextra_inputs: input_image\nmax_timestep_boundary: 1.0\nmin_timestep_boundary: 0.9\nnum_inference_steps: 8\n'})}),"\n",(0,s.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"num_train_epochs"}),": Number of optimization rounds per batch of samples"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"train_batch_size"}),": Batch size for one train step.In deepspeed training the global train batch size is ",(0,s.jsx)(n.code,{children:"per_device_train_batch_size"})," * ",(0,s.jsx)(n.code,{children:"gradient_accumulation_steps"})," * world_size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"learning_rate"}),": Learning rate"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"per_device_train_batch_size"}),": Training batch size per device"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"gradient_accumulation_steps"}),": Gradient accumulation steps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"weight_decay"}),": Weight decay coefficient"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"warmup_steps"}),": Learning rate warmup steps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"lr_scheduler_type"}),": Learning rate scheduler type"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"wan2_2-related-parameters",children:"Wan2_2 Related Parameters"}),"\n",(0,s.jsx)(n.p,{children:"The following parameters related to Wan2_2 are as follows:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"model_paths"}),": Model path of json file, e.g., ",(0,s.jsx)(n.code,{children:"wan22_paths.json"}),", including high_noise_model, low_noise_model, text_encoder, vae."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"tokenizer_path"}),": Tokenizer path. Leave empty to auto-download."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"reward_model_path"}),": Reward model path, e.g., face model."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"max_timestep_boundary"}),": Maximum value of the timestep interval, ranging from 0 to 1. Default is 1. This needs to be manually set only when training mixed models with multiple DiTs, for example, ",(0,s.jsx)(n.a,{href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B",children:"Wan-AI/Wan2.2-I2V-A14B"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"min_timestep_boundary"}),": Minimum value of the timestep interval, ranging from 0 to 1. Default is 1. This needs to be manually set only when training mixed models with multiple DiTs, for example, ",(0,s.jsx)(n.a,{href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B",children:"Wan-AI/Wan2.2-I2V-A14B"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"model_id_with_origin_paths"}),": Model ID with origin paths, e.g., Wan-AI/Wan2.1-T2V-1.3B",":diffusion_pytorch_model","*.safetensors. Comma-separated."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"trainable_models"}),": Models to train, e.g., dit, vae, text_encoder."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"extra_inputs"}),": Additional model inputs, comma-separated."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_gradient_checkpointing_offload"}),": Whether to offload gradient checkpointing to CPU memory."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"num_inference_steps"}),": Number of inference steps, default is 8 for the distilled wan2_2 model."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"note",children:"Note"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The reward model is constructed based on facial information, Please ensure that the first frame of the video contains a human face."}),"\n",(0,s.jsxs)(n.li,{children:["Download the reward model(antelopev2.zip) and unzip the onnx files to ",(0,s.jsx)(n.code,{children:"reward_model_path"})," directory."]}),"\n",(0,s.jsxs)(n.li,{children:["Download the official Wan2.2 pipeline and Distilled Wan2.2 DiT safetensors. Put them in the ",(0,s.jsx)(n.code,{children:"model_paths"})," directory, e.g., ",(0,s.jsx)(n.code,{children:"wan22_paths.json"})," file."]}),"\n",(0,s.jsx)(n.li,{children:"According to the data/example_video_dataset/metadata.csv file, adapt your video dataset to the corresponding format"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"refernece-model",children:"Refernece Model"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Official Wan2.2 pipeline"}),": ",(0,s.jsx)(n.a,{href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B",children:"Wan-AI/Wan2.2-I2V-A14B"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Distilled Wan2.2 DiT safetensors"}),": ",(0,s.jsx)(n.a,{href:"https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main",children:"lightx2v/Wan2.2-Lightning"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Reward Model"}),": ",(0,s.jsx)(n.a,{href:"https://github.com/deepinsight/insightface/releases/download/v0.7/antelopev2.zip",children:"deepinsight/insightface"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"preprocess-checkpoints",children:"Preprocess checkpoints"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Run ",(0,s.jsx)(n.code,{children:"merge_model.py"})," to merge multiple files of ",(0,s.jsx)(n.code,{children:"Official Wan2.2 pipeline"})," high noise model and low noise model into one file, respectively."]}),"\n",(0,s.jsxs)(n.li,{children:["Run ",(0,s.jsx)(n.code,{children:"merge_lora.py"})," to merge ",(0,s.jsx)(n.code,{children:"Distilled Wan2.2 DiT safetensors"})," lora to the base model of ",(0,s.jsx)(n.code,{children:"Official Wan2.2 pipeline"})," high noise model and low noise model, respectively."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"setup-environments",children:"Setup environments"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"pip install -r requirements_torch260_diffsynth.txt\n"})}),"\n",(0,s.jsx)(n.h2,{id:"reference-example",children:"Reference Example"}),"\n",(0,s.jsx)(n.p,{children:"You can refer to the following configuration file to set up Reward FL training:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"./examples/docs_examples/example_reward_fl.yaml"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Run ",(0,s.jsx)(n.code,{children:"run_reward_fl_ds_pipeline.sh"})," to get start."]}),"\n",(0,s.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsxs)(n.p,{children:["[1]: Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2510.14255",children:"https://arxiv.org/abs/2510.14255"})]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);