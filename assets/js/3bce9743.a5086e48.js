"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[839],{12464:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>u,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"User Guides/Algorithms/RAFT_Plus_Plus","title":"RAFT++ (Reward rAnked Fine-Tuning)","description":"Introduction","source":"@site/docs/User Guides/Algorithms/RAFT_Plus_Plus.md","sourceDirName":"User Guides/Algorithms","slug":"/User Guides/Algorithms/RAFT_Plus_Plus","permalink":"/ROLL/docs/User Guides/Algorithms/RAFT_Plus_Plus","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Algorithms/RAFT_Plus_Plus.md","tags":[],"version":"current","lastUpdatedAt":1764905933000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Proximal Policy Optimization (PPO)","permalink":"/ROLL/docs/User Guides/Algorithms/PPO"},"next":{"title":"Reinforce++","permalink":"/ROLL/docs/User Guides/Algorithms/Reinforce_Plus_Plus"}}');var i=r(74848),o=r(28453);const l={},t="RAFT++ (Reward rAnked Fine-Tuning)",a={},d=[{value:"Introduction",id:"introduction",level:2},{value:"RAFT++ Configuration Parameters",id:"raft-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"raft-reward-ranked-fine-tuning",children:"RAFT++ (Reward rAnked Fine-Tuning)"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"RAFT++ (Reward rAnked Fine-Tuning) is a ranking-based reinforcement learning algorithm that optimizes policies by comparing rewards of different responses. RAFT++ works as follows:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Group Sampling"}),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reward Ranking"}),": Each solution is evaluated and assigned a reward based on its correctness or quality, then ranked according to the rewards."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Policy Update"}),": The model updates its parameters by comparing rewards of different solutions within the group, reinforcing strategies that obtain higher rewards."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"raft-configuration-parameters",children:"RAFT++ Configuration Parameters"}),"\n",(0,i.jsxs)(n.p,{children:["In ROLL, the RAFT++ algorithm-specific configuration parameters are as follows (",(0,i.jsx)(n.code,{children:"roll.pipeline.rlvr.rlvr_config.RLVRConfig"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# RAFT++ core config\nadv_estimator: "grpo"\n\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n\n# advantage\nwhiten_advantages: false\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# advantage\nadvantage_clip: 2.0\ndual_clip_loss: true\n# clip\nreward_clip: 10\n\n# reward\nadd_token_level_kl: false\n'})}),"\n",(0,i.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, set to "grpo", which is the core configuration of RAFT++ algorithm']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"norm_mean_type"}),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"norm_std_type"}),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values, default value is false"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,i.jsx)(n.p,{children:"The following parameters are common configuration items for PPO-class algorithms:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"rollout_batch_size"}),": Number of prompts per rollout_batch_size, default value is 64"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num_return_sequences_in_group"}),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group), default value is 8"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"prompt_length"}),": Maximum length of prompts, default value is 2048"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"response_length"}),": Maximum length of responses, default value is 4096"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization rounds per batch of samples, default value is 1"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"use_kl_loss"}),": Whether to use KL divergence loss, default value is true"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"kl_loss_coef"}),": KL-loss coefficient, default value is 0.001"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"loss_agg_mode"}),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range, default value is 2.0"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dual_clip_loss"}),": Whether to use dual clipping loss, default value is true"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"reward_clip"}),": Reward value clipping range, default value is 10"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"add_token_level_kl"}),": Whether to add token-level KL penalty, default value is false"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"reference-example",children:"Reference Example"}),"\n",(0,i.jsx)(n.p,{children:"You can refer to the following configuration file to set up RAFT++ training:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"./examples/docs_examples/example_raft_pp.yaml"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.p,{children:["[1] Xiong, W.; Yao, J.; Xu, Y.; Pang, B.; Wang, L.; Sahoo, D.; Li, J.; Jiang, N.; Zhang, T.; Xiong, C.; Dong, H. A Minimalist Approach to LLM Reasoning: From Rejection Sampling to Reinforce. arXiv April 15, 2025. ",(0,i.jsx)(n.a,{href:"https://doi.org/10.48550/arXiv.2504.11343",children:"https://doi.org/10.48550/arXiv.2504.11343"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>t});var s=r(96540);const i={},o=s.createContext(i);function l(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);