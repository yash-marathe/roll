"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[7151],{2484:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>s});var t=a(8168),r=(a(6540),a(5680));const i={},l="StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)",o={unversionedId:"English/UserGuide/algorithms/agentic_GiGPO",id:"English/UserGuide/algorithms/agentic_GiGPO",title:"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/agentic_GiGPO.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/agentic_GiGPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/agentic_GiGPO",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/agentic_GiGPO.md",tags:[],version:"current",lastUpdatedAt:1755690404,formattedLastUpdatedAt:"Aug 20, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"TOPR (Tapered Off-Policy REINFORCE)",permalink:"/ROLL/docs/English/UserGuide/algorithms/TOPR"},next:{title:"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)",permalink:"/ROLL/docs/English/UserGuide/algorithms/agentic_StarPO"}},p={},s=[{value:"Introduction",id:"introduction",level:2},{value:"GiGPO Configuration Parameters",id:"gigpo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Environment Manager Parameters",id:"environment-manager-parameters",level:3},{value:"Reference Examples",id:"reference-examples",level:2},{value:"References",id:"references",level:2}],g={toc:s},u="wrapper";function m({components:e,...n}){return(0,r.yg)(u,(0,t.A)({},g,n,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"stepwiselearninggigpo-group-in-group-policy-optimization"},"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)"),(0,r.yg)("h2",{id:"introduction"},"Introduction"),(0,r.yg)("p",null,"GiGPO (Group-in-Group Policy Optimization) is a novel reinforcement learning algorithm for LLM agent training. It achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence."),(0,r.yg)("p",null,"GiGPO introduces a two-level structure for estimating relative advantage:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"At the episode level, GiGPO computes macro relative advantages based on groups of complete trajectories"),(0,r.yg)("li",{parentName:"ol"},"At the step level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories")),(0,r.yg)("p",null,"This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts."),(0,r.yg)("h2",{id:"gigpo-configuration-parameters"},"GiGPO Configuration Parameters"),(0,r.yg)("p",null,"In ROLL, the core implementation of GiGPO is located at ",(0,r.yg)("inlineCode",{parentName:"p"},"roll/pipeline/agentic/utils.py"),". The specific configuration parameters for the GiGPO algorithm are as follows (",(0,r.yg)("inlineCode",{parentName:"p"},"roll.pipeline.agentic.agentic_config.AgenticConfig"),"):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'# GiGPO core config\nadv_estimator: "gigpo"\nbatch_adjust_mode: "copy"\nstep_reward_weight: 1.0\nepisode_reward_weight: 1.0\nstep_reward_gamma: 0.95\n\n# rollout_batch_size is the number of trajectories\nrollout_batch_size: 1024\nval_batch_size: 1024\nsequence_length: 1024\n\nadvantage_clip: 0.2\nppo_epochs: 1\n\n# pg_clip: 0.1\n#dual_clip_loss: True\ninit_kl_coef: 0.0\nwhiten_advantages: true\nentropy_loss_coef: 0\nmax_grad_norm: 1.0\n\nreward_normalization:\n  grouping: traj_group_id # Can be tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by calculates reward/adv\n  method: mean # asym_clip / identity / mean_std / mean\n\ntrain_env_manager:\n  max_env_num_per_worker: 16\n  num_env_groups: 128\n  # under the same group, the env config and env seed are ensured to be equal\n  group_size: 8\n  tags: [FrozenLake]\n  num_groups_partition: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation\n\nenv_manager_cls: roll.pipeline.agentic.env_manager.step_env_manager.StepEnvManager\n')),(0,r.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, set to "gigpo", which is the core configuration of the GiGPO algorithm'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"batch_adjust_mode"),': Batch adjustment mode, optional values are "copy", "delete", "auto", default value is "copy"'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"step_reward_weight"),": Step reward weight, used in the GiGPO algorithm, default value is 1.0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"episode_reward_weight"),": Episode reward weight, used in the GiGPO algorithm, default value is 1.0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"step_reward_gamma"),": Discount factor for step reward calculation, default value is 0.95"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"env_manager_cls"),": Environment manager class, GiGPO needs to use ",(0,r.yg)("inlineCode",{parentName:"li"},"roll.pipeline.agentic.env_manager.step_env_manager.StepEnvManager"))),(0,r.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,r.yg)("p",null,"The following parameters are common configuration items for PPO-class algorithms:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of trajectories per rollout batch, default value is 1024"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"val_batch_size"),": Validation batch size, default value is 1024"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"sequence_length"),": Maximum sequence length, default value is 1024"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range, default value is 0.2"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization epochs per batch of samples, default value is 1"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"init_kl_coef"),": Initial coefficient for KL penalty, default value is 0.0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values, default value is true"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"entropy_loss_coef"),": Entropy loss coefficient, default value is 0"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"max_grad_norm"),": Maximum norm for gradient clipping, default value is 1.0")),(0,r.yg)("h3",{id:"environment-manager-parameters"},"Environment Manager Parameters"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.max_env_num_per_worker"),": Maximum number of environments per worker, default value is 16"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.num_env_groups"),": Number of training environment groups, default value is 128"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.group_size"),": Number of environments per group, default value is 8"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.tags"),": List of environment tags, default value is ","[FrozenLake]"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_env_manager.num_groups_partition"),": Group allocation for each environment type, default value is ","[128]")),(0,r.yg)("h2",{id:"reference-examples"},"Reference Examples"),(0,r.yg)("p",null,"You can refer to the following configuration files to set up GiGPO training:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_gigpo.yaml"))),(0,r.yg)("h2",{id:"references"},"References"),(0,r.yg)("p",null,"[1]"," Feng, L.; Xue, Z.; Liu, T.; An, B. Group-in-Group Policy Optimization for LLM Agent Training. arXiv 2025, 2505.10978."))}m.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>g,yg:()=>d});var t=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,t)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach(function(n){r(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function o(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=t.createContext({}),s=function(e){var n=t.useContext(p),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},g=function(e){var n=s(e.components);return t.createElement(p.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef(function(e,n){var a=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),u=s(a),c=r,d=u["".concat(p,".").concat(c)]||u[c]||m[c]||i;return a?t.createElement(d,l(l({ref:n},g),{},{components:a})):t.createElement(d,l({ref:n},g))});function d(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=c;var o={};for(var p in n)hasOwnProperty.call(n,p)&&(o[p]=n[p]);o.originalType=e,o[u]="string"==typeof e?e:r,l[1]=o;for(var s=2;s<i;s++)l[s]=a[s];return t.createElement.apply(null,l)}return t.createElement.apply(null,a)}c.displayName="MDXCreateElement"}}]);