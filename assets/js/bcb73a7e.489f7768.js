"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[5703],{1232:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/AgenticPipeline-bda0e93b4092e57a95d4c93410e0f8d5.jpg"},5680:(e,t,n)=>{n.d(t,{xA:()=>p,yg:()=>m});var i=n(6540);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,i)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach(function(t){a(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function o(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=i.createContext({}),s=function(e){var t=i.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=s(e.components);return i.createElement(c.Provider,{value:t},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},d=i.forwardRef(function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,c=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),g=s(n),d=a,m=g["".concat(c,".").concat(d)]||g[d]||u[d]||r;return n?i.createElement(m,l(l({ref:t},p),{},{components:n})):i.createElement(m,l({ref:t},p))});function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,l=new Array(r);l[0]=d;var o={};for(var c in t)hasOwnProperty.call(t,c)&&(o[c]=t[c]);o.originalType=e,o[g]="string"==typeof e?e:a,l[1]=o;for(var s=2;s<r;s++)l[s]=n[s];return i.createElement.apply(null,l)}return i.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8461:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>s});var i=n(8168),a=(n(6540),n(5680));const r={},l="AgenticPipeline",o={unversionedId:"English/DesignImplementation/AgenticPipeline",id:"English/DesignImplementation/AgenticPipeline",title:"AgenticPipeline",description:"Agentic Pipeline Architecture Diagram",source:"@site/docs/English/DesignImplementation/AgenticPipeline.md",sourceDirName:"English/DesignImplementation",slug:"/English/DesignImplementation/AgenticPipeline",permalink:"/ROLL/docs/English/DesignImplementation/AgenticPipeline",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/DesignImplementation/AgenticPipeline.md",tags:[],version:"current",lastUpdatedAt:1761727400,formattedLastUpdatedAt:"Oct 29, 2025",frontMatter:{},sidebar:"tutorialSidebar",next:{title:"RLVR Pipeline",permalink:"/ROLL/docs/English/DesignImplementation/RLVRPipeline"}},c={},s=[{value:"Agentic Pipeline Architecture Diagram",id:"agentic-pipeline-architecture-diagram",level:2},{value:"AgenticPipeline",id:"agenticpipeline-1",level:2},{value:"Main Attributes",id:"main-attributes",level:3},{value:"Core Configuration",id:"core-configuration",level:4},{value:"Actor-Critic Architecture Clusters",id:"actor-critic-architecture-clusters",level:4},{value:"Environment Interaction Scheduler",id:"environment-interaction-scheduler",level:4},{value:"Controllers and Auxiliary Tools",id:"controllers-and-auxiliary-tools",level:4},{value:"Core Process",id:"core-process",level:3},{value:"model_update",id:"model_update",level:4},{value:"train_rollout",id:"train_rollout",level:4},{value:"val_rollout",id:"val_rollout",level:4},{value:"cal_ref_log_probs",id:"cal_ref_log_probs",level:4},{value:"cal_old_log_probs_values",id:"cal_old_log_probs_values",level:4},{value:"adv",id:"adv",level:4},{value:"critic.train_step (optional)",id:"critictrain_step-optional",level:4},{value:"actor_train.train_step",id:"actor_traintrain_step",level:4},{value:"compute_data_metrics",id:"compute_data_metrics",level:4},{value:"do_checkpoint",id:"do_checkpoint",level:4},{value:"tracker.log",id:"trackerlog",level:4}],p={toc:s},g="wrapper";function u({components:e,...t}){return(0,a.yg)(g,(0,i.A)({},p,t,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"agenticpipeline"},"AgenticPipeline"),(0,a.yg)("h2",{id:"agentic-pipeline-architecture-diagram"},"Agentic Pipeline Architecture Diagram"),(0,a.yg)("p",null,(0,a.yg)("img",{alt:"Agentic Pipeline Architecture Diagram",src:n(1232).A,width:"4602",height:"4230"})),(0,a.yg)("h2",{id:"agenticpipeline-1"},"AgenticPipeline"),(0,a.yg)("p",null,"AgenticPipeline is a core component in the ROLL framework, used for reinforcement learning training of agents. AgenticPipeline inherits from BasePipeline and implements the PPO (Proximal Policy Optimization) algorithm based on the Actor-Critic architecture, providing a complete distributed training pipeline for large language model agent training."),(0,a.yg)("h3",{id:"main-attributes"},"Main Attributes"),(0,a.yg)("h4",{id:"core-configuration"},"Core Configuration"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"pipeline_config: The core configuration object of the AgenticPipeline class, of type AgenticConfig, containing all configuration parameters for the entire reinforcement learning training pipeline.")),(0,a.yg)("h4",{id:"actor-critic-architecture-clusters"},"Actor-Critic Architecture Clusters"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"actor_train: The policy network training cluster in AgenticPipeline, responsible for executing the core training logic of the PPO algorithm."),(0,a.yg)("li",{parentName:"ul"},"actor_infer: The policy network inference cluster in AgenticPipeline, responsible for interacting with the environment to generate training data."),(0,a.yg)("li",{parentName:"ul"},"reference: The reference model cluster in AgenticPipeline, serving as a baseline model in the policy optimization process for calculating KL divergence."),(0,a.yg)("li",{parentName:"ul"},"critic (optional): Estimates the state value function (only used in GAE mode)")),(0,a.yg)("h4",{id:"environment-interaction-scheduler"},"Environment Interaction Scheduler"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"train_rollout_scheduler: Collects experience data during training, where infer_cluster=actor_infer"),(0,a.yg)("li",{parentName:"ul"},"val_rollout_scheduler: Collects experience data during validation to evaluate model performance, where infer_cluster=actor_infer")),(0,a.yg)("h4",{id:"controllers-and-auxiliary-tools"},"Controllers and Auxiliary Tools"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"kl_ctrl: Adaptively adjusts the KL penalty coefficient to prevent the policy update from deviating too far from the reference policy"),(0,a.yg)("li",{parentName:"ul"},"tokenizer: Handles text encoding and decoding"),(0,a.yg)("li",{parentName:"ul"},"running: Calculates and maintains runtime statistics")),(0,a.yg)("h3",{id:"core-process"},"Core Process"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"def run():\n    Initialize TPS timer\n    for global_step in range(max_steps):\n        # 1. Model state management\n        Update model parameters (actor_train -> actor_infer)\n        # 2. Evaluation phase (executed every eval_steps)\n        if global_step % eval_steps == 0:\n            batch = Validation environment rollout(val_batch_size)\n            Calculate evaluation metrics (score mean/max/min)\n            Save render results (optional) \n        # 3. Training data collection\n        batch = Training environment rollout(rollout_batch_size)\n        # 4. Calculate key probabilities and values\n        ref_log_probs = Reference model.calculate log probabilities(batch)\n        old_log_probs = Actor_train model.calculate log probabilities(batch)\n        if using GAE estimator:\n            values = Critic model.calculate value function(batch)\n        # 5. Reward processing and advantage calculation\n        Normalize reward scores by group\n        Apply KL penalty\n        Calculate advantage function (GAE or other methods)\n        # 6. Model training\n        if using GAE estimator:\n            Critic model.training step(batch)\n        if global_step > critic_warmup:\n            Actor model.training step(batch)\n        # 7. Record and save\n        Record training metrics\n        Save checkpoints\n        Print sample logs (every logging_steps)\n")),(0,a.yg)("h4",{id:"model_update"},"model_update"),(0,a.yg)("p",null,"Synchronize training model parameters to the inference model to ensure that the inference model used for generating rollout data uses the latest training parameters. In the PPO algorithm, the training model actor_train is responsible for parameter updates and gradient calculations, while the inference model actor_infer is responsible for generating rollout data. To ensure training consistency, the inference model needs to periodically synchronize the latest training data, so that the generated rollout data can reflect the true performance of the current policy."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"# Initialize phase to set synchronization pairs\nself.set_model_update_pair(\n            src_cluster=self.actor_train,\n            tgt_cluster=self.actor_infer,\n      frequency=self.pipeline_config.actor_train.model_update_frequency,)\n\n# Execute synchronization in training loop\nmodel_update_metrics: Dict = self.model_update(global_step)\nmetrics.update(model_update_metrics)\n")),(0,a.yg)("h4",{id:"train_rollout"},"train_rollout"),(0,a.yg)("p",null,"Generate rollout data for training, i.e., let the agent interact with the environment to produce experience data (state, action, reward sequences)."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"self.train_rollout_scheduler.get_batch(batch, self.pipeline_config.rollout_batch_size)\n")),(0,a.yg)("h4",{id:"val_rollout"},"val_rollout"),(0,a.yg)("p",null,"Use the validation set rollout scheduler to generate a batch of validation data. Validate every eval_steps steps."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"self.val_rollout_scheduler.get_batch(batch,self.pipeline_config.val_batch_size)\n")),(0,a.yg)("h4",{id:"cal_ref_log_probs"},"cal_ref_log_probs"),(0,a.yg)("p",null,(0,a.yg)("inlineCode",{parentName:"p"},"reference.compute_log_probs")," calculates the log probabilities of the reference model for the current batch data. Used for subsequent KL divergence penalty calculation to prevent the training policy from deviating too far from the initial policy."),(0,a.yg)("h4",{id:"cal_old_log_probs_values"},"cal_old_log_probs_values"),(0,a.yg)("p",null,"Calculate the log probabilities (old policy probabilities) and value function estimates of the current training model for rollout data, which is a key step in the PPO algorithm for calculating the importance sampling ratio. actor_train.compute_log_probs uses the current training model to calculate the log probabilities of rollout data. critic.compute_values, if using GAE, also calculates the state value function."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'self.actor_train.compute_log_probs(batch, blocking=False)\nif self.pipeline_config.adv_estimator == "gae":\n  self.critic.compute_values(batch, blocking=False)\n')),(0,a.yg)("h4",{id:"adv"},"adv"),(0,a.yg)("p",null,"Implements reward processing and advantage calculation, which is a core step in the PPO algorithm, responsible for converting environment rewards into training signals."),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"get_score_normalize_fn")," standardizes the scores given by the reward model"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"apply_kl_penalty")," adds the KL divergence between the log probabilities of actor_train and reference as a penalty term to the reward."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"compute_advantage")," calculates the advantage function based on the normalized rewards and the critic's value estimates.")),(0,a.yg)("h4",{id:"critictrain_step-optional"},"critic.train_step (optional)"),(0,a.yg)("p",null,"Based on the collected data and calculated returns (or value differences), the critic updates its parameters to more accurately predict state values."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'if self.pipeline_config.adv_estimator == "gae":\n    self.critic.train_step(batch, blocking=False)\n')),(0,a.yg)("h4",{id:"actor_traintrain_step"},"actor_train.train_step"),(0,a.yg)("p",null,(0,a.yg)("inlineCode",{parentName:"p"},"actor_train.train_step")," updates its parameters based on the calculated advantage function and KL penalty, in order to generate text with higher rewards."),(0,a.yg)("h4",{id:"compute_data_metrics"},"compute_data_metrics"),(0,a.yg)("p",null,"Calculate and statistics key metrics of training data, providing comprehensive data analysis for monitoring the training process."),(0,a.yg)("h4",{id:"do_checkpoint"},"do_checkpoint"),(0,a.yg)("p",null,"Save checkpoints"),(0,a.yg)("h4",{id:"trackerlog"},"tracker.log"),(0,a.yg)("p",null,"Generate text sample logs"))}u.isMDXComponent=!0}}]);