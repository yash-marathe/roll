"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[4659],{28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var r=i(96540);const a={},t=r.createContext(a);function s(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(t.Provider,{value:n},e.children)}},74296:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"User Guides/Agentic/agentic_GiGPO","title":"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)","description":"Introduction","source":"@site/docs/User Guides/Agentic/agentic_GiGPO.md","sourceDirName":"User Guides/Agentic","slug":"/User Guides/Agentic/agentic_GiGPO","permalink":"/ROLL/docs/User Guides/Agentic/agentic_GiGPO","draft":false,"unlisted":false,"editUrl":"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/User Guides/Agentic/agentic_GiGPO.md","tags":[],"version":"current","lastUpdatedAt":1764905933000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Tool Use Guide","permalink":"/ROLL/docs/User Guides/Agentic/Tool_Use"},"next":{"title":"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)","permalink":"/ROLL/docs/User Guides/Agentic/agentic_StarPO"}}');var a=i(74848),t=i(28453);const s={},o="StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"GiGPO Configuration Parameters",id:"gigpo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Environment Manager Parameters",id:"environment-manager-parameters",level:3},{value:"Reference Examples",id:"reference-examples",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"stepwiselearninggigpo-group-in-group-policy-optimization",children:"StepWiseLearning\u2014\u2014GiGPO (Group-in-Group Policy Optimization)"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"GiGPO (Group-in-Group Policy Optimization) is a novel reinforcement learning algorithm for LLM agent training. It achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence."}),"\n",(0,a.jsx)(n.p,{children:"GiGPO introduces a two-level structure for estimating relative advantage:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"At the episode level, GiGPO computes macro relative advantages based on groups of complete trajectories"}),"\n",(0,a.jsx)(n.li,{children:"At the step level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts."}),"\n",(0,a.jsx)(n.h2,{id:"gigpo-configuration-parameters",children:"GiGPO Configuration Parameters"}),"\n",(0,a.jsxs)(n.p,{children:["In ROLL, the core implementation of GiGPO is located at ",(0,a.jsx)(n.code,{children:"roll/pipeline/agentic/utils.py"}),". The specific configuration parameters for the GiGPO algorithm are as follows (",(0,a.jsx)(n.code,{children:"roll.pipeline.agentic.agentic_config.AgenticConfig"}),"):"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# GiGPO core config\nadv_estimator: "gigpo"\nbatch_adjust_mode: "copy"\nstep_reward_weight: 1.0\nepisode_reward_weight: 1.0\nstep_reward_gamma: 0.95\n\n# rollout_batch_size is the number of trajectories\nrollout_batch_size: 1024\nval_batch_size: 1024\nsequence_length: 1024\n\nadvantage_clip: 0.2\nppo_epochs: 1\n\n# pg_clip: 0.1\n#dual_clip_loss: True\ninit_kl_coef: 0.0\nwhiten_advantages: true\nentropy_loss_coef: 0\nmax_grad_norm: 1.0\n\nreward_normalization:\n  grouping: traj_group_id # Can be tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by calculates reward/adv\n  method: mean # asym_clip / identity / mean_std / mean\n\ntrain_env_manager:\n  max_env_num_per_worker: 16\n  num_env_groups: 128\n  # under the same group, the env config and env seed are ensured to be equal\n  group_size: 8\n  tags: [FrozenLake]\n  num_groups_partition: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation\n\nenv_manager_cls: roll.pipeline.agentic.env_manager.step_env_manager.StepEnvManager\n'})}),"\n",(0,a.jsx)(n.h3,{id:"core-parameter-descriptions",children:"Core Parameter Descriptions"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"adv_estimator"}),': Advantage estimator type, set to "gigpo", which is the core configuration of the GiGPO algorithm']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"batch_adjust_mode"}),': Batch adjustment mode, optional values are "copy", "delete", "auto", default value is "copy"']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"step_reward_weight"}),": Step reward weight, used in the GiGPO algorithm, default value is 1.0"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"episode_reward_weight"}),": Episode reward weight, used in the GiGPO algorithm, default value is 1.0"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"step_reward_gamma"}),": Discount factor for step reward calculation, default value is 0.95"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"env_manager_cls"}),": Environment manager class, GiGPO needs to use ",(0,a.jsx)(n.code,{children:"roll.pipeline.agentic.env_manager.step_env_manager.StepEnvManager"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"ppo-related-parameters",children:"PPO Related Parameters"}),"\n",(0,a.jsx)(n.p,{children:"The following parameters are common configuration items for PPO-class algorithms:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"rollout_batch_size"}),": Number of trajectories per rollout batch, default value is 1024"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"val_batch_size"}),": Validation batch size, default value is 1024"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"sequence_length"}),": Maximum sequence length, default value is 1024"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"advantage_clip"}),": Advantage value clipping range, default value is 0.2"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"ppo_epochs"}),": Number of optimization epochs per batch of samples, default value is 1"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"init_kl_coef"}),": Initial coefficient for KL penalty, default value is 0.0"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"whiten_advantages"}),": Whether to whiten advantage values, default value is true"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"entropy_loss_coef"}),": Entropy loss coefficient, default value is 0"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"max_grad_norm"}),": Maximum norm for gradient clipping, default value is 1.0"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"environment-manager-parameters",children:"Environment Manager Parameters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.max_env_num_per_worker"}),": Maximum number of environments per worker, default value is 16"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.num_env_groups"}),": Number of training environment groups, default value is 128"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.group_size"}),": Number of environments per group, default value is 8"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.tags"}),": List of environment tags, default value is [FrozenLake]"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"train_env_manager.num_groups_partition"}),": Group allocation for each environment type, default value is [128]"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"reference-examples",children:"Reference Examples"}),"\n",(0,a.jsx)(n.p,{children:"You can refer to the following configuration files to set up GiGPO training:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"./examples/docs_examples/example_gigpo.yaml"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsx)(n.p,{children:"[1] Feng, L.; Xue, Z.; Liu, T.; An, B. Group-in-Group Policy Optimization for LLM Agent Training. arXiv 2025, 2505.10978."})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);