"use strict";(self.webpackChunkdocs_roll=self.webpackChunkdocs_roll||[]).push([[7595],{1643:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var r=t(8168),a=(t(6540),t(5680));const o={},i="Quick Start: Multi-Node Deployment Guide",l={unversionedId:"English/QuickStart/multi_nodes_quick_start",id:"English/QuickStart/multi_nodes_quick_start",title:"Quick Start: Multi-Node Deployment Guide",description:"Environment Preparation",source:"@site/docs/English/QuickStart/multi_nodes_quick_start.md",sourceDirName:"English/QuickStart",slug:"/English/QuickStart/multi_nodes_quick_start",permalink:"/ROLL/docs/English/QuickStart/multi_nodes_quick_start",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/QuickStart/multi_nodes_quick_start.md",tags:[],version:"current",lastUpdatedAt:1756102296,formattedLastUpdatedAt:"Aug 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Installation",permalink:"/ROLL/docs/English/QuickStart/installation"},next:{title:"Frequently Asked Questions (Q&A)",permalink:"/ROLL/docs/English/QuickStart/qa_issues"}},s={},c=[{value:"Environment Preparation",id:"environment-preparation",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"Pipeline Execution",id:"pipeline-execution",level:2},{value:"Reference: Multi-GPU V100 Memory Configuration Key Points",id:"reference-multi-gpu-v100-memory-configuration-key-points",level:2}],p={toc:c},u="wrapper";function m({components:e,...n}){return(0,a.yg)(u,(0,r.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"quick-start-multi-node-deployment-guide"},"Quick Start: Multi-Node Deployment Guide"),(0,a.yg)("h2",{id:"environment-preparation"},"Environment Preparation"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Purchase multiple machines equipped with GPUs and install GPU drivers synchronously, with one machine as the master node and others as worker nodes (the example below uses 2 machines with 2 GPUs each)"),(0,a.yg)("li",{parentName:"ol"},"Connect to the GPU instances remotely and enter the machine terminal"),(0,a.yg)("li",{parentName:"ol"},"Run the following command on each machine to install the Docker environment and NVIDIA container toolkit")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"curl -fsSL https://github.com/alibaba/ROLL/blob/main/scripts/install_docker_nvidia_container_toolkit.sh | sudo bash\n")),(0,a.yg)("h2",{id:"environment-configuration"},"Environment Configuration"),(0,a.yg)("p",null,"Choose your desired Docker image from the ",(0,a.yg)("a",{parentName:"p",href:"https://alibaba.github.io/ROLL/docs/English/QuickStart/image_address"},"image addresses"),". The following example uses ",(0,a.yg)("em",{parentName:"p"},"torch2.6.0 + vLLM0.8.4")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"# 1. Start a Docker container with GPU support, expose container ports, and keep the container running\nsudo docker run -dit \\\n  --gpus all \\\n  -p 9001:22 \\\n  --ipc=host \\\n  --shm-size=10gb \\\n  roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch260-vllm084 \\\n  /bin/bash\n\n# 2. Enter the Docker container\n#    You can use the `sudo docker ps` command to find the running container ID or name.\nsudo docker exec -it <container_id> /bin/bash\n\n# 3. Verify that GPUs are visible\nnvidia-smi\n\n# 4. Clone the project code\ngit clone https://github.com/alibaba/ROLL.git\n\n# 5. Install project dependencies (choose the requirements file corresponding to your image)\ncd ROLL\npip install -r requirements_torch260_vllm.txt -i https://mirrors.aliyun.com/pypi/simple/\n")),(0,a.yg)("h2",{id:"pipeline-execution"},"Pipeline Execution"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Configure environment variables on the master node:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},'export MASTER_ADDR="ip of master node"\nexport MASTER_PORT="port of master node"  # Default: 6379\nexport WORLD_SIZE=2\nexport RANK=0\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\n')),(0,a.yg)("p",null,"Notes:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"MASTER_ADDR")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"MASTER_PORT")," define the communication endpoint of the distributed cluster."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"WORLD_SIZE")," specifies the total number of nodes in the cluster (e.g., 2 nodes)."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"RANK")," identifies the role of the node (0 represents the master node, 1, 2, 3, etc. represent worker nodes)."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"NCCL_SOCKET_IFNAME")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"GLOO_SOCKET_IFNAME")," specify the network interface used for GPU/cluster communication (usually eth0).")),(0,a.yg)("ol",{start:2},(0,a.yg)("li",{parentName:"ol"},"Run the pipeline on the master node")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"bash examples/agentic_demo/run_agentic_pipeline_frozen_lake_multi_nodes_demo.sh\n")),(0,a.yg)("p",null,"After the Ray cluster starts, you will see log examples like the following:\n",(0,a.yg)("img",{alt:"log_ray_multi_nodes",src:t(3746).A,width:"2758",height:"204"})),(0,a.yg)("ol",{start:3},(0,a.yg)("li",{parentName:"ol"},"Configure environment variables on the worker node")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},'export MASTER_ADDR="ip of master node"\nexport MASTER_PORT="port of master node" # Default: 6379\nexport WORLD_SIZE=2\nexport RANK=1\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\n')),(0,a.yg)("ol",{start:4},(0,a.yg)("li",{parentName:"ol"},"Connect to the Ray cluster started by the master node on the worker node:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell"},"ray start --address='ip of master node:port of master node' --num-gpus=2\n")),(0,a.yg)("h2",{id:"reference-multi-gpu-v100-memory-configuration-key-points"},"Reference: Multi-GPU V100 Memory Configuration Key Points"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"# Reduce the expected number of GPUs from 8 to the 2 V100s you actually have\nnum_gpus_per_node: 2\n# Training processes now map to GPUs 0-3\nactor_train.device_mapping: list(range(0,4))\n# Inference processes now map to GPUs 0-3\nactor_infer.device_mapping: list(range(0,4))\n# Reference model processes now map to GPUs 0-3\nreference.device_mapping: list(range(0,4))\n\n# Significantly reduce the batch size during Rollout/Validation stages to prevent out-of-memory errors when a single GPU processes large batches\nrollout_batch_size: 64\nval_batch_size: 16\n\n# V100 has better native support for FP16 than BF16 (unlike A100/H100). Switching to FP16 can improve compatibility and stability while saving GPU memory.\nactor_train.model_args.dtype: fp16\nactor_infer.model_args.dtype: fp16\nreference.model_args.dtype: fp16\n\n# Switch the large model training framework from DeepSpeed to Megatron-LM, where parameters can be sent in batches for faster execution\nstrategy_name: megatron_train\nstrategy_config:\n  tensor_model_parallel_size: 1\n  pipeline_model_parallel_size: 1\n  expert_model_parallel_size: 1\n  use_distributed_optimizer: true\n  recompute_granularity: full\n\n# In Megatron training, the global training batch size is per_device_train_batch_size * gradient_accumulation_steps * world_size, where world_size = 4\nactor_train.training_args.per_device_train_batch_size: 1\nactor_train.training_args.gradient_accumulation_steps: 16  \n\n# Reduce the maximum number of actions per trajectory to make each Rollout trajectory shorter, reducing the length of LLM-generated content\nmax_actions_per_traj: 10    \n\n# Reduce the number of parallel training environment groups and validation environment groups to accommodate single GPU resources\ntrain_env_manager.env_groups: 1\ntrain_env_manager.n_groups: 1\nval_env_manager.env_groups: 2\nval_env_manager.n_groups: [1, 1]\nval_env_manager.tags: [SimpleSokoban, FrozenLake]\n\n# Reduce the total number of training steps to run a complete training process faster for quick debugging\nmax_steps: 100\n")))}m.isMDXComponent=!0},3746:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_ray_multi_nodes-1caa4dc6e157ccf6d8f9542480af8b93.png"},5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>d});var r=t(6540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach(function(n){a(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},g=r.forwardRef(function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),g=a,d=u["".concat(s,".").concat(g)]||u[g]||m[g]||o;return t?r.createElement(d,i(i({ref:n},p),{},{components:t})):r.createElement(d,i({ref:n},p))});function d(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:a,i[1]=l;for(var c=2;c<o;c++)i[c]=t[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"}}]);