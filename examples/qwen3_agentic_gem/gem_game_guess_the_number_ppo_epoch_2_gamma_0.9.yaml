defaults:
  - ../config/traj_envs_gem_games@_here_
  - ../config/deepspeed_zero@_here_
  - ../config/deepspeed_zero2@_here_
  - ../config/deepspeed_zero3@_here_
  - ../config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

exp_name: "agentic_pipeline"
seed: 42
logging_dir: ./output/logs
output_dir: ./output

#track_with: wandb
#tracker_kwargs:
#  api_key:
#  project: roll-agentic
#  name: ${exp_name}_sokoban
#  notes: "agentic_pipeline"
#  tags:
#    - agentic
#    - roll
#    - baseline

checkpoint_config:
  type: file_system
  output_dir: /data/cpfs_0/rl_examples/models/${exp_name}

num_gpus_per_node: 8

max_steps: 1024
save_steps: 10000
logging_steps: 1
eval_steps: 10
resume_from_checkpoint: false

rollout_batch_size: 128
val_batch_size: 128
sequence_length: 2048

advantage_clip: 20
ppo_epochs: 2
adv_estimator: "step_reinforce"
batch_adjust_mode: "random_sample"
step_reward_gamma: 0.9

#pg_clip: 0.1
#dual_clip_loss: True
init_kl_coef: 0.0
whiten_advantages: false
entropy_loss_coef: 0
max_grad_norm: 1.0
loss_agg_mode: token-mean

pretrain: Qwen/Qwen3-1.7B-Base
reward_pretrain: Qwen/Qwen3-1.7B-Base

actor_train:
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: ~
  training_args:
    learning_rate: 1.0e-6
    weight_decay: 0
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 8
    lr_scheduler_type: constant
  strategy_args:
#    strategy_name: deepspeed_train
#    strategy_config: ${deepspeed_zero3}
    strategy_name: megatron_train
    strategy_config:
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      use_distributed_optimizer: true
      recompute_granularity: full
  device_mapping: list(range(0,8))
  infer_batch_size: 2

actor_infer:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
  generating_args:
    max_new_tokens: ${max_tokens_per_step} # single-turn response length
    top_p: 1.0
    top_k: -1
    num_beams: 1
    temperature: 1.0
    num_return_sequences: 1
  strategy_args:
    strategy_name: vllm
    strategy_config:
      gpu_memory_utilization: 0.8
      block_size: 16
      load_format: auto
  device_mapping: list(range(0,8))

reference:
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~
  strategy_args:
    strategy_name: hf_infer
    strategy_config: ~
  device_mapping: list(range(0,8))
  infer_batch_size: 2

reward_normalization:
  grouping: batch # 可以tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by计算reward/adv
  method: mean_std # asym_clip / identity / mean_std

train_env_manager:
  max_env_num_per_worker: 16
  num_env_groups: 128
  # under the same group, the env config and env seed are ensured to be equal
  group_size: 1
  tags: [GuessTheNumber]
  num_groups_partition: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation

val_env_manager:
  max_env_num_per_worker: 32
  num_env_groups: 128
  group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
  tags: [GuessTheNumber]
  num_groups_partition: [128] # TODO: If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation


# Here, you can override variables defined in the imported envs. max_tokens_per_step: 128 in custom_env.SimpleSokoban, here replaced by 64
max_tokens_per_step: 512
max_actions_per_traj: 20
default_history_length: ${max_actions_per_traj}
env_manager_cls: roll.pipeline.agentic.env_manager.step_env_manager.StepEnvManager

custom_envs:
  GuessTheNumber:
    env_type: game:GuessTheNumber-v0
    max_steps: ${max_actions_per_traj}
    max_tokens_per_step: ${max_tokens_per_step}
    env_manager_cls: ${env_manager_cls}
    use_thread_lock: true
    history_length: ${default_history_length}
    agent_system_template: ${agent_system_template}
    agent_template: ${agent_template}
    env_config:
      min_number: 1
      max_number: 20
      max_turns: ${max_actions_per_traj} # From GuessTheNumber-v0 registration

agent_system_template: ~
agent_template: |
  You are playing language games. Make valid actions to win.
  Observation:
  {history}
  {current_observation}
  Please reason step by step, and put your final answer within \\boxed{{}}.
  
